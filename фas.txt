import type {
  RTCPeerConnection,
  MediaStreamTrack,
  MediaStream,
} from 'react-native-webrtc';
import type RTCDataChannel from 'react-native-webrtc/lib/typescript/RTCDataChannel';
export interface SuccessCallbacks {
  onHangUpStarted?: () => void;
  onHangUpDone?: () => void;

  onPeerConnectionCreatingStarted?: () => void;
  onPeerConnectionCreated?: (pc: RTCPeerConnection) => void;
  onRTCPeerConnectionStateChange?: (
    state:
      | 'new'
      | 'connecting'
      | 'connected'
      | 'disconnected'
      | 'failed'
      | 'closed'
  ) => void;

  onGetUserMediaSetted?: (stream: MediaStream) => void;
  onLocalStreamSetted?: (stream: MediaStream) => void;
  onLocalStreamAddedTrack?: (track: MediaStreamTrack) => void;
  onLocalStreamRemovedTrack?: (track: MediaStreamTrack) => void;
  onRemoteStreamSetted?: (stream: MediaStream) => void;

  onDataChannelOpen?: (channel: RTCDataChannel) => void;
  onDataChannelMessage?: (message: any) => void;
  onDataChannelClose?: () => void;

  onIceGatheringComplete?: () => void;
  onIceGatheringTimeout?: () => void;
  onIceGatheringStateChange?: (state: string) => void;

  onMicrophonePermissionGranted?: () => void;
  onMicrophonePermissionDenied?: () => void;

  onIOSTransceiverSetted?: () => void;
}

export interface BaseProps extends SuccessCallbacks {
  onError?: (event: ErrorEvent) => void;
  onSuccess?: (stage: string, data?: any) => void;
}
export type SpeechActivityState = {
  isUserSpeaking: boolean;
  isAssistantSpeaking: boolean;
  inputBuffered: boolean;
  outputBuffered: boolean;
  lastUserEventAt: number | null;
  lastAssistantEventAt: number | null;
};

export type Listener = (s: SpeechActivityState) => void;
export type MediaTrackConstraints = {
  width?: ConstrainNumber;
  height?: ConstrainNumber;
  frameRate?: ConstrainNumber;
  facingMode?: ConstrainString;
  deviceId?: ConstrainString;
  groupId?: ConstrainString;
};
type ConstrainNumber =
  | number
  | {
      exact?: number;
      ideal?: number;
      max?: number;
      min?: number;
    };
type ConstrainString =
  | string
  | {
      exact?: string;
      ideal?: string;
    };

export interface Constraints {
  audio?: boolean | MediaTrackConstraints;
  video?: boolean | MediaTrackConstraints;
}
export type ErrorStage =
  | 'hangup'
  | 'ice_gathering'
  | 'peer_connection'
  | 'microphone_permission'
  | 'remote_stream'
  | 'local_stream'
  | 'data_channel'
  | 'get_user_media'
  | 'ios_transceiver'
  | 'init_peer_connection'
  | 'create_offer'
  | 'set_local_description'
  | 'set_remote_description'
  | 'fetch_token'
  | 'openai_api';

export type ErrorSeverity = 'critical' | 'warning' | 'info';

export interface ErrorEvent {
  stage: ErrorStage;
  error: Error;
  severity: ErrorSeverity;
  recoverable: boolean;
  timestamp: number;
  context?: Record<string, any>;
}
import { AddableMessage, ExtendedChatMsg } from './Chat';
import type { RealtimeStatus } from '@react-native-openai-realtime/types';
import type { RealtimeClientClass } from '@react-native-openai-realtime/components';

export type RealtimeContextValue = {
  client: RealtimeClientClass | null;
  status: RealtimeStatus;
  clearChatHistory: () => void;
  chat: ExtendedChatMsg[];
  connect: () => Promise<void>;
  disconnect: () => Promise<void> | void;
  sendResponse: (opts?: any) => void;
  sendResponseStrict: (opts: {
    instructions: string;
    modalities?: Array<'audio' | 'text'>;
    conversation?: 'auto' | 'none';
  }) => void;
  updateSession: (patch: Partial<any>) => void;
  sendRaw: (event: any) => void;
  addMessage: (m: AddableMessage | AddableMessage[]) => string | string[];
  clearAdded: () => void;

  getNextTs: () => number;
};
export type ChatAdapterOptions = {
  isMeaningfulText?: (text: string) => boolean;
};

export type ChatOptions = {
  enabled?: boolean; // по умолчанию true — встроенный чат-стор включён
  isMeaningfulText?: (text: string) => boolean;
  userAddOnDelta?: boolean;
  userPlaceholderOnStart?: boolean;
  assistantAddOnDelta?: boolean;
  assistantPlaceholderOnStart?: boolean;
};

export type ChatMsg = {
  id: string;
  type: 'text' | 'ui';
  time: number;
  role: 'user' | 'assistant';
  text?: string;
  ts: number;
  status: 'streaming' | 'done' | 'canceled';
  responseId?: string;
  itemId?: string;
};

export type UIChatMsg = {
  id: string;
  role: 'assistant' | 'user' | 'system' | 'tool';
  ts: number;
  type: 'ui';
  time: number;
  kind: string; // тип вашего UI-сообщения
  payload: any; // любые данные для рендера
};

// Расширенный тип чата: встроенные сообщения + ваши UI-сообщения
export type ExtendedChatMsg = ChatMsg | UIChatMsg;

// Что можно добавить через addMessage (одно или много)
export type AddableMessage =
  | {
      id?: string;
      role?: 'assistant' | 'user' | 'system' | 'tool';
      ts?: number;
      type?: 'text';
      text: string;
    }
  | {
      id?: string;
      role?: 'assistant' | 'user' | 'system' | 'tool';
      ts?: number;
      type: 'ui';
      kind: string;
      payload: any;
    };
export interface UseSessionOptionsParams {
  /** Client из useRealtime() */
  client: any;
  /** Callback при успешной операции */
  onSuccess?: (
    stage:
      | 'voice_initialized'
      | 'voice_closed'
      | 'assistant_cancelled'
      | 'text_initialized'
  ) => void;
  /** Callback при ошибке */
  onError?: (
    stage:
      | 'voice_init'
      | 'voice_close'
      | 'assistant_cancel'
      | 'text_init'
      | 'text_close',
    error: any
  ) => void;
}
// src/types/Events.ts

/**
 * Типы всех событий, которые можно слушать через client.on()
 */
export type RealtimeEventMap = {
  // User events (пользовательский ввод)
  'user:item_started': { itemId: string };
  'user:delta': { itemId: string; delta: string };
  'user:completed': { itemId: string; transcript: string };
  'user:failed': { itemId: string; error?: any };
  'user:truncated': { itemId: string };

  // Assistant events (ответы ассистента)
  'assistant:response_started': { responseId: string };
  'assistant:item_started': { itemId: string };
  'assistant:delta': {
    responseId: string;
    delta: string;
    channel?: 'audio_transcript' | 'output_text';
  };
  'assistant:completed': { responseId: string; status: 'done' | 'canceled' };

  // Tool events (вызовы функций)
  'tool:call_delta': { call_id: string; name: string; delta: string };
  'tool:call_done': { call_id: string; name: string; args: any };

  // Speech activity events (определение речи)
  'speech_started': void;
  'speech_stopped': void;
  'timeout_triggered': void;

  // Buffer events (буферы аудио)
  'input_audio_buffer.committed': any;
  'input_audio_buffer.cleared': any;
  'output_audio_buffer.started': any;
  'output_audio_buffer.stopped': any;
  'output_audio_buffer.cleared': any;

  // DataChannel events (низкоуровневые)
  'dataChannelOpen': any;
  'dataChannelMessage': any;
  'dataChannelClose': void;

  // Error events
  'error': { scope: 'tool' | 'server' | string; error: any };

  // Raw server events (любое серверное событие)
  [key: string]: any;
};

/**
 * Типизированный слушатель событий
 */
export type RealtimeEventListener<K extends keyof RealtimeEventMap> = (
  payload: RealtimeEventMap[K]
) => void;

/**
 * Коллбэки для всех Success событий (полный список)
 */
export interface RealtimeSuccessCallbacks {
  // Connection lifecycle
  onHangUpStarted?: () => void;
  onHangUpDone?: () => void;

  // PeerConnection events
  onPeerConnectionCreatingStarted?: () => void;
  onPeerConnectionCreated?: (pc: any) => void;
  onRTCPeerConnectionStateChange?: (
    state:
      | 'new'
      | 'connecting'
      | 'connected'
      | 'disconnected'
      | 'failed'
      | 'closed'
  ) => void;

  // Media events
  onGetUserMediaSetted?: (stream: any) => void;
  onLocalStreamSetted?: (stream: any) => void;
  onLocalStreamAddedTrack?: (track: any) => void;
  onLocalStreamRemovedTrack?: (track: any) => void;
  onRemoteStreamSetted?: (stream: any) => void;

  // DataChannel events
  onDataChannelOpen?: (channel: any) => void;
  onDataChannelMessage?: (message: any) => void;
  onDataChannelClose?: () => void;

  // ICE events
  onIceGatheringComplete?: () => void;
  onIceGatheringTimeout?: () => void;
  onIceGatheringStateChange?: (state: string) => void;

  // Permission events
  onMicrophonePermissionGranted?: () => void;
  onMicrophonePermissionDenied?: () => void;

  // iOS specific
  onIOSTransceiverSetted?: () => void;

  // Generic success handler
  onSuccess?: (stage: string, data?: any) => void;
}

/**
 * Коллбэки для Error событий
 */
export interface RealtimeErrorCallbacks {
  onError?: (event: {
    stage: string;
    error: Error;
    severity: 'critical' | 'warning' | 'info';
    recoverable: boolean;
    timestamp: number;
    context?: Record<string, any>;
  }) => void;
}
// src/types/Responce.ts
export type ResponseCreateParams = {
  instructions: string;
  modalities?: Array<'audio' | 'text'>;
  conversation?: 'auto' | 'none'; // <-- только 'auto' | 'none'
};

export type ResponseCreateOptions = {
  instructions?: string;
  modalities?: Array<'audio' | 'text'>;
  conversation?: 'auto' | 'none';
};

export type ResponseCreateStrict = Omit<
  ResponseCreateOptions,
  'instructions'
> & {
  instructions: string;
};
export * from './Chat';
export * from './ClientStructure';
export * from './Constraints';
export * from './Context';
export * from './ErrorStage';
export * from './Responce';
export * from './Speech';
export * from './SucessCallbacks';
export * from './Events';
export * from './Modes';
import type { RTCOfferOptions } from 'react-native-webrtc/lib/typescript/RTCUtil';
import { VOICE_IDS, VoiceId } from '@react-native-openai-realtime/helpers';
import type {
  RealtimeContextValue,
  Constraints,
  ChatOptions,
  RealtimeSuccessCallbacks,
  RealtimeErrorCallbacks,
} from '@react-native-openai-realtime/types';
import { RealtimeClientClass } from '@react-native-openai-realtime/components';

export type RTCIceServer = {
  credential?: string;
  url?: string;
  urls?: string | string[];
  username?: string;
};

export type RTCConfiguration = {
  bundlePolicy?: 'balanced' | 'max-compat' | 'max-bundle';
  iceCandidatePoolSize?: number;
  iceServers?: RTCIceServer[];
  iceTransportPolicy?: 'all' | 'relay';
  rtcpMuxPolicy?: 'negotiate' | 'require';
};

export type TokenProvider = () => Promise<string>;

export type ChatMode = 'voice' | 'text';

export type SessionConfig = {
  model?: string;
  voice?: VoiceId;
  modalities?: Array<'audio' | 'text'>;
  turn_detection?: {
    type: 'server_vad';
    silence_duration_ms?: number;
    threshold?: number;
    prefix_padding_ms?: number;
  } | null;
  input_audio_transcription?: { model: string; language?: string };
  tools?: any[];
  instructions?: string;
};

export type RealtimeClientHooks = {
  onOpen?: (dc: any) => void;
  onEvent?: (evt: any) => void;
  onError?: (e: any) => void;

  onUserTranscriptionDelta?: (payload: {
    itemId: string;
    delta: string;
  }) => 'consume' | void;
  onUserTranscriptionCompleted?: (payload: {
    itemId: string;
    transcript: string;
  }) => 'consume' | void;

  onAssistantTextDelta?: (payload: {
    responseId: string;
    delta: string;
    channel: 'audio_transcript' | 'output_text';
  }) => 'consume' | void;

  onAssistantCompleted?: (payload: {
    responseId: string;
    status: 'done' | 'canceled';
  }) => 'consume' | void;

  onToolCall?: (payload: {
    name: string;
    args: any;
    call_id: string;
  }) => Promise<any> | any;
};

export type MiddlewareCtx = {
  event: any;
  send: (e: any) => void | Promise<void>;
  client: RealtimeClientClass;
};

export type IncomingMiddleware = (
  ctx: MiddlewareCtx
) => Promise<any | 'stop' | null | void> | any | 'stop' | null | void;

export type OutgoingMiddleware = (
  event: any
) => any | null | 'stop' | Promise<any | null | 'stop'>;

export type Logger = {
  debug?: (...a: any[]) => void;
  info?: (...a: any[]) => void;
  warn?: (...a: any[]) => void;
  error?: (...a: any[]) => void;
};

export type RealtimeStatus =
  | 'idle'
  | 'connecting'
  | 'connected'
  | 'disconnected'
  | 'error'
  | 'user_speaking'
  | 'assistant_speaking';

export type RealtimeClientOptionsBeforePrune = {
  deleteChatHistoryOnDisconnect?: boolean;
  tokenProvider: TokenProvider;
  voice?: keyof typeof VOICE_IDS;
  webrtc?: {
    iceServers?: RTCIceServer[];
    dataChannelLabel?: string;
    offerOptions?: RTCOfferOptions & { voiceActivityDetection?: boolean };
    configuration?: RTCConfiguration;
  };
  media?: { getUserMedia?: Constraints };
  session?: Partial<SessionConfig>;
  autoSessionUpdate?: boolean;
  greet?: {
    enabled?: boolean;
    response?: {
      instructions?: string;
      modalities?: Array<'audio' | 'text'>;
    };
  };
  hooks?: RealtimeClientHooks;
  middleware?: {
    incoming?: IncomingMiddleware[];
    outgoing?: OutgoingMiddleware[];
  };
  policy?: {
    isMeaningfulText?: (text: string) => boolean;
  };
  chat?: ChatOptions;
  logger?: Logger;
  allowConnectWithoutMic?: boolean;
};

/**
 * ОБНОВЛЕНО: Добавлены все Success и Error коллбэки в публичные пропсы
 */
export type RealTimeClientProps = RealtimeSuccessCallbacks &
  RealtimeErrorCallbacks & {
    // Быстрые чат-поведения
    chatUserAddOnDelta?: boolean;
    chatInverted?: boolean;
    deleteChatHistoryOnDisconnect?: boolean;
    chatUserPlaceholderOnStart?: boolean;
    chatAssistantAddOnDelta?: boolean;
    chatAssistantPlaceholderOnStart?: boolean;

    tokenProvider?: TokenProvider;

    webrtc?: {
      iceServers?: RTCIceServer[];
      dataChannelLabel?: string;
      offerOptions?: RTCOfferOptions & { voiceActivityDetection?: boolean };
      configuration?: RTCConfiguration;
    };

    media?: { getUserMedia?: Constraints };
    session?: Partial<SessionConfig>;
    autoSessionUpdate?: boolean;

    // Greet настройки
    greetEnabled?: boolean;
    greetInstructions?: string;
    greetModalities?: Array<'audio' | 'text'>;

    // Hooks
    onOpen?: RealtimeClientHooks['onOpen'];
    onEvent?: RealtimeClientHooks['onEvent'];
    // onError уже в RealtimeErrorCallbacks
    onUserTranscriptionDelta?: RealtimeClientHooks['onUserTranscriptionDelta'];
    onUserTranscriptionCompleted?: RealtimeClientHooks['onUserTranscriptionCompleted'];
    onAssistantTextDelta?: RealtimeClientHooks['onAssistantTextDelta'];
    onAssistantCompleted?: RealtimeClientHooks['onAssistantCompleted'];
    onToolCall?: RealtimeClientHooks['onToolCall'];

    // Middleware
    incomingMiddleware?: IncomingMiddleware[];
    outgoingMiddleware?: OutgoingMiddleware[];

    // Policy/chat
    policyIsMeaningfulText?: (text: string) => boolean;
    chatEnabled?: boolean;
    chatIsMeaningfulText?: (text: string) => boolean;
    logger?: Logger;
    autoConnect?: boolean;
    attachChat?: boolean;

    allowConnectWithoutMic?: boolean;

    children?:
      | React.ReactNode
      | ((ctx: RealtimeContextValue) => React.ReactNode);
  };

export type CoreConfig = Omit<
  RealTimeClientProps,
  'autoConnect' | 'attachChat' | 'children'
>;
import { createContext } from 'react';
import type { RealtimeContextValue } from '@react-native-openai-realtime/types';

export const RealtimeContext = createContext<RealtimeContextValue>({
  client: null,
  status: 'idle',
  chat: [],
  connect: async () => {},
  disconnect: async () => {},
  sendResponse: () => {},
  sendResponseStrict: () => {},
  updateSession: () => {},
  clearChatHistory: () => {},
  sendRaw: () => {},
  addMessage: () => '',
  clearAdded: () => {},

  getNextTs: () => Date.now(),
});

export const RealtimeProvider = RealtimeContext.Provider;
export * from './RealtimeContext';
export * from './RealtimeContext';
policy — это глобальные правила по умолчанию для всей библиотеки. chat — это настройки только встроенного чат-хранилища/адаптера. Если заданы оба, chat имеет приоритет для чата.

Детально

policy.isMeaningfulText

Глобальный дефолтный предикат “текст осмысленный?”.
Используется библиотекой как fallback, когда для конкретного модуля (например, чата) не задан свой предикат.
Может быть полезен и вам самим в middleware/хуках (в примере мы использовали этот предикат, чтобы фильтровать «пустые» assistant-дeльты).
chat.isMeaningfulText

Предикат только для встроенного чата (ChatStore/ChatAdapter): решает, создавать ли пузырь на финализации и удалять ли пустышки.
Имеет приоритет над policy.isMeaningfulText.
Работает, только если chat.enabled !== false (встроенный чат включен).
Если chat.enabled=false, встроенного чата нет, и chat.isMeaningfulText игнорируется.
Как выбирается итоговое правило для чата
Внутри RealtimeClient при создании ChatStore:
const predicate = options.chat?.isMeaningfulText ?? options.policy?.isMeaningfulText ?? defaultPredicate;

Когда что использовать

Хочу одно правило везде и сразу: задайте только policy.isMeaningfulText. Чат возьмет его как дефолт (если вы не переопределите chat.isMeaningfulText).
Хочу отдельную логику именно для чата: задайте chat.isMeaningfulText (оно перекроет policy только для чата).
Хочу отключить встроенный чат, но продолжать фильтровать события в middleware/хуках: chat.enabled = false, используйте policy.isMeaningfulText в своих перехватчиках.
Использую ChatAdapter вручную: можете передать свой isMeaningfulText прямо в attachChatAdapter(..., { isMeaningfulText }) — он перекроет любые options.
Мини-примеры

Один предикат для всего (просто в policy)
policy: {
isMeaningfulText: t => t.replace(/[^\p{L}\p{N}]+/gu,'').trim().length >= 2,
},
// chat не задаем — чат возьмет из policy

Чату — мягче, глобально — строже
policy: {
isMeaningfulText: t => t.replace(/\s+/g,'').length >= 3, // глобально строгий
},
chat: {
enabled: true,
isMeaningfulText: t => !!t.trim(), // для чата помягче
}

Без встроенного чата, но с глобальным правилом (для своих middleware)
chat: { enabled: false },
policy: {
isMeaningfulText: t => !/^(эм+|мм+|ээ+|угу+|ага+).?$/i.test(t.trim()),
}

Если нужна наглядность

policy — это «общая политика по умолчанию».
chat — это «частные настройки именно чата» (перекрывают policy для чата).
chat.enabled включает/выключает встроенный чат целиком. Policy ничего не включает/выключает, это просто набор правил/эвристик, которые могут использовать разные части системы.

Зачем нужна isMeaningfulText и что она делает

Это предикат “текст осмысленный?”. Он решает, считать ли фрагмент текста полноценным содержимым или “пустышкой”.
Где используется:
Встроенный чат (ChatStore/ChatAdapter): при финализации сообщения, если текст неосмысленный (пусто, одни пробелы/пунктуация/междометия) — пузырь не попадает в чат или удаляется.
В middleware/хуках: можно программно “съедать” ничтожные дельты ассистента/пользователя, чтобы UI не моргал “…” и не копил мусор.
Разница между policy.isMeaningfulText и chat.isMeaningfulText

policy.isMeaningfulText — глобальный дефолт для всей библиотеки (его удобно использовать и в middleware/хуках). Если нигде не переопределено, чату достанется именно он.
chat.isMeaningfulText — настройка только встроенного чата (ChatStore/ChatAdapter). Если указали — она перекрывает policy для чата.
Итого: итоговое правило для чата выбирается так:
chat.isMeaningfulText ?? policy.isMeaningfulText ?? (t => !!t.trim())
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';

export const DEFAULTS: RealtimeClientOptionsBeforePrune = {
  // tokenProvider — обязателен, без него нельзя получить ephemeral token
  tokenProvider: async () => {
    throw new Error(
      'tokenProvider is required: provide a function returning ephemeral token'
    );
  },

  webrtc: {
    iceServers: [
      { urls: 'stun:stun.l.google.com:19302' },
      { urls: 'stun:stun1.l.google.com:19302' },
    ],
    dataChannelLabel: 'oai-events',
    offerOptions: {
      offerToReceiveAudio: true,
      voiceActivityDetection: true,
    } as any,
    configuration: { iceCandidatePoolSize: 10 },
  },

  media: {
    getUserMedia: {
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      } as any,
      video: false,
    },
  },

  session: {
    model: 'gpt-4o-realtime-preview-2024-12-17',
    voice: 'alloy',
    modalities: ['audio', 'text'],
    input_audio_transcription: { model: 'whisper-1' },
    turn_detection: {
      type: 'server_vad',
      silence_duration_ms: 700,
      prefix_padding_ms: 300,
      threshold: 0.5,
    },
  },

  autoSessionUpdate: true,

  greet: {
    enabled: true,
    response: {
      instructions: 'Привет! Я на связи и готов помочь.',
      modalities: ['audio', 'text'],
    },
  },

  hooks: {},

  middleware: {
    incoming: [],
    outgoing: [],
  },

  policy: {
    isMeaningfulText: (t: string) => !!t.trim(),
  },

  chat: {
    enabled: true,
    isMeaningfulText: (t: string) => !!t.trim(),
  },

  logger: {
    info: (...a) => console.log('[INFO]', ...a),
    debug: (...a) => console.log('[DEBUG]', ...a),
    warn: (...a) => console.log('[WARN]', ...a),
    error: (...a) => console.log('[ERROR]', ...a),
  },
};
export * from './defaultOptions';
// src/managers/MessageSender.ts
import type {
  ResponseCreateParams,
  ResponseCreateStrict,
  RealtimeClientOptionsBeforePrune,
} from '@react-native-openai-realtime/types';
import { ErrorHandler } from '@react-native-openai-realtime/handlers';
import { DataChannelManager } from '@react-native-openai-realtime/managers';

export class MessageSender {
  private dataChannelManager: DataChannelManager;
  private options: RealtimeClientOptionsBeforePrune;
  private errorHandler: ErrorHandler;

  constructor(
    dataChannelManager: DataChannelManager,
    options: RealtimeClientOptionsBeforePrune,
    errorHandler: ErrorHandler
  ) {
    this.dataChannelManager = dataChannelManager;
    this.options = options;
    this.errorHandler = errorHandler;
  }

  async sendRaw(event: any): Promise<void> {
    try {
      if (!this.dataChannelManager.isOpen()) {
        throw new Error('DataChannel is not open');
      }

      // Outgoing middleware
      if (this.options.middleware?.outgoing) {
        for (const mw of this.options.middleware.outgoing) {
          const res = await Promise.resolve(mw(event));

          if (res === 'stop') {
            return;
          }
          if (res && typeof res === 'object') {
            event = res;
          }
        }
      }

      if (!this.dataChannelManager.isOpen()) {
        throw new Error('DataChannel is not open');
      }

      this.dataChannelManager.send(event);
    } catch (e: any) {
      this.errorHandler.handle('data_channel', e, 'warning', true, { event });
    }
  }

  sendResponse(): void;
  sendResponse(params: ResponseCreateParams): void;
  sendResponse(params?: any): void {
    const response = params ?? {};
    this.sendRaw({ type: 'response.create', response });
  }

  sendResponseStrict(options: ResponseCreateStrict) {
    this.sendRaw({ type: 'response.create', response: options });
  }

  updateSession(patch: Partial<any>) {
    this.sendRaw({ type: 'session.update', session: patch });
  }

  sendToolOutput(call_id: string, output: any) {
    this.sendRaw({
      type: 'conversation.item.create',
      item: {
        type: 'function_call_output',
        call_id,
        output: JSON.stringify(output),
      },
    });
  }

  // ИСПРАВЛЕНО: Правильная отправка текстовых сообщений
  async sendTextMessage(
    text: string,
    options?: {
      responseModality?: 'text' | 'audio';
      instructions?: string;
      conversation?: 'auto' | 'none';
    }
  ): Promise<void> {
    const msg = (text ?? '').trim();
    if (!msg) return;

    // 1. Создаем conversation item с текстом пользователя
    await this.sendRaw({
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'user',
        content: [
          {
            type: 'input_text',
            text: msg,
          },
        ],
      },
    });

    // 2. Создаем response с правильными параметрами
    const modality = options?.responseModality ?? 'text';

    // ВАЖНО: используем 'auto' вместо 'none' для сохранения контекста
    const response: ResponseCreateStrict = {
      instructions:
        options?.instructions ?? 'Ответь на сообщение пользователя.',
      modalities: modality === 'text' ? ['text'] : ['audio', 'text'],
      // По умолчанию 'auto' - добавляет в историю разговора
      conversation: options?.conversation ?? 'auto',
    };

    this.sendResponseStrict(response);
  }
}
// core/managers/OpenAIApiClient.ts
import { ErrorHandler } from '@react-native-openai-realtime/handlers';

export class OpenAIApiClient {
  private errorHandler: ErrorHandler;

  constructor(errorHandler: ErrorHandler) {
    this.errorHandler = errorHandler;
  }

  async postSDP(localSdp: string, ephemeralKey: string): Promise<string> {
    try {
      const resp = await fetch('https://api.openai.com/v1/realtime/calls', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${ephemeralKey}`,
          'Content-Type': 'application/sdp',
          'OpenAI-Beta': 'realtime=v1',
        },
        body: localSdp || '',
      });

      const text = await resp.text();

      if (!resp.ok) {
        this.errorHandler.handle(
          'openai_api',
          new Error(`OpenAI ${resp.status}: ${text.slice(0, 200)}`)
        );
        throw new Error(text);
      }

      if (!text.startsWith('v=')) {
        this.errorHandler.handle(
          'openai_api',
          new Error(`Invalid SDP from OpenAI: ${text.slice(0, 100)}`)
        );
        throw new Error('Invalid SDP');
      }

      return text;
    } catch (e: any) {
      this.errorHandler.handle('openai_api', e);
      throw e;
    }
  }
}
// В файле: MediaManager.ts
// Замените класс целиком:

import { mediaDevices, MediaStream } from 'react-native-webrtc';
import type { RTCPeerConnection } from 'react-native-webrtc';
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';
import {
  ErrorHandler,
  SuccessHandler,
} from '@react-native-openai-realtime/handlers';

export class MediaManager {
  private localStream: MediaStream | null = null;
  private remoteStream: MediaStream | null = null;
  private options: RealtimeClientOptionsBeforePrune;
  private errorHandler: ErrorHandler;
  private successHandler: SuccessHandler;

  constructor(
    options: RealtimeClientOptionsBeforePrune,
    errorHandler: ErrorHandler,
    successHandler: SuccessHandler
  ) {
    this.options = options;
    this.errorHandler = errorHandler;
    this.successHandler = successHandler;
  }

  // ✅ ИСПРАВЛЕНО: Не вызываем errorHandler внутри
  async getUserMedia(): Promise<MediaStream> {
    // Сначала останавливаем старый stream если есть
    this.stopLocalStream();

    const constraints = this.options.media?.getUserMedia!;
    const stream = await mediaDevices.getUserMedia(constraints);
    this.localStream = stream;
    this.successHandler.getUserMediaSetted(stream);
    this.successHandler.localStreamSetted(stream);
    return stream;
  }

  addLocalStreamToPeerConnection(pc: RTCPeerConnection, stream: MediaStream) {
    // КРИТИЧНО: Проверяем, что PeerConnection не закрыт
    if (!pc || pc.connectionState === 'closed') {
      const error = new Error('Cannot add tracks: PeerConnection is closed');
      this.errorHandler.handle('local_stream', error, 'critical', false);
      throw error;
    }

    stream.getTracks().forEach((track) => {
      try {
        // Дополнительная проверка перед добавлением каждого трека
        if (pc.connectionState === 'closed') {
          throw new Error('PeerConnection closed during track addition');
        }

        pc.addTrack(track, stream);
        this.successHandler.localStreamAddedTrack(track);
      } catch (e: any) {
        this.errorHandler.handle('local_stream', e);
        throw e; // Пробрасываем ошибку, чтобы прервать connect()
      }
    });
  }

  setupRemoteStream(pc: RTCPeerConnection) {
    // @ts-ignore
    pc.ontrack = (event: any) => {
      try {
        if (!this.remoteStream) {
          this.remoteStream = new MediaStream();
        }
        this.remoteStream.addTrack(event.track);
        this.successHandler.remoteStreamSetted(this.remoteStream);
      } catch (e: any) {
        this.errorHandler.handle('remote_stream', e);
      }
    };
  }

  getLocalStream() {
    return this.localStream;
  }

  getRemoteStream() {
    return this.remoteStream;
  }

  stopLocalStream() {
    if (this.localStream) {
      try {
        this.localStream.getTracks().forEach((t) => {
          try {
            t.stop();
          } catch {}
        });
      } catch {}
      this.localStream = null;
    }
  }

  cleanup() {
    this.stopLocalStream();
    this.remoteStream = null;
  }
}
import {
  ErrorHandler,
  SuccessHandler,
} from '@react-native-openai-realtime/handlers';
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';
import { RTCPeerConnection } from 'react-native-webrtc';

export class PeerConnectionManager {
  private pc: RTCPeerConnection | null = null;
  private options: RealtimeClientOptionsBeforePrune;
  private errorHandler: ErrorHandler;
  private successHandler: SuccessHandler;

  constructor(
    options: RealtimeClientOptionsBeforePrune,
    errorHandler: ErrorHandler,
    successHandler: SuccessHandler
  ) {
    this.options = options;
    this.errorHandler = errorHandler;
    this.successHandler = successHandler;
  }

  create(): RTCPeerConnection {
    try {
      // ВАЖНО: Если уже есть pc - закрываем его ПЕРЕД созданием нового
      if (this.pc) {
        try {
          this.pc.close();
        } catch {}
        this.pc = null;
      }

      this.successHandler.peerConnectionCreatingStarted();

      const pc = new RTCPeerConnection(
        this.options.webrtc?.configuration ??
          ({
            iceServers: this.options.webrtc?.iceServers,
            iceCandidatePoolSize:
              this.options.webrtc?.configuration?.iceCandidatePoolSize ?? 10,
          } as RTCConfiguration)
      );

      this.pc = pc;
      this.successHandler.peerConnectionCreated(pc);

      // Setup listeners
      this.setupListeners(pc);

      return pc;
    } catch (e: any) {
      this.errorHandler.handle('peer_connection', e);
      throw e;
    }
  }

  private setupListeners(pc: RTCPeerConnection) {
    // @ts-ignore
    pc.onconnectionstatechange = () =>
      this.successHandler.rtcPeerConnectionStateChange(pc.connectionState);

    // @ts-ignore
    pc.onicecandidate = (e: any) => {
      if (!e.candidate) {
        this.successHandler.iceGatheringComplete();
      }
    };

    // @ts-ignore
    pc.oniceconnectionstatechange = () =>
      this.successHandler.iceGatheringStateChange(pc.iceConnectionState);
  }

  async createOffer() {
    if (!this.pc) {
      throw new Error('PeerConnection not created');
    }

    try {
      const offerOptions = this.options.webrtc?.offerOptions as any;
      const offer = await this.pc.createOffer(offerOptions);
      return offer;
    } catch (e: any) {
      this.errorHandler.handle('create_offer', e);
      throw e;
    }
  }

  async setLocalDescription(offer: any) {
    if (!this.pc) {
      throw new Error('PeerConnection not created');
    }

    try {
      await this.pc.setLocalDescription(offer);
    } catch (e: any) {
      this.errorHandler.handle('set_local_description', e);
      throw e;
    }
  }

  async setRemoteDescription(answer: string) {
    if (!this.pc) {
      throw new Error('PeerConnection not created');
    }

    try {
      await this.pc.setRemoteDescription({ type: 'answer', sdp: answer });
    } catch (e: any) {
      this.errorHandler.handle('set_remote_description', e);
      throw e;
    }
  }

  async waitForIceGathering(): Promise<void> {
    if (!this.pc) {
      throw new Error('PeerConnection not created');
    }

    return new Promise((resolve) => {
      try {
        if (this.pc!.iceGatheringState === 'complete') {
          this.successHandler.iceGatheringComplete();
          resolve();
          return;
        }

        const timeout = setTimeout(() => {
          this.successHandler.iceGatheringTimeout();
          // @ts-ignore
          this.pc!.onicegatheringstatechange = null;
          resolve();
        }, 2500);

        // @ts-ignore
        this.pc!.onicegatheringstatechange = () => {
          this.successHandler.iceGatheringStateChange(
            this.pc!.iceGatheringState
          );
          if (this.pc!.iceGatheringState === 'complete') {
            clearTimeout(timeout);
            // @ts-ignore
            this.pc!.onicegatheringstatechange = null;
            this.successHandler.iceGatheringComplete();
            resolve();
          }
        };
      } catch (e: any) {
        this.errorHandler.handle('ice_gathering', e, 'warning', true);
        resolve();
      }
    });
  }

  getPeerConnection() {
    return this.pc;
  }

  close() {
    if (this.pc) {
      try {
        this.pc.close();
      } catch {}
      this.pc = null;
    }
  }

  isConnected() {
    return !!this.pc && this.pc.connectionState === 'connected';
  }
}
export * from './DataChannelManager';
export * from './EventRouter';
export * from './MediaManager';
export * from './MessageSender';
export * from './OpenAIApiManager';
export * from './PeerConnectionManager';
import type { RTCPeerConnection } from 'react-native-webrtc';
import type RTCDataChannel from 'react-native-webrtc/lib/typescript/RTCDataChannel';
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';
import {
  ErrorHandler,
  SuccessHandler,
} from '@react-native-openai-realtime/handlers';

type MessageHandler = (message: any) => void | Promise<void>;

export class DataChannelManager {
  private dc: RTCDataChannel | null = null;
  private options: RealtimeClientOptionsBeforePrune;
  private errorHandler: ErrorHandler;
  private successHandler: SuccessHandler;
  private onMessage?: MessageHandler;

  constructor(
    options: RealtimeClientOptionsBeforePrune,
    errorHandler: ErrorHandler,
    successHandler: SuccessHandler
  ) {
    this.options = options;
    this.errorHandler = errorHandler;
    this.successHandler = successHandler;
  }

  create(pc: RTCPeerConnection, onMessage: MessageHandler): RTCDataChannel {
    try {
      // КРИТИЧНО: Проверяем, что PeerConnection не закрыт
      if (!pc || pc.connectionState === 'closed') {
        throw new Error('Cannot create DataChannel: PeerConnection is closed');
      }

      // Закрываем старый DataChannel если есть
      if (this.dc) {
        try {
          this.dc.close();
        } catch {}
        this.dc = null;
      }

      this.onMessage = onMessage;

      // @ts-ignore
      const dc = pc.createDataChannel(this.options.webrtc?.dataChannelLabel!, {
        ordered: true,
        maxRetransmits: 3,
      });

      this.dc = dc;
      this.setupListeners(dc);

      return dc;
    } catch (e: any) {
      this.errorHandler.handle('data_channel', e);
      throw e;
    }
  }

  private setupListeners(dc: RTCDataChannel) {
    // @ts-ignore
    dc.onopen = () => {
      try {
        this.successHandler.dataChannelOpen(dc);
        this.handleOpen();
      } catch (e: any) {
        this.errorHandler.handle('data_channel', e);
      }
    };

    // @ts-ignore
    dc.onmessage = async (message: any) => {
      try {
        const text =
          typeof message.data === 'string'
            ? message.data
            : String(message.data);
        let evt: any;

        try {
          evt = JSON.parse(text);
        } catch (err: any) {
          this.errorHandler.handle(
            'data_channel',
            err instanceof Error ? err : new Error(String(err)),
            'warning',
            true,
            {
              raw:
                typeof text === 'string' && text.length > 2000
                  ? text.slice(0, 2000) + '…'
                  : text,
              hint: 'Failed to JSON.parse DataChannel message',
            }
          );
          return;
        }

        this.successHandler.dataChannelMessage(evt);

        if (this.onMessage) {
          await this.onMessage(evt);
        }
      } catch (err: any) {
        this.errorHandler.handle('data_channel', err);
      }
    };

    // @ts-ignore
    dc.onclose = () => this.successHandler.dataChannelClose();

    // @ts-ignore
    dc.onerror = (error: any) =>
      this.errorHandler.handle('data_channel', error, 'warning', true);
  }

  private handleOpen() {
    // КРИТИЧНО: Проверяем готовность перед отправкой команд
    if (!this.dc || this.dc.readyState !== 'open') {
      this.options.logger?.warn?.(
        '[DataChannel] handleOpen called but channel not ready'
      );
      return;
    }

    // session.update (auto)
    if (this.options.autoSessionUpdate && this.options.session) {
      try {
        this.send({
          type: 'session.update',
          session: this.options.session,
        });
      } catch (e: any) {
        this.options.logger?.error?.(
          '[DataChannel] Failed to send session.update:',
          e
        );
      }
    }

    // greet
    if (this.options.greet?.enabled !== false) {
      try {
        const response = {
          instructions:
            this.options.greet?.response?.instructions ?? 'Привет! Я на связи.',
          modalities: this.options.greet?.response?.modalities ?? [
            'audio',
            'text',
          ],
        };
        this.send({ type: 'response.create', response });
      } catch (e: any) {
        this.options.logger?.error?.('[DataChannel] Failed to send greet:', e);
      }
    }

    this.options.hooks?.onOpen?.(this.dc);
  }

  send(event: any): void {
    if (!this.dc || this.dc.readyState !== 'open') {
      throw new Error('DataChannel is not open');
    }

    this.dc.send(JSON.stringify(event));
  }

  getDataChannel() {
    return this.dc;
  }

  close() {
    if (this.dc) {
      try {
        this.dc.close();
        this.successHandler.dataChannelClose();
      } catch {}
      this.dc = null;
    }
  }

  isOpen() {
    return !!this.dc && this.dc.readyState === 'open';
  }
}
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';
import { createDefaultRouter } from '@react-native-openai-realtime/helpers';
import type { RealtimeClientClass } from '@react-native-openai-realtime/components';

type Listener = (payload: any) => void;

export class EventRouter {
  private listeners = new Map<string, Set<Listener>>();
  private incomingRouter: (msg: any) => void | Promise<void>;
  private functionArgsBuffer = new Map<string, string>();
  private options: RealtimeClientOptionsBeforePrune;

  private sendRef: (e: any) => Promise<void>;
  private clientRef: any;

  constructor(
    options: RealtimeClientOptionsBeforePrune,
    sendRaw: (e: any) => Promise<void>,
    client?: any
  ) {
    this.options = options;
    this.sendRef = sendRaw;
    this.clientRef = client;

    this.incomingRouter = createDefaultRouter(
      this.emit.bind(this),
      options,
      this.functionArgsBuffer,
      sendRaw
    );
  }

  setContext(
    client: RealtimeClientClass | null,
    sendRaw: (e: any) => Promise<void>
  ) {
    this.clientRef = client;
    this.sendRef = sendRaw;
  }

  on(type: string, handler: Listener) {
    const set = this.listeners.get(type) ?? new Set<Listener>();
    set.add(handler);
    this.listeners.set(type, set);
    return () => set.delete(handler);
  }

  private emit(type: string, payload?: any) {
    const set = this.listeners.get(type);
    if (set) set.forEach((fn) => fn(payload));
  }

  async processIncomingMessage(evt: any) {
    // incoming middleware — теперь с реальным send и client
    if (this.options.middleware?.incoming) {
      for (const mw of this.options.middleware.incoming) {
        const res = await mw({
          event: evt,
          send: this.sendRef,
          client: this.clientRef,
        });
        if (res === 'stop') return;
        if (res && typeof res === 'object') {
          evt = res;
        }
      }
    }

    await this.incomingRouter(evt);
    // ВАЖНО: НЕ вызываем здесь hooks.onEvent — он уже вызывается внутри createDefaultRouter
  }

  cleanup() {
    this.listeners.clear();
    this.functionArgsBuffer.clear();
  }
}
import type {
  MiddlewareCtx,
  SpeechActivityState,
  Listener,
} from '@react-native-openai-realtime/types';

class SpeechActivityStore {
  private state: SpeechActivityState = {
    isUserSpeaking: false,
    isAssistantSpeaking: false,
    inputBuffered: false,
    outputBuffered: false,
    lastUserEventAt: null,
    lastAssistantEventAt: null,
  };
  private listeners = new Set<Listener>();

  get() {
    return this.state;
  }
  subscribe(fn: Listener) {
    this.listeners.add(fn);
    return () => this.listeners.delete(fn);
  }
  private emit() {
    const s = this.state;
    this.listeners.forEach((fn) => fn(s));
  }

  setUserSpeaking(v: boolean) {
    if (this.state.isUserSpeaking !== v) {
      this.state = {
        ...this.state,
        isUserSpeaking: v,
        lastUserEventAt: Date.now(),
      };
      this.emit();
    }
  }
  setAssistantSpeaking(v: boolean) {
    if (this.state.isAssistantSpeaking !== v) {
      this.state = {
        ...this.state,
        isAssistantSpeaking: v,
        lastAssistantEventAt: Date.now(),
      };
      this.emit();
    }
  }
  setInputBuffered(v: boolean) {
    if (this.state.inputBuffered !== v) {
      this.state = {
        ...this.state,
        inputBuffered: v,
        lastUserEventAt: Date.now(),
      };
      this.emit();
    }
  }
  setOutputBuffered(v: boolean) {
    if (this.state.outputBuffered !== v) {
      this.state = {
        ...this.state,
        outputBuffered: v,
        lastAssistantEventAt: Date.now(),
      };
      this.emit();
    }
  }
}

export const speechActivityStore = new SpeechActivityStore();
export type SpeechActivityStoreType = typeof speechActivityStore;
// Утилита: безопасное сравнение типа события

export function createSpeechActivityMiddleware(store = speechActivityStore) {
  return ({ event }: MiddlewareCtx) => {
    const t = event?.type;

    // Пользователь (как было)
    if (t === 'input_audio_buffer.speech_started' || t === 'speech_started') {
      store.setUserSpeaking(true);
      store.setAssistantSpeaking(false);
    }
    if (t === 'input_audio_buffer.speech_stopped' || t === 'speech_stopped') {
      store.setUserSpeaking(false);
      store.setInputBuffered(false);
    }
    if (
      t === 'input_audio_buffer.committed' ||
      t === 'input_audio_buffer.commit'
    ) {
      store.setInputBuffered(true);
      store.setUserSpeaking(true);
    }
    if (
      t === 'input_audio_buffer.cleared' ||
      t === 'input_audio_buffer.clear'
    ) {
      store.setInputBuffered(false);
    }

    // Ассистент (патч)
    if (t === 'output_audio_buffer.started') {
      store.setOutputBuffered(true);
      store.setAssistantSpeaking(true);
      store.setInputBuffered(false);
      store.setUserSpeaking(false);
    }
    // Патч: на .stopped и .cleared выключаем ассистентскую речь
    if (
      t === 'output_audio_buffer.stopped' ||
      t === 'output_audio_buffer.cleared'
    ) {
      store.setAssistantSpeaking(false);
      store.setOutputBuffered(false);
    }

    // Патч: на ответ отменён — тоже гасим
    if (t === 'response.canceled' || t === 'response.cancelled') {
      store.setAssistantSpeaking(false);
      store.setOutputBuffered(false);
    }

    return;
  };
}
// React-хук: подписка на store
export * from './speechMiddleware';
// src/adapters/ChatAdapter.ts
import {
  ChatAdapterOptions,
  ChatMsg,
} from '@react-native-openai-realtime/types';
import { RealtimeClientClass } from '@react-native-openai-realtime/components';
/**
 * Без дублирования: только подписка на встроенный ChatStore клиента.
 * Если chatStore отключён — возвращаем noop и ничего не делаем.
 */
export function attachChatAdapter(
  client: RealtimeClientClass, // RealtimeClientClass
  setChat: React.Dispatch<React.SetStateAction<ChatMsg[]>>,
  _opts?: ChatAdapterOptions
) {
  try {
    const initial =
      typeof client?.getChat === 'function' ? client.getChat() : [];
    if (Array.isArray(initial)) {
      setChat(initial);
    }
  } catch {
    // no-op
  }

  if (typeof client?.onChatUpdate === 'function') {
    const unsub = client.onChatUpdate((chat: ChatMsg[]) => setChat(chat));
    return () => {
      try {
        unsub?.();
      } catch {}
    };
  }

  // Fallback: если встроенного ChatStore нет — ничего не делаем
  return () => {};
}
export * from './ChatAdapter';
export * from './ChatStore';
// src/adapters/ChatStore.ts
import type { ChatMsg } from '@react-native-openai-realtime/types';

type Side = 'user' | 'assistant';
type Listener = (chat: ChatMsg[]) => void;

type StoreRow = {
  inChat: boolean; // уже есть сообщение в чате
  hasText: boolean; // был ли хоть какой-то текст
  buffer: string; // накопитель для дельт, если мы не показываем их сразу
};

type ChatStoreOptions = {
  isMeaningfulText?: (t: string) => boolean;

  // Управление поведением добавления в чат во время транскрипции
  userAddOnDelta?: boolean; // true — добавлять юзер-сообщение при первой дельте
  userPlaceholderOnStart?: boolean; // true — создать пустое сообщение на user:item_started
  assistantAddOnDelta?: boolean; // true — добавлять ассистентское сообщение при первой дельте
  assistantPlaceholderOnStart?: boolean; // true — создать пустое на assistant:response_started
};

export class ChatStore {
  private chat: ChatMsg[] = [];
  private listeners = new Set<Listener>();

  private seqRef = 0;
  private userOrderRef = new Map<string, number>();
  private respOrderRef = new Map<string, number>();

  private userState = new Map<string, StoreRow>();
  private assistantState = new Map<string, StoreRow>();

  private isMeaningful: (text: string) => boolean;

  private cfg: Required<Omit<ChatStoreOptions, 'isMeaningfulText'>> = {
    userAddOnDelta: true,
    userPlaceholderOnStart: false,
    assistantAddOnDelta: true,
    assistantPlaceholderOnStart: false,
  };

  constructor(opts?: ChatStoreOptions) {
    this.isMeaningful = opts?.isMeaningfulText ?? ((t: string) => !!t.trim());
    if (opts) {
      this.cfg = {
        ...this.cfg,
        userAddOnDelta: opts.userAddOnDelta ?? this.cfg.userAddOnDelta,
        userPlaceholderOnStart:
          opts.userPlaceholderOnStart ?? this.cfg.userPlaceholderOnStart,
        assistantAddOnDelta:
          opts.assistantAddOnDelta ?? this.cfg.assistantAddOnDelta,
        assistantPlaceholderOnStart:
          opts.assistantPlaceholderOnStart ??
          this.cfg.assistantPlaceholderOnStart,
      };
    }
  }

  get() {
    return this.chat;
  }

  subscribe(fn: Listener) {
    this.listeners.add(fn);
    return () => this.listeners.delete(fn);
  }

  private emit() {
    const snap = this.chat.slice();
    this.listeners.forEach((fn) => fn(snap));
  }

  private orderBy(side: Side) {
    return side === 'user' ? this.userOrderRef : this.respOrderRef;
  }

  private stateBy(side: Side) {
    return side === 'user' ? this.userState : this.assistantState;
  }

  private ensureOrder(side: Side, id: string) {
    const map = this.orderBy(side);
    if (!map.has(id)) {
      map.set(id, ++this.seqRef);
    }
    return map.get(id)!;
  }

  startUser(itemId: string) {
    // Регистрируем порядок
    this.userOrderRef.set(itemId, ++this.seqRef);

    const placeholder = this.cfg.userPlaceholderOnStart;
    this.userState.set(itemId, {
      inChat: placeholder,
      hasText: false,
      buffer: '',
    });

    if (placeholder) {
      const ts0 = this.userOrderRef.get(itemId)!;
      const now = Date.now();
      const msg: ChatMsg & { time: number } = {
        id: itemId,
        itemId,
        type: 'text',
        role: 'user',
        text: '',
        ts: ts0,
        time: now,
        status: 'streaming',
      };
      this.chat = [...this.chat, msg];
      this.emit();
    }
  }

  startAssistant(responseId: string) {
    this.respOrderRef.set(responseId, ++this.seqRef);

    const placeholder = this.cfg.assistantPlaceholderOnStart;
    this.assistantState.set(responseId, {
      inChat: placeholder,
      hasText: false,
      buffer: '',
    });

    if (placeholder) {
      const ts0 = this.respOrderRef.get(responseId)!;
      const now = Date.now();
      const msg: ChatMsg & { time: number } = {
        id: responseId,
        responseId,
        role: 'assistant',
        text: '',
        ts: ts0,
        type: 'text',
        time: now,
        status: 'streaming',
      };
      this.chat = [...this.chat, msg];
      this.emit();
    }
  }

  putDelta(side: Side, id: string, delta: string) {
    if (!id || !delta) return;

    const store = this.stateBy(side);
    const st = store.get(id) || { inChat: false, hasText: false, buffer: '' };
    // Убедимся, что есть порядковый ts
    const ts0 = this.orderBy(side).get(id) ?? this.ensureOrder(side, id);

    const addOnDelta =
      side === 'user' ? this.cfg.userAddOnDelta : this.cfg.assistantAddOnDelta;

    if (!st.inChat) {
      if (addOnDelta) {
        // Добавляем новое сообщение в чат при первой дельте
        const now = Date.now();
        const msg: ChatMsg & { time: number } = {
          id,
          type: 'text',
          itemId: side === 'user' ? id : undefined,
          responseId: side === 'assistant' ? id : undefined,
          role: side,
          text: (st.buffer || '') + delta,
          ts: ts0,
          time: now,
          status: 'streaming',
        };
        this.chat = [...this.chat, msg];
        st.inChat = true;
        st.hasText = true;
        st.buffer = ''; // сбрасываем буфер, так как теперь рендерим прямо в чате
      } else {
        // Не добавляем в чат — просто накапливаем буфер
        st.buffer += delta;
        st.hasText = true;
        // emit не делаем — в чате пока ничего не меняется
      }
    } else {
      // Сообщение уже в чате (плейсхолдер или ранее добавленное по дельте) — дописываем текст
      this.chat = this.chat.map((m) => {
        const match = side === 'user' ? m.itemId === id : m.responseId === id;
        return match ? { ...m, text: (m.text || '') + delta } : m;
      });
      st.hasText = true;
    }

    store.set(id, st);
    if (st.inChat) this.emit();
  }

  finalize(
    side: Side,
    id: string,
    status: 'done' | 'canceled',
    finalText?: string
  ) {
    const store = this.stateBy(side);
    const st = store.get(id); // сохраним ссылку, чтобы вытащить buffer
    store.delete(id);

    const idx = this.chat.findIndex((m) =>
      side === 'user' ? m.itemId === id : m.responseId === id
    );

    // Текст, который будем фиксировать:
    const buffered = st?.buffer ?? '';
    const textToUse = finalText ?? buffered;

    if (idx === -1) {
      if (this.isMeaningful(textToUse)) {
        const ts0 = this.orderBy(side).get(id) ?? Date.now();
        const now = Date.now();
        const msg: ChatMsg & { time: number } = {
          id,
          type: 'text',
          itemId: side === 'user' ? id : undefined,
          responseId: side === 'user' ? undefined : id,
          role: side,
          text: textToUse,
          ts: ts0,
          time: now,
          status: 'done',
        };
        this.chat = [...this.chat, msg];
        this.emit();
      }
      return;
    }

    // Сообщение уже в ленте — обновим финальный текст/статус
    const msg = this.chat[idx];
    // Если финального текста нет, а в чате был плейсхолдер — подставим буфер
    const mergedText = finalText ?? (msg?.text || '' || buffered);

    if (!this.isMeaningful(mergedText)) {
      const copy = [...this.chat];
      copy.splice(idx, 1);
      this.chat = copy;
      this.emit();
      return;
    }

    const copy = [...this.chat];
    copy[idx] = { ...msg!, text: mergedText, status };
    this.chat = copy;
    this.emit();
  }

  destroy() {
    this.listeners.clear();
    this.chat = [];
    this.userOrderRef.clear();
    this.respOrderRef.clear();
    this.userState.clear();
    this.assistantState.clear();
    this.seqRef = 0;
  }
}
import { ChatStore } from '@react-native-openai-realtime/adapters';
import {
  ErrorHandler,
  SuccessHandler,
} from '@react-native-openai-realtime/handlers';
import { applyDefaults } from '@react-native-openai-realtime/helpers';
import {
  PeerConnectionManager,
  MediaManager,
  DataChannelManager,
  MessageSender,
  EventRouter,
  OpenAIApiClient,
} from '@react-native-openai-realtime/managers';
import type {
  RealtimeClientOptionsBeforePrune,
  ResponseCreateParams,
  ResponseCreateStrict,
  TokenProvider,
  RealtimeEventMap,
  RealtimeEventListener,
} from '@react-native-openai-realtime/types';

type ConnectionState =
  | 'idle'
  | 'connecting'
  | 'connected'
  | 'disconnected'
  | 'error';
type ConnectionListener = (state: ConnectionState) => void;

export class RealtimeClientClass {
  private options: RealtimeClientOptionsBeforePrune;

  private connectionState: ConnectionState = 'idle';
  private connectionListeners = new Set<ConnectionListener>();

  private connecting = false;
  private disconnecting = false;

  private peerConnectionManager: PeerConnectionManager;
  private mediaManager: MediaManager;
  private dataChannelManager: DataChannelManager;
  private messageSender: MessageSender;
  private eventRouter: EventRouter;
  private apiClient: OpenAIApiClient;

  private errorHandler: ErrorHandler;
  private successHandler: SuccessHandler;

  private chatStore?: ChatStore;
  private chatWired = false;

  private connectSeq = 0;

  // Добавляем отслеживание состояния DataChannel
  private dataChannelReady = false;
  private peerConnectionConnected = false;

  constructor(
    userOptions: RealtimeClientOptionsBeforePrune,
    success?: SuccessHandler,
    error?: ErrorHandler
  ) {
    this.options = applyDefaults(userOptions);

    this.errorHandler =
      error ??
      new ErrorHandler(
        (event) => {
          if (event.severity === 'critical') {
            this.setConnectionState('error');
          }
          this.options.hooks?.onError?.(event);
        },
        { error: this.options.logger?.error }
      );

    const callbacks = {
      onPeerConnectionCreatingStarted: () => {
        this.peerConnectionConnected = false;
        this.dataChannelReady = false;
        this.setConnectionState('connecting');
      },
      onRTCPeerConnectionStateChange: (
        state:
          | 'new'
          | 'connecting'
          | 'connected'
          | 'disconnected'
          | 'failed'
          | 'closed'
      ) => {
        if (state === 'connected') {
          this.peerConnectionConnected = true;
          this.updateConnectionState();
        } else if (state === 'connecting' || state === 'new') {
          this.peerConnectionConnected = false;
          this.setConnectionState('connecting');
        } else if (state === 'failed') {
          this.peerConnectionConnected = false;
          this.dataChannelReady = false;
          this.setConnectionState('error');
        } else if (state === 'disconnected' || state === 'closed') {
          this.peerConnectionConnected = false;
          this.dataChannelReady = false;
          this.setConnectionState('disconnected');
        }
      },
      onDataChannelOpen: () => {
        this.dataChannelReady = true;
        this.updateConnectionState();
      },
      onDataChannelClose: () => {
        this.dataChannelReady = false;
        this.setConnectionState('disconnected');
      },
    };

    this.successHandler =
      success ?? new SuccessHandler(callbacks as any, undefined);

    this.peerConnectionManager = new PeerConnectionManager(
      this.options,
      this.errorHandler,
      this.successHandler
    );
    this.mediaManager = new MediaManager(
      this.options,
      this.errorHandler,
      this.successHandler
    );
    this.dataChannelManager = new DataChannelManager(
      this.options,
      this.errorHandler,
      this.successHandler
    );
    this.messageSender = new MessageSender(
      this.dataChannelManager,
      this.options,
      this.errorHandler
    );
    this.eventRouter = new EventRouter(
      this.options,
      this.sendRaw.bind(this),
      this
    );
    this.apiClient = new OpenAIApiClient(this.errorHandler);

    if (this.options.chat?.enabled !== false) {
      this.chatStore = new ChatStore({
        isMeaningfulText:
          this.options.chat?.isMeaningfulText ??
          this.options.policy?.isMeaningfulText,
        userAddOnDelta: this.options.chat?.userAddOnDelta,
        userPlaceholderOnStart: this.options.chat?.userPlaceholderOnStart,
        assistantAddOnDelta: this.options.chat?.assistantAddOnDelta,
        assistantPlaceholderOnStart:
          this.options.chat?.assistantPlaceholderOnStart,
      });
      this.wireChatStore();
    }
  }

  /**
   * Обновляет состояние подключения на основе состояния PeerConnection и DataChannel
   * Соединение считается полностью установленным только когда оба готовы
   */
  private updateConnectionState() {
    // Проверяем также readyState DataChannel для надежности
    const dc = this.dataChannelManager.getDataChannel();
    const dcActuallyOpen = dc && dc.readyState === 'open';

    if (
      this.peerConnectionConnected &&
      this.dataChannelReady &&
      dcActuallyOpen
    ) {
      this.setConnectionState('connected');
      this.options.logger?.info?.(
        '[RealtimeClient] ✅ Fully connected (PC + DC ready)'
      );
    } else if (this.peerConnectionConnected || this.dataChannelReady) {
      // Хотя бы одно из соединений установлено, но не оба
      this.options.logger?.debug?.(
        `[RealtimeClient] Partial connection (PC: ${this.peerConnectionConnected}, DC: ${this.dataChannelReady}, DC state: ${dc?.readyState})`
      );
      // Остаемся в connecting, пока не будут готовы оба
      if (this.connectionState !== 'connected') {
        this.setConnectionState('connecting');
      }
    }
  }

  setTokenProvider(tp: TokenProvider) {
    if (typeof tp !== 'function')
      throw new Error('setTokenProvider: invalid tokenProvider');
    this.options.tokenProvider = tp;
  }

  private setConnectionState(state: ConnectionState) {
    if (this.connectionState !== state) {
      this.connectionState = state;
      this.options.logger?.debug?.(`[RealtimeClient] Status changed: ${state}`);
      this.connectionListeners.forEach((listener) => listener(state));
    }
  }

  public getConnectionState(): ConnectionState {
    return this.connectionState;
  }

  public getStatus() {
    return this.connectionState;
  }

  /**
   * Возвращает true только если и PeerConnection и DataChannel полностью готовы
   */
  public isFullyConnected(): boolean {
    const dc = this.dataChannelManager.getDataChannel();
    return (
      this.connectionState === 'connected' &&
      this.peerConnectionConnected &&
      this.dataChannelReady &&
      !!dc &&
      dc.readyState === 'open'
    );
  }

  public onConnectionStateChange(listener: ConnectionListener) {
    this.connectionListeners.add(listener);
    return () => this.connectionListeners.delete(listener);
  }

  private wireChatStore(force = false) {
    if (!this.chatStore) return;
    if (this.chatWired && !force) return;

    this.on('user:item_started', ({ itemId }) =>
      this.chatStore!.startUser(itemId)
    );
    this.on('assistant:response_started', ({ responseId }) =>
      this.chatStore!.startAssistant(responseId)
    );
    this.on('user:delta', ({ itemId, delta }) =>
      this.chatStore!.putDelta('user', itemId, delta)
    );
    this.on('user:completed', ({ itemId, transcript }) =>
      this.chatStore!.finalize('user', itemId, 'done', transcript)
    );
    this.on('user:failed', ({ itemId }) =>
      this.chatStore!.finalize('user', itemId, 'done')
    );
    this.on('user:truncated', ({ itemId }) =>
      this.chatStore!.finalize('user', itemId, 'done')
    );
    this.on('assistant:delta', ({ responseId, delta }) =>
      this.chatStore!.putDelta('assistant', responseId, delta)
    );
    this.on('assistant:completed', ({ responseId, status }) =>
      this.chatStore!.finalize('assistant', responseId, status)
    );

    this.chatWired = true;
  }

  on<K extends keyof RealtimeEventMap>(
    type: K,
    handler: RealtimeEventListener<K>
  ): () => void;
  on(type: string, handler: (payload: any) => void): () => void;
  on(type: string, handler: (payload: any) => void): () => void {
    return this.eventRouter.on(type, handler);
  }

  private preConnectCleanup() {
    this.peerConnectionConnected = false;
    this.dataChannelReady = false;

    try {
      this.dataChannelManager.close();
    } catch {}
    try {
      this.peerConnectionManager.close();
    } catch {}
    try {
      this.mediaManager.cleanup();
    } catch {}
  }

  private makeAbortError() {
    const err: any = new Error('connect aborted');
    err.name = 'AbortError';
    err.__ABORT__ = true;
    return err;
  }

  private assertNotAborted(mySeq: number) {
    if (mySeq !== this.connectSeq || this.disconnecting) {
      throw this.makeAbortError();
    }
  }

  async enableMicrophone() {
    try {
      const pc = this.peerConnectionManager.getPeerConnection();
      if (!pc) throw new Error('PeerConnection not created');

      const stream = await this.mediaManager.getUserMedia();
      this.mediaManager.addLocalStreamToPeerConnection(pc, stream);
      try {
        const txs = (pc as any).getTransceivers?.() || [];
        const audioTx = txs.find(
          (t: any) =>
            t?.receiver?.track?.kind === 'audio' ||
            t?.sender?.track?.kind === 'audio'
        );
        const track = stream.getAudioTracks?.()[0];
        if (audioTx?.sender && track) {
          await audioTx.sender.replaceTrack(track);
          if (typeof audioTx.setDirection === 'function') {
            audioTx.setDirection('sendrecv');
          } else {
            // @ts-ignore
            audioTx.direction = 'sendrecv';
          }
        }
      } catch {}

      const offer = await this.peerConnectionManager.createOffer();
      await this.peerConnectionManager.setLocalDescription(offer);
      await this.peerConnectionManager.waitForIceGathering();

      const ephemeralKey = await this.options.tokenProvider();
      const answer = await this.apiClient.postSDP(offer.sdp, ephemeralKey);
      await this.peerConnectionManager.setRemoteDescription(answer);

      this.successHandler.microphonePermissionGranted?.();
      this.options.logger?.info?.(
        '[RealtimeClient] 🎤 Microphone enabled & renegotiated'
      );
    } catch (e: any) {
      this.errorHandler.handle('get_user_media', e, 'critical', false);
      throw e;
    }
  }

  public async disableMicrophone() {
    try {
      const pc = this.peerConnectionManager.getPeerConnection();
      if (!pc) throw new Error('PeerConnection not created');

      const local = this.mediaManager.getLocalStream();
      if (local?.getAudioTracks) {
        local.getAudioTracks().forEach((t: any) => {
          try {
            t.stop();
          } catch {}
        });
      }

      try {
        const txs = (pc as any).getTransceivers?.() || [];
        const audioTx = txs.find(
          (t: any) =>
            t?.sender?.track?.kind === 'audio' ||
            t?.receiver?.track?.kind === 'audio'
        );
        if (audioTx?.sender) {
          await audioTx.sender.replaceTrack(null);
        }
        if (audioTx) {
          if (typeof audioTx.setDirection === 'function') {
            audioTx.setDirection('recvonly');
          } else {
            // @ts-ignore
            audioTx.direction = 'recvonly';
          }
        }
      } catch {}

      const offer = await this.peerConnectionManager.createOffer();
      await this.peerConnectionManager.setLocalDescription(offer);
      await this.peerConnectionManager.waitForIceGathering();
      const ephemeralKey = await this.options.tokenProvider();
      const answer = await this.apiClient.postSDP(offer.sdp, ephemeralKey);
      await this.peerConnectionManager.setRemoteDescription(answer);

      this.mediaManager.stopLocalStream();

      this.options.logger?.info?.(
        '[RealtimeClient] 🔇 Microphone disabled & renegotiated'
      );
    } catch (e: any) {
      this.errorHandler.handle('local_stream', e, 'warning', true);
      throw e;
    }
  }

  async connect() {
    if (this.connecting) {
      this.errorHandler.handle(
        'init_peer_connection',
        new Error('connect() called while connecting'),
        'warning',
        true
      );
      return;
    }
    this.connecting = true;
    const mySeq = ++this.connectSeq;

    try {
      this.setConnectionState('connecting');
      this.preConnectCleanup();

      if (this.chatStore && !this.chatWired) {
        this.wireChatStore(true);
      }

      this.assertNotAborted(mySeq);

      let ephemeralKey: string;
      try {
        const fn = this.options.tokenProvider;
        if (typeof fn !== 'function')
          throw new Error('tokenProvider is not set');
        ephemeralKey = await fn();
        if (!ephemeralKey) throw new Error('Empty ephemeral token');
      } catch (e: any) {
        this.setConnectionState('error');
        this.errorHandler.handle('fetch_token', e, 'critical', false);
        throw e;
      }

      this.assertNotAborted(mySeq);

      const wantsAudioModality =
        Array.isArray(this.options.session?.modalities) &&
        this.options.session!.modalities!.includes('audio');
      const wantsTurnDetection = !!this.options.session?.turn_detection;
      const mustCaptureMic = wantsAudioModality || wantsTurnDetection;

      const shouldTryMic =
        mustCaptureMic || this.options.allowConnectWithoutMic === false;

      let localStream: any = null;
      let needsRecvOnlyTransceiver = false;

      if (shouldTryMic) {
        try {
          localStream = await this.mediaManager.getUserMedia();
          this.assertNotAborted(mySeq);
          this.options.logger?.info?.(
            '[RealtimeClient] ✅ Microphone permission granted'
          );
        } catch (e: any) {
          this.options.logger?.warn?.(
            '[RealtimeClient] ⚠️ Microphone permission denied:',
            e.message || e
          );

          if (this.options.allowConnectWithoutMic === false) {
            this.errorHandler.handle('get_user_media', e, 'critical', false);
            throw e;
          }

          this.errorHandler.handle('get_user_media', e, 'warning', true, {
            reason: 'Will use recvonly transceiver as fallback',
            allowConnectWithoutMic: true,
          });

          needsRecvOnlyTransceiver = true;
        }
      } else {
        this.options.logger?.info?.(
          '[RealtimeClient] 📝 Text mode - no microphone needed'
        );
        needsRecvOnlyTransceiver = true;
      }

      this.assertNotAborted(mySeq);

      const pc = this.peerConnectionManager.create();
      this.assertNotAborted(mySeq);

      this.mediaManager.setupRemoteStream(pc);

      if (localStream) {
        this.mediaManager.addLocalStreamToPeerConnection(pc, localStream);
        this.options.logger?.info?.(
          '[RealtimeClient] 🎤 Local microphone stream added to PeerConnection'
        );
      } else if (needsRecvOnlyTransceiver) {
        try {
          // @ts-ignore
          if (typeof pc.addTransceiver === 'function') {
            // @ts-ignore
            pc.addTransceiver('audio', { direction: 'recvonly' });
            this.successHandler.iosTransceiverSetted?.();
            this.options.logger?.info?.(
              '[RealtimeClient] 🔇 Added recvonly audio transceiver (no mic)'
            );
          } else {
            this.options.logger?.warn?.(
              '[RealtimeClient] ⚠️ addTransceiver not available, continuing anyway'
            );
          }
        } catch (e2: any) {
          this.errorHandler.handle('ios_transceiver', e2, 'info', true);
          this.options.logger?.warn?.(
            '[RealtimeClient] ⚠️ Could not add transceiver:',
            e2.message || e2
          );
        }
      }

      this.assertNotAborted(mySeq);

      this.dataChannelManager.create(pc, async (evt) => {
        await this.eventRouter.processIncomingMessage(evt);
      });

      const offer = await this.peerConnectionManager.createOffer();
      await this.peerConnectionManager.setLocalDescription(offer);
      await this.peerConnectionManager.waitForIceGathering();

      this.assertNotAborted(mySeq);

      const answer = await this.apiClient.postSDP(offer.sdp, ephemeralKey);
      await this.peerConnectionManager.setRemoteDescription(answer);

      this.options.logger?.info?.(
        '[RealtimeClient] 🎉 Connection process completed'
      );
    } catch (e: any) {
      if (e?.__ABORT__ || e?.name === 'AbortError') {
        this.options.logger?.info?.('[RealtimeClient] 🛑 Connection aborted');
        if (this.getStatus() !== 'disconnected') {
          this.setConnectionState('disconnected');
        }
      } else {
        this.options.logger?.error?.(
          '[RealtimeClient] ❌ Connection failed:',
          e.message || e
        );
        if (this.getStatus() !== 'error') {
          this.setConnectionState('error');
          this.errorHandler.handle('init_peer_connection', e);
        }
        throw e;
      }
    } finally {
      this.connecting = false;
    }
  }

  async disconnect() {
    if (this.disconnecting) return;
    this.disconnecting = true;

    this.connectSeq++;

    try {
      this.successHandler.hangUpStarted();

      try {
        this.dataChannelManager.close();
      } catch {}
      try {
        this.mediaManager.cleanup();
      } catch {}
      try {
        this.peerConnectionManager.close();
      } catch {}
      try {
        this.eventRouter.cleanup();
      } catch {}

      this.peerConnectionConnected = false;
      this.dataChannelReady = false;
      this.chatWired = false;

      if (
        this.options.deleteChatHistoryOnDisconnect !== false &&
        this.chatStore
      ) {
        try {
          this.options.logger?.debug?.(
            '[RealtimeClient] Destroying chat history'
          );
          this.chatStore.destroy();
        } catch {}
      } else {
        try {
          this.options.logger?.debug?.(
            '[RealtimeClient] Preserving chat history on disconnect'
          );
        } catch {}
      }

      this.setConnectionState('disconnected');
      this.successHandler.hangUpDone();
    } catch (e: any) {
      this.errorHandler.handle('hangup', e, 'warning', true);
    } finally {
      this.disconnecting = false;
    }
  }

  async sendRaw(event: any): Promise<void> {
    return this.messageSender.sendRaw(event);
  }

  sendResponse(): void;
  sendResponse(params: ResponseCreateParams): void;
  sendResponse(params?: any): void {
    this.messageSender.sendResponse(params);
  }

  sendResponseStrict(options: ResponseCreateStrict) {
    this.messageSender.sendResponseStrict(options);
  }

  updateSession(patch: Partial<any>) {
    this.messageSender.updateSession(patch);
  }

  sendToolOutput(call_id: string, output: any) {
    this.messageSender.sendToolOutput(call_id, output);
  }

  getPeerConnection() {
    return this.peerConnectionManager.getPeerConnection();
  }

  getDataChannel() {
    return this.dataChannelManager.getDataChannel();
  }

  getLocalStream() {
    return this.mediaManager.getLocalStream();
  }

  getRemoteStream() {
    return this.mediaManager.getRemoteStream();
  }

  getChat() {
    return this.chatStore?.get() ?? [];
  }

  public clearChatHistory() {
    if (this.chatStore) {
      this.options.logger?.info?.(
        '[RealtimeClient] Manually clearing chat history'
      );
      this.chatStore.destroy();
    }
  }

  onChatUpdate(handler: (chat: any[]) => void) {
    if (!this.chatStore) {
      return () => {};
    }
    return this.chatStore.subscribe(handler);
  }

  isConnected() {
    return this.isFullyConnected();
  }
}
import {
  AddableMessage,
  ChatMsg,
  CoreConfig,
  ExtendedChatMsg,
  RealtimeClientOptionsBeforePrune,
  RealTimeClientProps,
  RealtimeContextValue,
  TokenProvider,
} from '@react-native-openai-realtime/types';
import {
  useCallback,
  useEffect,
  useMemo,
  useRef,
  useState,
  forwardRef,
  useImperativeHandle,
} from 'react';
import { AppState, AppStateStatus } from 'react-native';
import { RealtimeClientClass } from '@react-native-openai-realtime/components';
import { attachChatAdapter } from '@react-native-openai-realtime/adapters';
import { RealtimeProvider } from '@react-native-openai-realtime/context';
import { prune } from '@react-native-openai-realtime/helpers';
import {
  SuccessHandler,
  ErrorHandler,
} from '@react-native-openai-realtime/handlers';

const makeId = () => `${Date.now()}-${Math.random().toString(36).slice(2, 8)}`;

export type RealTimeClientHandle = {
  enableMicrophone: () => Promise<void>;
  getClient: () => RealtimeClientClass | null;
  getStatus: () =>
    | 'idle'
    | 'connecting'
    | 'connected'
    | 'disconnected'
    | 'error';
  setTokenProvider: (tp: TokenProvider) => void;

  disableMicrophone: () => Promise<void>;
  connect: () => Promise<void>;
  disconnect: () => Promise<void>;

  sendRaw: (e: any) => Promise<void> | void;
  sendResponse: (opts?: any) => void;
  sendResponseStrict: (opts: {
    instructions: string;
    modalities?: Array<'audio' | 'text'>;
    conversation?: 'auto' | 'none';
  }) => void;
  updateSession: (patch: Partial<any>) => void;

  addMessage: (m: AddableMessage | AddableMessage[]) => string | string[];
  clearAdded: () => void;
  clearChatHistory: () => void;
  getNextTs: () => number;
};

export const RealTimeClient = forwardRef<
  RealTimeClientHandle,
  RealTimeClientProps
>((props, ref) => {
  const {
    tokenProvider,
    webrtc,
    media,
    chatInverted,
    session,
    autoSessionUpdate,
    greetEnabled,
    greetInstructions,
    greetModalities,
    onOpen,
    onEvent,
    onError,
    onUserTranscriptionDelta,
    onUserTranscriptionCompleted,
    onAssistantTextDelta,
    onAssistantCompleted,
    deleteChatHistoryOnDisconnect = true,
    onToolCall,
    incomingMiddleware,
    outgoingMiddleware,
    policyIsMeaningfulText,
    chatEnabled,
    chatIsMeaningfulText,
    logger,
    autoConnect = false,
    attachChat = true,
    children,
    chatUserAddOnDelta,
    chatUserPlaceholderOnStart,
    chatAssistantAddOnDelta,
    chatAssistantPlaceholderOnStart,

    allowConnectWithoutMic = true,

    // Success callbacks
    onHangUpStarted,
    onHangUpDone,
    onPeerConnectionCreatingStarted,
    onPeerConnectionCreated,
    onRTCPeerConnectionStateChange,
    onGetUserMediaSetted,
    onLocalStreamSetted,
    onLocalStreamAddedTrack,
    onLocalStreamRemovedTrack,
    onRemoteStreamSetted,
    onDataChannelOpen,
    onDataChannelMessage,
    onDataChannelClose,
    onIceGatheringComplete,
    onIceGatheringTimeout,
    onIceGatheringStateChange,
    onMicrophonePermissionGranted,
    onMicrophonePermissionDenied,
    onIOSTransceiverSetted,
    onSuccess,
  } = props;

  const clientRef = useRef<RealtimeClientClass | null>(null);
  const connectionUnsubRef = useRef<(() => void) | null>(null);
  const detachChatRef = useRef<null | (() => void)>(null);
  const mountedRef = useRef(false);
  const connectCalledRef = useRef(false);

  const [status, setStatus] = useState<
    'idle' | 'connecting' | 'connected' | 'disconnected' | 'error'
  >('idle');
  const [chat, setChat] = useState<ChatMsg[]>([]);
  const [addedMessages, setAddedMessages] = useState<ExtendedChatMsg[]>([]);

  const optionsSnapshot: CoreConfig = useMemo(() => {
    return prune({
      tokenProvider,
      webrtc,
      media,
      chatInverted,
      session,
      autoSessionUpdate,
      greet:
        greetEnabled !== undefined || greetInstructions || greetModalities
          ? {
              enabled: greetEnabled ?? true,
              response: {
                instructions: greetInstructions,
                modalities: greetModalities,
              },
            }
          : undefined,
      hooks: prune({
        onOpen,
        onEvent,
        onError,
        onUserTranscriptionDelta,
        onUserTranscriptionCompleted,
        onAssistantTextDelta,
        onAssistantCompleted,
        onToolCall,
      }) as any,
      middleware: prune({
        incoming: incomingMiddleware,
        outgoing: outgoingMiddleware,
      }) as any,
      policy: prune({ isMeaningfulText: policyIsMeaningfulText }),
      deleteChatHistoryOnDisconnect,
      chat: prune({
        enabled: chatEnabled,
        isMeaningfulText: chatIsMeaningfulText,
        userAddOnDelta: chatUserAddOnDelta,
        userPlaceholderOnStart: chatUserPlaceholderOnStart,
        assistantAddOnDelta: chatAssistantAddOnDelta,
        assistantPlaceholderOnStart: chatAssistantPlaceholderOnStart,
      }),
      logger,
      allowConnectWithoutMic,
    }) as CoreConfig;
  }, [
    tokenProvider,
    deleteChatHistoryOnDisconnect,
    webrtc,
    media,
    chatInverted,
    session,
    autoSessionUpdate,
    greetEnabled,
    greetInstructions,
    greetModalities,
    onOpen,
    onEvent,
    onError,
    onUserTranscriptionDelta,
    onUserTranscriptionCompleted,
    onAssistantTextDelta,
    onAssistantCompleted,
    onToolCall,
    incomingMiddleware,
    outgoingMiddleware,
    policyIsMeaningfulText,
    chatEnabled,
    chatIsMeaningfulText,
    logger,
    chatUserAddOnDelta,
    chatUserPlaceholderOnStart,
    chatAssistantAddOnDelta,
    chatAssistantPlaceholderOnStart,
    allowConnectWithoutMic,
  ]);

  // В файле: RealTimeClient.tsx
  // Замените часть с созданием successHandler/errorHandler:

  const ensureClient = useCallback(() => {
    if (!clientRef.current) {
      // ✅ ИСПРАВЛЕНО: ErrorHandler НЕ меняет статус для warning
      const errorHandler = new ErrorHandler(
        (event) => {
          // Меняем статус только для критических ошибок
          if (event.severity === 'critical') {
            setStatus('error');
          }
          // Вызываем пользовательский onError
          onError?.(event);
        },
        { error: logger?.error }
      );

      // Комбинированный SuccessHandler
      const successHandler = new SuccessHandler(
        {
          onPeerConnectionCreatingStarted: () => {
            setStatus('connecting');
            onPeerConnectionCreatingStarted?.();
          },
          onPeerConnectionCreated: (pc) => {
            onPeerConnectionCreated?.(pc);
          },
          onRTCPeerConnectionStateChange: (state) => {
            // Синхронизируем статус только для валидных состояний
            if (state === 'connected') setStatus('connected');
            else if (state === 'connecting' || state === 'new')
              setStatus('connecting');
            else if (state === 'failed') setStatus('error');
            else if (state === 'disconnected' || state === 'closed')
              setStatus('disconnected');

            onRTCPeerConnectionStateChange?.(state);
          },
          onDataChannelOpen: (channel) => {
            setStatus('connected');
            onDataChannelOpen?.(channel);
          },
          onDataChannelClose: () => {
            setStatus('disconnected');
            onDataChannelClose?.();
          },

          // Остальные callbacks - просто прокидываем
          onGetUserMediaSetted,
          onLocalStreamSetted,
          onLocalStreamAddedTrack,
          onLocalStreamRemovedTrack,
          onRemoteStreamSetted,
          onIceGatheringComplete,
          onIceGatheringTimeout,
          onIceGatheringStateChange,
          onMicrophonePermissionGranted,
          onMicrophonePermissionDenied,
          onIOSTransceiverSetted,
          onHangUpStarted,
          onHangUpDone,
          onDataChannelMessage,
        },
        onSuccess
      );

      clientRef.current = new RealtimeClientClass(
        optionsSnapshot as RealtimeClientOptionsBeforePrune,
        successHandler,
        errorHandler
      );

      setStatus(clientRef.current.getConnectionState());
      const unsub = clientRef.current.onConnectionStateChange((s) =>
        setStatus(s)
      );
      connectionUnsubRef.current = unsub;

      if (tokenProvider) {
        try {
          clientRef.current.setTokenProvider(tokenProvider);
        } catch {}
      }
    }
    return clientRef.current!;
  }, [
    optionsSnapshot,
    tokenProvider,
    onHangUpStarted,
    onHangUpDone,
    onPeerConnectionCreatingStarted,
    onPeerConnectionCreated,
    onRTCPeerConnectionStateChange,
    onGetUserMediaSetted,
    onLocalStreamSetted,
    onLocalStreamAddedTrack,
    onLocalStreamRemovedTrack,
    onRemoteStreamSetted,
    onDataChannelOpen,
    onDataChannelMessage,
    onDataChannelClose,
    onIceGatheringComplete,
    onIceGatheringTimeout,
    onIceGatheringStateChange,
    onMicrophonePermissionGranted,
    onMicrophonePermissionDenied,
    onIOSTransceiverSetted,
    onSuccess,
    onError,
    logger,
  ]);

  useEffect(() => {
    if (clientRef.current && tokenProvider) {
      try {
        clientRef.current.setTokenProvider(tokenProvider);
      } catch {}
    }
  }, [tokenProvider]);

  useEffect(() => {
    const onAppState = (state: AppStateStatus) => {
      if (state === 'background') {
        const currentStatus = clientRef.current?.getStatus?.();
        if (currentStatus === 'connecting') {
          setTimeout(() => {
            if (AppState.currentState === 'background') {
              clientRef.current?.disconnect().catch(() => {});
            }
          }, 1200);
        } else {
          clientRef.current?.disconnect().catch(() => {});
        }
      }
    };
    const sub = AppState.addEventListener('change', onAppState);
    return () => sub.remove();
  }, []);

  const connect = useCallback(async () => {
    const client = ensureClient();
    try {
      if (attachChat && !detachChatRef.current) {
        const isMeaningful =
          chatIsMeaningfulText ??
          policyIsMeaningfulText ??
          ((t: string) => !!t.trim());
        detachChatRef.current = attachChatAdapter(client, setChat, {
          isMeaningfulText: isMeaningful,
        });
      }
      await client.connect();
    } catch (e) {
      throw e;
    }
  }, [ensureClient, attachChat, chatIsMeaningfulText, policyIsMeaningfulText]);

  const disconnect = useCallback(async () => {
    const client = clientRef.current;
    if (!client) return;
    try {
      await client.disconnect();
    } finally {
      if (detachChatRef.current) {
        detachChatRef.current();
        detachChatRef.current = null;
      }
      if (deleteChatHistoryOnDisconnect) {
        setChat([]);
        setAddedMessages([]);
      }
    }
  }, [deleteChatHistoryOnDisconnect]);

  // Защита от многократного autoConnect
  useEffect(() => {
    mountedRef.current = true;
    return () => {
      mountedRef.current = false;
      connectCalledRef.current = false;
    };
  }, []);

  useEffect(() => {
    if (!autoConnect || !mountedRef.current || connectCalledRef.current) return;

    connectCalledRef.current = true;
    const t = setTimeout(() => {
      if (mountedRef.current) {
        connect().catch(() => {});
      }
    }, 50);

    return () => {
      clearTimeout(t);
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [autoConnect]);

  const getNextTs = useCallback((): number => {
    try {
      const chatMax =
        (chat?.length ? Math.max(...chat.map((m: any) => m?.ts ?? 0)) : 0) || 0;
      const addedMax =
        (addedMessages?.length
          ? Math.max(...addedMessages.map((m: any) => m?.ts ?? 0))
          : 0) || 0;
      const maxTs = Math.max(chatMax, addedMax);
      return maxTs > 0 ? maxTs + 1 : Date.now();
    } catch {
      return Date.now();
    }
  }, [chat, addedMessages]);

  const normalize = useCallback(
    (m: AddableMessage): ExtendedChatMsg => {
      const base = {
        id: (m as any).id ?? makeId(),
        role: (m as any).role ?? 'assistant',
        ts: (m as any).ts ?? getNextTs(),
        time: Date.now(),
      };

      if (
        (m as any).type === 'ui' ||
        ('kind' in (m as any) && 'payload' in (m as any))
      ) {
        return {
          ...base,
          type: 'ui',
          kind: (m as any).kind,
          payload: (m as any).payload,
        } as unknown as ExtendedChatMsg;
      }

      return {
        ...base,
        type: 'text',
        text: (m as any).text ?? '',
        status: 'done',
      } as unknown as ExtendedChatMsg;
    },
    [getNextTs]
  );

  const addMessage = useCallback(
    (m: AddableMessage | AddableMessage[]) => {
      const arr = Array.isArray(m) ? m : [m];
      const normalized = arr.map(normalize);
      setAddedMessages((prev) => [...prev, ...normalized]);
      const ids = normalized.map((x) => (x as any).id as string);
      return Array.isArray(m) ? ids : ids[0];
    },
    [normalize]
  );

  const clearAdded = useCallback(() => setAddedMessages([]), []);

  const mergedChat = useMemo<ExtendedChatMsg[]>(() => {
    const merged = [...(chat ?? []), ...addedMessages];
    return chatInverted
      ? merged.sort((a: any, b: any) => (a.ts ?? 0) - (b.ts ?? 0))
      : merged.sort((a: any, b: any) => (b.ts ?? 0) - (a.ts ?? 0));
  }, [chat, addedMessages, chatInverted]);

  const clearChatHistory = useCallback(() => {
    clientRef.current?.clearChatHistory();
  }, []);

  const value: RealtimeContextValue = useMemo(
    () => ({
      client: clientRef.current,
      status,
      clearChatHistory,
      chat: mergedChat,
      connect,
      disconnect,
      sendResponse: (opts?: any) => clientRef.current?.sendResponse(opts),
      sendResponseStrict: (opts: {
        instructions: string;
        modalities?: Array<'audio' | 'text'>;
        conversation?: 'auto' | 'none';
      }) => clientRef.current?.sendResponseStrict(opts),
      updateSession: (patch: Partial<any>) =>
        clientRef.current?.updateSession(patch),
      sendRaw: (e: any) => clientRef.current?.sendRaw(e),
      addMessage,
      clearAdded,
      getNextTs,
    }),
    [
      status,
      mergedChat,
      connect,
      disconnect,
      addMessage,
      clearAdded,
      clearChatHistory,
      getNextTs,
    ]
  );

  useImperativeHandle(
    ref,
    () => ({
      getClient: () => clientRef.current,
      enableMicrophone: async () =>
        await clientRef.current?.enableMicrophone?.(),
      disableMicrophone: async () =>
        await clientRef.current?.disableMicrophone?.(),
      getStatus: () => status,
      setTokenProvider: (tp: TokenProvider) => {
        try {
          clientRef.current?.setTokenProvider(tp);
        } catch {}
      },
      connect: async () => {
        await connect();
      },
      disconnect: async () => {
        await disconnect();
      },
      sendRaw: (e: any) => clientRef.current?.sendRaw(e),
      sendResponse: (opts?: any) => clientRef.current?.sendResponse(opts),
      sendResponseStrict: (opts) => clientRef.current?.sendResponseStrict(opts),
      updateSession: (patch) => clientRef.current?.updateSession(patch),

      addMessage,
      clearAdded,
      clearChatHistory,
      getNextTs,
    }),
    [
      status,
      connect,
      disconnect,
      addMessage,
      clearAdded,
      clearChatHistory,
      getNextTs,
    ]
  );

  const renderedChildren =
    typeof children === 'function' ? (children as any)(value) : children;

  return (
    <RealtimeProvider value={value}>
      {renderedChildren ?? null}
    </RealtimeProvider>
  );
});
export * from './RealtimeClientClass';
export * from './RealTimeClient';
import { RealtimeContext } from '@react-native-openai-realtime/context';
import { useContext } from 'react';
import type { RealtimeContextValue } from '@react-native-openai-realtime/types';

export function useRealtime(): RealtimeContextValue {
  // ✅ Получаем контекст
  const context = useContext(RealtimeContext);

  // ✅ Проверяем, что контекст существует
  if (!context) {
    throw new Error('useRealtime must be used within a RealtimeProvider');
  }

  return context;
}
import { useEffect, useRef, useState } from 'react';
import { useRealtime } from '@react-native-openai-realtime/hooks';
import type { RealtimeClientClass } from '@react-native-openai-realtime/components';

type Mode = 'server' | 'stats' | 'auto';

export type UseMicrophoneActivityOptions = {
  client?: RealtimeClientClass | null;
  mode?: Mode; // default: 'auto'
  silenceMs?: number; // таймаут без "дельт", после которого считаем тишину (по server-событиям)
  levelThreshold?: number; // порог срабатывания для stats-режима
  pollInterval?: number; // период опроса getStats
};

export function useMicrophoneActivity(opts?: UseMicrophoneActivityOptions) {
  const { client: ctxClient } = useRealtime();
  const client = opts?.client ?? (ctxClient as RealtimeClientClass | null);
  const [isMicActive, setIsMicActive] = useState(false);
  const [level, setLevel] = useState(0);
  const [isCapturing, setIsCapturing] = useState(false);

  const lastHeardRef = useRef<number>(0);
  const silenceMs = opts?.silenceMs ?? 600;
  const threshold = opts?.levelThreshold ?? 0.02;
  const pollInterval = opts?.pollInterval ?? 250;
  const mode = opts?.mode ?? 'auto';

  useEffect(() => {
    if (!client) return;

    // server-mode: активируем активность на дельты пользователя
    let unsubDelta: (() => void) | null = null;
    let unsubCompleted: (() => void) | null = null;
    let silenceTimer: any = null;

    const enableServer = mode === 'server' || mode === 'auto';
    if (enableServer) {
      unsubDelta = client.on('user:delta', () => {
        lastHeardRef.current = Date.now();
        setIsMicActive(true);
        if (silenceTimer) clearTimeout(silenceTimer);
        silenceTimer = setTimeout(() => {
          if (Date.now() - lastHeardRef.current >= silenceMs) {
            setIsMicActive(false);
          }
        }, silenceMs + 20);
      });
      unsubCompleted = client.on('user:completed', () => {
        if (silenceTimer) clearTimeout(silenceTimer);
        silenceTimer = setTimeout(() => setIsMicActive(false), silenceMs / 2);
      });
    }

    // stats-mode: опрашиваем audioLevel локального sender'а
    let poll: any = null;
    const enableStats = mode !== 'server'; // stats | auto
    if (enableStats) {
      poll = setInterval(async () => {
        try {
          const pc = client.getPeerConnection?.();
          if (!pc || !pc.getSenders) return;
          const senders = pc.getSenders();
          const audioSender = senders.find(
            (s: any) => s.track && s.track.kind === 'audio'
          );
          if (!audioSender || !audioSender.getStats) return;

          // заодно проверим "идёт ли захват"
          const localStream = client.getLocalStream?.();
          const capturing =
            !!localStream &&
            typeof localStream.getAudioTracks === 'function' &&
            localStream
              .getAudioTracks()
              .some((t: any) => t.enabled && t.readyState === 'live');
          setIsCapturing(capturing);

          const stats = await audioSender.getStats();
          let lvl = 0;
          stats.forEach((r: any) => {
            if (r.type === 'media-source' && typeof r.audioLevel === 'number') {
              lvl = Math.max(lvl, r.audioLevel);
            }
            if (r.type === 'track' && typeof r.audioLevel === 'number') {
              lvl = Math.max(lvl, r.audioLevel);
            }
            if (
              typeof r.totalAudioEnergy === 'number' &&
              typeof r.totalSamplesDuration === 'number' &&
              r.totalSamplesDuration > 0
            ) {
              const energy = r.totalAudioEnergy / r.totalSamplesDuration;
              lvl = Math.max(lvl, Math.min(1, Math.sqrt(energy)));
            }
            if (typeof r.audioInputLevel === 'number') {
              // старые реализации: 0..32767
              lvl = Math.max(lvl, Math.min(1, r.audioInputLevel / 32767));
            }
          });
          setLevel(lvl);

          if (mode === 'stats') {
            setIsMicActive(lvl > threshold);
          }
          if (mode === 'auto') {
            // если не было server-дэлт давно — ориентируемся на stats
            if (Date.now() - lastHeardRef.current > 2 * silenceMs) {
              setIsMicActive(lvl > threshold);
            }
          }
        } catch {
          // no-op
        }
      }, pollInterval);
    } else {
      // хотя бы проверим isCapturing
      const local = client.getLocalStream?.();
      const capturing =
        !!local &&
        typeof local.getAudioTracks === 'function' &&
        local
          .getAudioTracks()
          .some((t: any) => t.enabled && t.readyState === 'live');
      setIsCapturing(capturing);
    }

    return () => {
      if (unsubDelta) unsubDelta();
      if (unsubCompleted) unsubCompleted();
      if (silenceTimer) clearTimeout(silenceTimer);
      if (poll) clearInterval(poll);
    };
  }, [client, mode, silenceMs, threshold, pollInterval]);

  return { isMicActive, level, isCapturing };
}
import {
  speechActivityStore,
  SpeechActivityStoreType,
} from '@react-native-openai-realtime/middlewares';
import { useEffect, useState } from 'react';
import type { SpeechActivityState } from '@react-native-openai-realtime/types';

export function useSpeechActivity(
  store = speechActivityStore as SpeechActivityStoreType
) {
  const [state, setState] = useState<SpeechActivityState>(() => store.get());
  useEffect(() => store.subscribe(setState) as any, [store]);
  return state;
}
export * from './useRealtime';
export * from './useSpeechActivity';
export * from './useMicrophoneActivity';
export * from './useSessionOptions';
export * from './useSessionOptions';
import { useCallback, useEffect, useRef, useState } from 'react';
import InCallManager from 'react-native-incall-manager';
import { RealtimeClientClass } from '@react-native-openai-realtime/components';

const delay = (ms: number) => new Promise((r) => setTimeout(r, ms));

export const useSessionOptions = (client: RealtimeClientClass) => {
  const clientRef = useRef<RealtimeClientClass>(client);
  const lastResponseIdRef = useRef<string | null>(null);
  const [mode, setMode] = useState<'text' | 'voice'>('text');
  const [isModeReady, setIsModeReady] = useState<
    'idle' | 'connecting' | 'connected' | 'disconnected'
  >('idle');

  useEffect(() => {
    clientRef.current = client;
  }, [client]);

  useEffect(() => {
    const unsubscribe = subscribeToAssistantEvents(() => restartSpeakerRoute());
    return () => unsubscribe();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  const waitUntilDataChannelOpen = useCallback(async (timeoutMs = 5000) => {
    const start = Date.now();
    while (Date.now() - start < timeoutMs) {
      try {
        const dc = clientRef.current?.getDataChannel?.();
        if (dc && dc.readyState === 'open') {
          return true;
        }
      } catch {}
      await delay(100);
    }
    return false;
  }, []);

  const setRemoteTracksEnabled = useCallback((enabled: boolean) => {
    try {
      const remote = clientRef.current?.getRemoteStream?.();
      if (remote && typeof remote.getAudioTracks === 'function') {
        remote.getAudioTracks().forEach((t: any) => {
          t.enabled = enabled;
        });
      }
    } catch (e) {
      console.warn('⚠️ setRemoteTracksEnabled failed:', e);
    }
  }, []);

  const setMicrophoneEnabled = useCallback((enabled: boolean) => {
    try {
      const clientRefCurrent = clientRef.current;
      if (
        !clientRefCurrent ||
        typeof clientRefCurrent.getLocalStream !== 'function'
      ) {
        console.warn('⚠️ Client or getLocalStream not available');
        return;
      }

      const localStream = clientRefCurrent.getLocalStream?.();
      if (localStream) {
        console.log('🎤 localStream:', localStream);
        localStream.getAudioTracks().forEach((track: any) => {
          track.enabled = enabled;
          console.log(
            `🎤 Microphone track ${enabled ? 'enabled' : 'disabled'}`
          );
        });
      } else {
        console.warn('⚠️ No local stream available');
      }
    } catch (e) {
      console.warn('⚠️ setMicrophoneEnabled failed:', e);
    }
  }, []);

  const restartSpeakerRoute = useCallback(async () => {
    try {
      InCallManager.start({ media: 'audio', auto: false, ringback: '' });
      InCallManager.setSpeakerphoneOn(true);
      InCallManager.setForceSpeakerphoneOn(true);
    } catch (e) {
      console.warn('⚠️ restartSpeakerRoute failed:', e);
    }
  }, []);

  const subscribeToAssistantEvents = useCallback(
    (onAssistantStarted?: () => void) => {
      if (!clientRef.current?.on) return () => {};

      const off1 = clientRef.current.on(
        'assistant:response_started',
        ({ responseId }: any) => {
          lastResponseIdRef.current = responseId;
          setRemoteTracksEnabled(true);
          onAssistantStarted?.();
          console.log('🎤 Assistant started:', responseId);
        }
      );

      const off2 = clientRef.current.on(
        'assistant:completed',
        ({ responseId }: any) => {
          if (lastResponseIdRef.current === responseId) {
            lastResponseIdRef.current = null;
            console.log('✅ Assistant completed:', responseId);
          }
        }
      );

      return () => {
        try {
          off1?.();
          off2?.();
        } catch {}
      };
    },
    [setRemoteTracksEnabled]
  );

  const cancelAssistantNow = useCallback(
    async (onComplete?: () => void, onFail?: (err: any) => void) => {
      try {
        const chan = (clientRef.current as any)?.getDataChannel?.();
        if (!chan || chan.readyState !== 'open') return;

        const rid = lastResponseIdRef.current ?? undefined;

        InCallManager.stop();
        await (clientRef.current as any)?.sendRaw({
          type: 'response.cancel',
          ...(rid ? { response_id: rid } : {}),
        });
        await (clientRef.current as any)?.sendRaw({
          type: 'output_audio_buffer.clear',
        });
        setRemoteTracksEnabled(false);
        setMicrophoneEnabled(false);
        await delay(120);
        onComplete?.();
      } catch (e) {
        onFail?.(e);
      }
    },
    [setRemoteTracksEnabled, setMicrophoneEnabled]
  );

  const enforceTextSession = useCallback(async () => {
    try {
      console.log('📝 Switching to TEXT mode...');

      await cancelAssistantNow();

      // 2. Отключаем треки
      setRemoteTracksEnabled(false);
      setMicrophoneEnabled(false);

      // 3. Останавливаем InCallManager
      InCallManager.stop();

      // 4. Обновляем сессию на текстовый режим
      await clientRef.current?.sendRaw({
        type: 'session.update',
        session: {
          modalities: ['text'],
          turn_detection: null,
          input_audio_transcription: null,
        },
      });

      console.log('✅ TEXT mode activated');
    } catch (e) {
      console.error('❌ Failed to enforce text session:', e);
      throw new Error('Failed to enforce text session');
    }
  }, [cancelAssistantNow, setRemoteTracksEnabled, setMicrophoneEnabled]);

  const enforceVoiceSession = useCallback(async () => {
    try {
      console.log('🎤 Switching to VOICE mode...');

      await clientRef.current?.sendRaw({
        type: 'session.update',
        session: {
          modalities: ['audio', 'text'],
          turn_detection: {
            type: 'server_vad',
            threshold: 0.7,
            prefix_padding_ms: 500,
            silence_duration_ms: 1200,
          },
          input_audio_transcription: {
            model: 'whisper-1',
          },
        },
      });

      console.log('✅ Session updated to voice mode');

      // 2. Ждем немного, чтобы сервер применил изменения
      await delay(300);

      // 3. Запускаем speaker route
      await restartSpeakerRoute();

      // 4. Включаем треки
      setRemoteTracksEnabled(true);
      setMicrophoneEnabled(true);

      console.log('✅ VOICE mode activated');
    } catch (e) {
      console.error('❌ Failed to enforce voice session:', e);
      throw new Error('Failed to enforce voice session');
    }
  }, [restartSpeakerRoute, setRemoteTracksEnabled, setMicrophoneEnabled]);

  const initSession = useCallback(
    async (newMode: 'text' | 'voice') => {
      console.log(`🔄 Initializing session in ${newMode} mode...`);
      setIsModeReady('connecting');

      const dcOpened = await waitUntilDataChannelOpen(5000);
      if (!dcOpened) {
        setIsModeReady('disconnected');
        throw new Error('DataChannel not open');
      }

      try {
        if (newMode === 'text') {
          await enforceTextSession();
        } else {
          await enforceVoiceSession();
        }

        setMode(newMode);
        setIsModeReady('connected');
        console.log(`✅ Session initialized in ${newMode} mode`);
      } catch (e) {
        console.error('❌ Failed to init session:', e);
        setIsModeReady('disconnected');
        throw new Error('Failed to init session');
      }
    },
    [waitUntilDataChannelOpen, enforceTextSession, enforceVoiceSession]
  );

  const handleSendMessage = useCallback(async (text: string) => {
    if (!text.trim()) {
      console.warn('⚠️ Empty message');
      throw new Error('Empty message');
    }

    const dc = clientRef.current?.getDataChannel?.();
    if (!dc || dc.readyState !== 'open') {
      console.warn('⚠️ DataChannel not open');
      throw new Error('DataChannel not open');
    }

    try {
      await clientRef.current?.sendRaw({
        type: 'conversation.item.create',
        item: {
          type: 'message',
          role: 'user',
          content: [{ type: 'input_text', text }],
        },
      });

      await clientRef.current?.sendRaw({
        type: 'response.create',
        response: {
          modalities: ['text'],
        },
      });

      console.log('✅ Message sent');
    } catch (e) {
      console.error('❌ Failed to send message:', e);
      throw new Error('Failed to send message');
    }
  }, []);

  return {
    subscribeToAssistantEvents,
    handleSendMessage,
    initSession,
    isModeReady,
    mode,
    cancelAssistantNow,
  };
};
export * from './adapters';
export * from './components';
export * from './constants';
export * from './context';
export * from './handlers';
export * from './helpers';
export * from './hooks';
export * from './managers';
export * from './middlewares';
export * from './types';
import type { SuccessCallbacks } from '@react-native-openai-realtime/types';
import type { RTCPeerConnection, MediaStreamTrack } from 'react-native-webrtc';
import { MediaStream } from 'react-native-webrtc';
import type RTCDataChannel from 'react-native-webrtc/lib/typescript/RTCDataChannel';

export class SuccessHandler {
  private callbacks: SuccessCallbacks;
  private onSuccess?: (stage: string, data?: any) => void;

  constructor(
    callbacks: SuccessCallbacks = {},
    onSuccess?: (stage: string, data?: any) => void
  ) {
    this.callbacks = callbacks;
    this.onSuccess = onSuccess;
  }

  hangUpStarted() {
    this.onSuccess?.('hang_up_started');
    this.callbacks.onHangUpStarted?.();
  }
  hangUpDone() {
    this.onSuccess?.('hang_up_done');
    this.callbacks.onHangUpDone?.();
  }

  peerConnectionCreatingStarted() {
    this.onSuccess?.('peer_connection_creating_started');
    this.callbacks.onPeerConnectionCreatingStarted?.();
  }
  peerConnectionCreated(pc: RTCPeerConnection) {
    this.onSuccess?.('peer_connection_created', { pc });
    this.callbacks.onPeerConnectionCreated?.(pc as any);
  }

  rtcPeerConnectionStateChange(
    state:
      | 'new'
      | 'connecting'
      | 'connected'
      | 'disconnected'
      | 'failed'
      | 'closed'
  ) {
    this.onSuccess?.('rtc_peer_connection_state_change', { state });
    this.callbacks.onRTCPeerConnectionStateChange?.(state);
  }

  getUserMediaSetted(stream: MediaStream) {
    this.onSuccess?.('get_user_media_setted', { stream });
    this.callbacks.onGetUserMediaSetted?.(stream as any);
  }
  localStreamSetted(stream: MediaStream) {
    this.onSuccess?.('local_stream_setted', { stream });
    this.callbacks.onLocalStreamSetted?.(stream as any);
  }
  localStreamAddedTrack(track: MediaStreamTrack) {
    this.onSuccess?.('local_stream_added_track', { track });
    this.callbacks.onLocalStreamAddedTrack?.(track as any);
  }
  localStreamRemovedTrack(track: MediaStreamTrack) {
    this.onSuccess?.('local_stream_removed_track', { track });
    this.callbacks.onLocalStreamRemovedTrack?.(track as any);
  }
  remoteStreamSetted(stream: MediaStream) {
    this.onSuccess?.('remote_stream_setted', { stream });
    this.callbacks.onRemoteStreamSetted?.(stream as any);
  }

  dataChannelOpen(channel: RTCDataChannel) {
    this.onSuccess?.('data_channel_open', { channel });
    this.callbacks.onDataChannelOpen?.(channel);
  }
  dataChannelMessage(message: any) {
    this.onSuccess?.('data_channel_message', { message });
    this.callbacks.onDataChannelMessage?.(message);
  }
  dataChannelClose() {
    this.onSuccess?.('data_channel_close');
    this.callbacks.onDataChannelClose?.();
  }

  iceGatheringComplete() {
    this.onSuccess?.('ice_gathering_complete');
    this.callbacks.onIceGatheringComplete?.();
  }
  iceGatheringTimeout() {
    this.onSuccess?.('ice_gathering_timeout');
    this.callbacks.onIceGatheringTimeout?.();
  }
  iceGatheringStateChange(state: string) {
    this.onSuccess?.('ice_gathering_state_change', { state });
    this.callbacks.onIceGatheringStateChange?.(state);
  }

  microphonePermissionGranted() {
    this.onSuccess?.('microphone_permission_granted');
    this.callbacks.onMicrophonePermissionGranted?.();
  }
  microphonePermissionDenied() {
    this.onSuccess?.('microphone_permission_denied');
    this.callbacks.onMicrophonePermissionDenied?.();
  }

  iosTransceiverSetted() {
    this.onSuccess?.('ios_transceiver_setted');
    this.callbacks.onIOSTransceiverSetted?.();
  }
}
export * from './error';
export * from './success';
// Файл: handlers/error.ts
// Проверьте что у вас ИМЕННО так:

import type {
  ErrorEvent,
  ErrorStage,
  ErrorSeverity,
} from '@react-native-openai-realtime/types';

export class ErrorHandler {
  private onError?: (event: ErrorEvent) => void;
  private logger?: { error?: (...a: any[]) => void };

  constructor(
    onError?: (event: ErrorEvent) => void,
    logger?: { error?: (...a: any[]) => void }
  ) {
    this.onError = onError;
    this.logger = logger;
  }

  handle(
    stage: ErrorStage,
    error: Error | string,
    severity: ErrorSeverity = 'critical',
    recoverable: boolean = false,
    context?: Record<string, any>
  ) {
    const errorEvent: ErrorEvent = {
      stage,
      error: error instanceof Error ? error : new Error(String(error)),
      severity,
      recoverable,
      timestamp: Date.now(),
      context,
    };

    (this.logger?.error ?? console.error)(
      `[${stage}] ${severity}:`,
      errorEvent.error,
      context
    );

    // ✅ ВАЖНО: Вызываем onError только ПОСЛЕ логирования
    // onError НЕ должен менять connectionState если severity !== 'critical'
    if (this.onError) {
      this.onError(errorEvent);
    }

    return errorEvent;
  }
}
export function prune<T extends Record<string, any>>(
  obj?: T
): Partial<T> | undefined {
  if (!obj) return undefined;
  const out: any = {};
  for (const k of Object.keys(obj)) {
    const v = obj[k];
    if (v === undefined) continue;
    if (v && typeof v === 'object' && !Array.isArray(v)) {
      const child = prune(v);
      if (child && Object.keys(child).length > 0) out[k] = child;
    } else {
      out[k] = v;
    }
  }
  return Object.keys(out).length ? out : undefined;
}
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';
import { deepMerge } from '@react-native-openai-realtime/helpers';
import { DEFAULTS } from '@react-native-openai-realtime/constants';

export function applyDefaults(
  user: RealtimeClientOptionsBeforePrune
): RealtimeClientOptionsBeforePrune {
  const merged = deepMerge<RealtimeClientOptionsBeforePrune>(DEFAULTS, user);
  // Если greet.enabled true, а instructions не заданы — подставим дефолт
  if (
    merged.greet?.enabled &&
    (!merged.greet.response || !merged.greet.response.instructions)
  ) {
    merged.greet = {
      enabled: true,
      response: {
        instructions: DEFAULTS.greet!.response!.instructions!,
        modalities: DEFAULTS.greet!.response!.modalities,
      },
    };
  }
  return merged;
}
export const VOICE_IDS = [
  'alloy',
  'ash',
  'ballad',
  'coral',
  'echo',
  'sage',
  'shimmer',
  'verse',
] as const;

export type VoiceId = (typeof VOICE_IDS)[number];
// src/helpers/createDefaultRouter.ts
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';

type Emitter = (type: string, payload?: any) => void;

// Достаём input_text из созданного user item (typed-ввод)
function extractInputTextFromItem(item: any): string | null {
  try {
    const content = item?.content;
    if (!Array.isArray(content)) return null;
    for (const c of content) {
      if (!c || typeof c !== 'object') continue;
      if (c.type === 'input_text' && typeof c.text === 'string') {
        return c.text;
      }
      if (c.type === 'text' && typeof c.text === 'string') {
        return c.text;
      }
    }
    return null;
  } catch {
    return null;
  }
}

export function createDefaultRouter(
  emit: Emitter,
  options: RealtimeClientOptionsBeforePrune,
  functionArgsBuffer: Map<string, string>,
  sendRaw: (e: any) => void
) {
  return async function route(msg: any) {
    const hooks = options.hooks;
    hooks?.onEvent?.(msg);

    // ✅ ИСПРАВЛЕНО: Обработка создания пользовательского item
    if (msg.type === 'conversation.item.created') {
      const item = msg.item;
      const itemId = item?.id;

      // User item
      if (item?.role === 'user' && itemId) {
        emit('user:item_started', { itemId });

        // Если это текстовый ввод - сразу завершаем
        const typed = extractInputTextFromItem(item);
        if (typed && String(typed).trim()) {
          emit('user:completed', { itemId, transcript: String(typed) });
        }
      }

      // Assistant item (для совместимости)
      if (item?.role === 'assistant' && itemId) {
        emit('assistant:item_started', { itemId });
      }

      return;
    }

    // Ассистент начал ответ
    if (msg.type === 'response.created') {
      const responseId = msg.response?.id || msg.response_id;
      if (responseId) {
        emit('assistant:response_started', { responseId });
      }
      return;
    }

    // ✅ ИСПРАВЛЕНО: Обработка ВСЕХ типов текстовых дельт от ассистента
    if (msg.type === 'response.text.delta') {
      const responseId = msg.response_id;
      const delta = msg.delta || '';
      const consumed = hooks?.onAssistantTextDelta?.({
        responseId,
        delta,
        channel: 'output_text',
      });
      if (consumed !== 'consume') {
        emit('assistant:delta', { responseId, delta, channel: 'output_text' });
      }
      return;
    }

    // ✅ ДОБАВЛЕНО: response.output_item.text.delta
    if (msg.type === 'response.output_item.text.delta') {
      const responseId = msg.response_id;
      const delta = msg.delta || '';
      const consumed = hooks?.onAssistantTextDelta?.({
        responseId,
        delta,
        channel: 'output_text',
      });
      if (consumed !== 'consume') {
        emit('assistant:delta', { responseId, delta, channel: 'output_text' });
      }
      return;
    }

    // Для голосового режима - audio transcript
    if (msg.type === 'response.audio_transcript.delta') {
      const responseId = msg.response_id;
      const delta = msg.delta || '';
      const consumed = hooks?.onAssistantTextDelta?.({
        responseId,
        delta,
        channel: 'audio_transcript',
      });
      if (consumed !== 'consume') {
        emit('assistant:delta', {
          responseId,
          delta,
          channel: 'audio_transcript',
        });
      }
      return;
    }

    // ✅ ДОБАВЛЕНО: response.output_text.delta (альтернативный формат)
    if (msg.type === 'response.output_text.delta') {
      const responseId = msg.response_id;
      const delta = msg.delta || '';
      const consumed = hooks?.onAssistantTextDelta?.({
        responseId,
        delta,
        channel: 'output_text',
      });
      if (consumed !== 'consume') {
        emit('assistant:delta', { responseId, delta, channel: 'output_text' });
      }
      return;
    }

    // Транскрипция пользователя (аудио → текст)
    if (msg.type === 'conversation.item.input_audio_transcription.delta') {
      const itemId = msg.item_id;
      const delta = msg.delta || '';
      const consumed = hooks?.onUserTranscriptionDelta?.({ itemId, delta });
      if (consumed !== 'consume') {
        emit('user:delta', { itemId, delta });
      }
      return;
    }

    if (msg.type === 'conversation.item.input_audio_transcription.completed') {
      const itemId = msg.item_id;
      const transcript = msg.transcript || '';
      const consumed = hooks?.onUserTranscriptionCompleted?.({
        itemId,
        transcript,
      });
      if (consumed !== 'consume') {
        emit('user:completed', { itemId, transcript });
      }
      return;
    }

    if (msg.type === 'conversation.item.input_audio_transcription.failed') {
      const itemId = msg.item_id;
      emit('user:failed', { itemId, error: msg.error });
      return;
    }

    if (msg.type === 'conversation.item.truncated') {
      const itemId = msg.item_id || msg.item?.id;
      emit('user:truncated', { itemId });
      return;
    }

    // ✅ ИСПРАВЛЕНО: Завершение текстового ответа
    if (msg.type === 'response.text.done') {
      const responseId = msg.response_id;
      const consumed = hooks?.onAssistantCompleted?.({
        responseId,
        status: 'done',
      });
      if (consumed !== 'consume') {
        emit('assistant:completed', { responseId, status: 'done' });
      }
      return;
    }

    // ✅ ДОБАВЛЕНО: response.output_item.text.done
    if (msg.type === 'response.output_item.text.done') {
      const responseId = msg.response_id;
      const consumed = hooks?.onAssistantCompleted?.({
        responseId,
        status: 'done',
      });
      if (consumed !== 'consume') {
        emit('assistant:completed', { responseId, status: 'done' });
      }
      return;
    }

    // Завершение/отмена ответа ассистента
    if (msg.type === 'response.done' || msg.type === 'response.completed') {
      const responseId = msg.response_id || msg.response?.id;
      const consumed = hooks?.onAssistantCompleted?.({
        responseId,
        status: 'done',
      });
      if (consumed !== 'consume') {
        emit('assistant:completed', { responseId, status: 'done' });
      }
      return;
    }

    if (msg.type === 'response.cancelled' || msg.type === 'response.canceled') {
      const responseId = msg.response_id || msg.response?.id;
      const consumed = hooks?.onAssistantCompleted?.({
        responseId,
        status: 'canceled',
      });
      if (consumed !== 'consume') {
        emit('assistant:completed', { responseId, status: 'canceled' });
      }
      return;
    }

    if (msg.type === 'response.output_text.done') {
      const responseId = msg.response_id || msg.response?.id;
      const consumed = hooks?.onAssistantCompleted?.({
        responseId,
        status: 'done',
      });
      if (consumed !== 'consume') {
        emit('assistant:completed', { responseId, status: 'done' });
      }
      return;
    }

    if (msg.type === 'response.audio_transcript.done') {
      const responseId = msg.response_id;
      const consumed = hooks?.onAssistantCompleted?.({
        responseId,
        status: 'done',
      });
      if (consumed !== 'consume') {
        emit('assistant:completed', { responseId, status: 'done' });
      }
      return;
    }

    // Вызовы инструментов
    if (msg.type === 'response.function_call_arguments.delta') {
      const prev = functionArgsBuffer.get(msg.call_id) || '';
      functionArgsBuffer.set(msg.call_id, prev + (msg.delta || ''));
      emit('tool:call_delta', {
        call_id: msg.call_id,
        name: msg.name,
        delta: msg.delta || '',
      });
      return;
    }

    if (msg.type === 'response.function_call_arguments.done') {
      try {
        const argsStr = functionArgsBuffer.get(msg.call_id) || '{}';
        functionArgsBuffer.delete(msg.call_id);
        const args = JSON.parse(argsStr);

        emit('tool:call_done', { call_id: msg.call_id, name: msg.name, args });

        if (options.hooks?.onToolCall) {
          const output = await options.hooks.onToolCall({
            name: msg.name,
            args,
            call_id: msg.call_id,
          });
          if (output !== undefined) {
            sendRaw({
              type: 'conversation.item.create',
              item: {
                type: 'function_call_output',
                call_id: msg.call_id,
                output: JSON.stringify(output),
              },
            });
            sendRaw({ type: 'response.create' });
          }
        }
      } catch (e) {
        emit('error', { scope: 'tool', error: e });
      }
      return;
    }

    // ✅ ИСПРАВЛЕНО: Обработка серверных ошибок
    if (msg.type === 'error') {
      emit('error', { scope: 'server', error: msg.error });
      // НЕ вызываем hooks.onError с серверной ошибкой - это вызовет проблемы
      // hooks?.onError?.(msg.error);
      return;
    }

    // ✅ ДОБАВЛЕНО: Логирование необработанных событий для отладки
    if (options.logger?.debug) {
      const knownTypes = [
        'session.created',
        'session.updated',
        'input_audio_buffer.committed',
        'input_audio_buffer.cleared',
        'input_audio_buffer.speech_started',
        'input_audio_buffer.speech_stopped',
        'output_audio_buffer.started',
        'output_audio_buffer.stopped',
        'output_audio_buffer.cleared',
        'rate_limits.updated',
      ];
      if (!knownTypes.includes(msg.type)) {
        options.logger.debug(`[Router] Unhandled event: ${msg.type}`, msg);
      }
    }
  };
}
export function deepMerge<T>(base: Partial<T>, patch?: Partial<T>): T {
  const out: any = Array.isArray(base)
    ? [...(base as any)]
    : { ...(base as any) };
  if (!patch) {
    return out;
  }
  for (const [k, v] of Object.entries(patch as any)) {
    if (v && typeof v === 'object' && !Array.isArray(v)) {
      out[k] = deepMerge(out[k] ?? {}, v);
    } else {
      out[k] = v;
    }
  }
  return out;
}
export * from './applyDefaults';
export * from './createDefaultRouter';
export * from './deepMerge';
export * from './prune';
export * from './voice';
