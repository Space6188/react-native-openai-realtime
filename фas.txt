import type {
  RTCPeerConnection,
  MediaStreamTrack,
  MediaStream,
} from 'react-native-webrtc';
import type RTCDataChannel from 'react-native-webrtc/lib/typescript/RTCDataChannel';
export interface SuccessCallbacks {
  onHangUpStarted?: () => void;
  onHangUpDone?: () => void;

  onPeerConnectionCreatingStarted?: () => void;
  onPeerConnectionCreated?: (pc: RTCPeerConnection) => void;
  onRTCPeerConnectionStateChange?: (
    state:
      | 'new'
      | 'connecting'
      | 'connected'
      | 'disconnected'
      | 'failed'
      | 'closed'
  ) => void;

  onGetUserMediaSetted?: (stream: MediaStream) => void;
  onLocalStreamSetted?: (stream: MediaStream) => void;
  onLocalStreamAddedTrack?: (track: MediaStreamTrack) => void;
  onLocalStreamRemovedTrack?: (track: MediaStreamTrack) => void;
  onRemoteStreamSetted?: (stream: MediaStream) => void;

  onDataChannelOpen?: (channel: RTCDataChannel) => void;
  onDataChannelMessage?: (message: any) => void;
  onDataChannelClose?: () => void;

  onIceGatheringComplete?: () => void;
  onIceGatheringTimeout?: () => void;
  onIceGatheringStateChange?: (state: string) => void;

  onMicrophonePermissionGranted?: () => void;
  onMicrophonePermissionDenied?: () => void;

  onIOSTransceiverSetted?: () => void;
}

export interface BaseProps extends SuccessCallbacks {
  onError?: (event: ErrorEvent) => void;
  onSuccess?: (stage: string, data?: any) => void;
}
export type SpeechActivityState = {
  isUserSpeaking: boolean;
  isAssistantSpeaking: boolean;
  inputBuffered: boolean;
  outputBuffered: boolean;
  lastUserEventAt: number | null;
  lastAssistantEventAt: number | null;
};

export type Listener = (s: SpeechActivityState) => void;
export type MediaTrackConstraints = {
  width?: ConstrainNumber;
  height?: ConstrainNumber;
  frameRate?: ConstrainNumber;
  facingMode?: ConstrainString;
  deviceId?: ConstrainString;
  groupId?: ConstrainString;
};
type ConstrainNumber =
  | number
  | {
      exact?: number;
      ideal?: number;
      max?: number;
      min?: number;
    };
type ConstrainString =
  | string
  | {
      exact?: string;
      ideal?: string;
    };

export interface Constraints {
  audio?: boolean | MediaTrackConstraints;
  video?: boolean | MediaTrackConstraints;
}
export type ErrorStage =
  | 'hangup'
  | 'ice_gathering'
  | 'peer_connection'
  | 'microphone_permission'
  | 'remote_stream'
  | 'local_stream'
  | 'data_channel'
  | 'get_user_media'
  | 'ios_transceiver'
  | 'init_peer_connection'
  | 'create_offer'
  | 'set_local_description'
  | 'set_remote_description'
  | 'fetch_token'
  | 'openai_api';

export type ErrorSeverity = 'critical' | 'warning' | 'info';

export interface ErrorEvent {
  stage: ErrorStage;
  error: Error;
  severity: ErrorSeverity;
  recoverable: boolean;
  timestamp: number;
  context?: Record<string, any>;
}
import { AddableMessage, ExtendedChatMsg } from './Chat';
import type { RealtimeStatus } from '@react-native-openai-realtime/types';
import type { RealtimeClientClass } from '@react-native-openai-realtime/components';

export type RealtimeContextValue = {
  client: RealtimeClientClass | null;
  status: RealtimeStatus;
  clearChatHistory: () => void;
  chat: ExtendedChatMsg[];
  connect: () => Promise<void>;
  disconnect: () => Promise<void> | void;
  sendResponse: (opts?: any) => void;
  sendResponseStrict: (opts: {
    instructions: string;
    modalities?: Array<'audio' | 'text'>;
    conversation?: 'auto' | 'none';
  }) => void;
  updateSession: (patch: Partial<any>) => void;
  sendRaw: (event: any) => void;
  addMessage: (m: AddableMessage | AddableMessage[]) => string | string[];
  clearAdded: () => void;

  getNextTs: () => number;
};
export type ChatAdapterOptions = {
  isMeaningfulText?: (text: string) => boolean;
};

export type ChatOptions = {
  enabled?: boolean; // по умолчанию true — встроенный чат-стор включён
  isMeaningfulText?: (text: string) => boolean;
  userAddOnDelta?: boolean;
  userPlaceholderOnStart?: boolean;
  assistantAddOnDelta?: boolean;
  assistantPlaceholderOnStart?: boolean;
};

export type ChatMsg = {
  id: string;
  type: 'text' | 'ui';
  time: number;
  role: 'user' | 'assistant';
  text?: string;
  ts: number;
  status: 'streaming' | 'done' | 'canceled';
  responseId?: string;
  itemId?: string;
};

export type UIChatMsg = {
  id: string;
  role: 'assistant' | 'user' | 'system' | 'tool';
  ts: number;
  type: 'ui';
  time: number;
  kind: string; // тип вашего UI-сообщения
  payload: any; // любые данные для рендера
};

// Расширенный тип чата: встроенные сообщения + ваши UI-сообщения
export type ExtendedChatMsg = ChatMsg | UIChatMsg;

// Что можно добавить через addMessage (одно или много)
export type AddableMessage =
  | {
      id?: string;
      role?: 'assistant' | 'user' | 'system' | 'tool';
      ts?: number;
      type?: 'text';
      text: string;
    }
  | {
      id?: string;
      role?: 'assistant' | 'user' | 'system' | 'tool';
      ts?: number;
      type: 'ui';
      kind: string;
      payload: any;
    };
export interface UseSessionOptionsParams {
  /** Client из useRealtime() */
  client: any;
  /** Callback при успешной операции */
  onSuccess?: (
    stage:
      | 'voice_initialized'
      | 'voice_closed'
      | 'assistant_cancelled'
      | 'text_initialized'
  ) => void;
  /** Callback при ошибке */
  onError?: (
    stage:
      | 'voice_init'
      | 'voice_close'
      | 'assistant_cancel'
      | 'text_init'
      | 'text_close',
    error: any
  ) => void;
}
// src/types/Events.ts

/**
 * Типы всех событий, которые можно слушать через client.on()
 */
export type RealtimeEventMap = {
  // User events (пользовательский ввод)
  'user:item_started': { itemId: string };
  'user:delta': { itemId: string; delta: string };
  'user:completed': { itemId: string; transcript: string };
  'user:failed': { itemId: string; error?: any };
  'user:truncated': { itemId: string };

  // Assistant events (ответы ассистента)
  'assistant:response_started': { responseId: string };
  'assistant:item_started': { itemId: string };
  'assistant:delta': {
    responseId: string;
    delta: string;
    channel?: 'audio_transcript' | 'output_text';
  };
  'assistant:completed': { responseId: string; status: 'done' | 'canceled' };

  // Tool events (вызовы функций)
  'tool:call_delta': { call_id: string; name: string; delta: string };
  'tool:call_done': { call_id: string; name: string; args: any };

  // Speech activity events (определение речи)
  'speech_started': void;
  'speech_stopped': void;
  'timeout_triggered': void;

  // Buffer events (буферы аудио)
  'input_audio_buffer.committed': any;
  'input_audio_buffer.cleared': any;
  'output_audio_buffer.started': any;
  'output_audio_buffer.stopped': any;
  'output_audio_buffer.cleared': any;

  // DataChannel events (низкоуровневые)
  'dataChannelOpen': any;
  'dataChannelMessage': any;
  'dataChannelClose': void;

  // Error events
  'error': { scope: 'tool' | 'server' | string; error: any };

  // Raw server events (любое серверное событие)
  [key: string]: any;
};

/**
 * Типизированный слушатель событий
 */
export type RealtimeEventListener<K extends keyof RealtimeEventMap> = (
  payload: RealtimeEventMap[K]
) => void;

/**
 * Коллбэки для всех Success событий (полный список)
 */
export interface RealtimeSuccessCallbacks {
  // Connection lifecycle
  onHangUpStarted?: () => void;
  onHangUpDone?: () => void;

  // PeerConnection events
  onPeerConnectionCreatingStarted?: () => void;
  onPeerConnectionCreated?: (pc: any) => void;
  onRTCPeerConnectionStateChange?: (
    state:
      | 'new'
      | 'connecting'
      | 'connected'
      | 'disconnected'
      | 'failed'
      | 'closed'
  ) => void;

  // Media events
  onGetUserMediaSetted?: (stream: any) => void;
  onLocalStreamSetted?: (stream: any) => void;
  onLocalStreamAddedTrack?: (track: any) => void;
  onLocalStreamRemovedTrack?: (track: any) => void;
  onRemoteStreamSetted?: (stream: any) => void;

  // DataChannel events
  onDataChannelOpen?: (channel: any) => void;
  onDataChannelMessage?: (message: any) => void;
  onDataChannelClose?: () => void;

  // ICE events
  onIceGatheringComplete?: () => void;
  onIceGatheringTimeout?: () => void;
  onIceGatheringStateChange?: (state: string) => void;

  // Permission events
  onMicrophonePermissionGranted?: () => void;
  onMicrophonePermissionDenied?: () => void;

  // iOS specific
  onIOSTransceiverSetted?: () => void;

  // Generic success handler
  onSuccess?: (stage: string, data?: any) => void;
}

/**
 * Коллбэки для Error событий
 */
export type ErrorCallbackPayload = {
  stage: string;
  error: Error;
  severity: 'critical' | 'warning' | 'info';
  recoverable: boolean;
  timestamp: number;
  context?: Record<string, any>;
};
export interface RealtimeErrorCallbacks {
  onError?: (event: ErrorCallbackPayload) => void;
}
// src/types/Responce.ts
export type ResponseCreateParams = {
  instructions: string;
  modalities?: Array<'audio' | 'text'>;
  conversation?: 'auto' | 'none'; // <-- только 'auto' | 'none'
};

export type ResponseCreateOptions = {
  instructions?: string;
  modalities?: Array<'audio' | 'text'>;
  conversation?: 'auto' | 'none';
};

export type ResponseCreateStrict = Omit<
  ResponseCreateOptions,
  'instructions'
> & {
  instructions: string;
};
export * from './Chat';
export * from './ClientStructure';
export * from './Constraints';
export * from './Context';
export * from './ErrorStage';
export * from './Responce';
export * from './Speech';
export * from './SucessCallbacks';
export * from './Events';
export * from './Modes';
import type { RTCOfferOptions } from 'react-native-webrtc/lib/typescript/RTCUtil';
import { VOICE_IDS, VoiceId } from '@react-native-openai-realtime/helpers';
import type {
  RealtimeContextValue,
  Constraints,
  ChatOptions,
  RealtimeSuccessCallbacks,
  RealtimeErrorCallbacks,
  AddableMessage,
} from '@react-native-openai-realtime/types';
import { RealtimeClientClass } from '@react-native-openai-realtime/components';

export type RTCIceServer = {
  credential?: string;
  url?: string;
  urls?: string | string[];
  username?: string;
};

export type RTCConfiguration = {
  bundlePolicy?: 'balanced' | 'max-compat' | 'max-bundle';
  iceCandidatePoolSize?: number;
  iceServers?: RTCIceServer[];
  iceTransportPolicy?: 'all' | 'relay';
  rtcpMuxPolicy?: 'negotiate' | 'require';
};

export type TokenProvider = () => Promise<string>;

export type ChatMode = 'voice' | 'text';

export type SessionConfig = {
  model?: string;
  voice?: VoiceId;
  modalities?: Array<'audio' | 'text'>;
  turn_detection?: {
    type: 'server_vad';
    silence_duration_ms?: number;
    threshold?: number;
    prefix_padding_ms?: number;
  } | null;
  input_audio_transcription?: { model: string; language?: string };
  tools?: any[];
  instructions?: string;
};

export type RealtimeClientHooks = {
  onOpen?: (dc: any) => void;
  onEvent?: (evt: any) => void;
  onError?: (e: any) => void;

  onUserTranscriptionDelta?: (payload: {
    itemId: string;
    delta: string;
  }) => 'consume' | void;
  onUserTranscriptionCompleted?: (payload: {
    itemId: string;
    transcript: string;
  }) => 'consume' | void;

  onAssistantTextDelta?: (payload: {
    responseId: string;
    delta: string;
    channel: 'audio_transcript' | 'output_text';
  }) => 'consume' | void;

  onAssistantCompleted?: (payload: {
    responseId: string;
    status: 'done' | 'canceled';
  }) => 'consume' | void;

  onToolCall?: (payload: {
    name: string;
    args: any;
    call_id: string;
  }) => Promise<any> | any;
};

export type MiddlewareCtx = {
  event: any;
  send: (e: any) => void | Promise<void>;
  client: RealtimeClientClass;
};

export type IncomingMiddleware = (
  ctx: MiddlewareCtx
) => Promise<any | 'stop' | null | void> | any | 'stop' | null | void;

export type OutgoingMiddleware = (
  event: any
) => any | null | 'stop' | Promise<any | null | 'stop'>;

export type Logger = {
  debug?: (...a: any[]) => void;
  info?: (...a: any[]) => void;
  warn?: (...a: any[]) => void;
  error?: (...a: any[]) => void;
};

export type RealtimeStatus =
  | 'idle'
  | 'connecting'
  | 'connected'
  | 'disconnected'
  | 'error'
  | 'user_speaking'
  | 'assistant_speaking';

export type RealtimeClientOptionsBeforePrune = {
  deleteChatHistoryOnDisconnect?: boolean;
  tokenProvider: TokenProvider;
  voice?: keyof typeof VOICE_IDS;
  webrtc?: {
    iceServers?: RTCIceServer[];
    dataChannelLabel?: string;
    offerOptions?: RTCOfferOptions & { voiceActivityDetection?: boolean };
    configuration?: RTCConfiguration;
  };
  media?: { getUserMedia?: Constraints };
  session?: Partial<SessionConfig>;
  autoSessionUpdate?: boolean;
  greet?: {
    enabled?: boolean;
    response?: {
      instructions?: string;
      modalities?: Array<'audio' | 'text'>;
    };
  };
  hooks?: RealtimeClientHooks;
  middleware?: {
    incoming?: IncomingMiddleware[];
    outgoing?: OutgoingMiddleware[];
  };
  policy?: {
    isMeaningfulText?: (text: string) => boolean;
  };
  chat?: ChatOptions;
  logger?: Logger;
  allowConnectWithoutMic?: boolean;
};

/**
 * ОБНОВЛЕНО: Добавлены все Success и Error коллбэки в публичные пропсы
 */
export type RealTimeClientProps = RealtimeSuccessCallbacks &
  RealtimeErrorCallbacks & {
    // Быстрые чат-поведения
    chatUserAddOnDelta?: boolean;
    chatInverted?: boolean;
    deleteChatHistoryOnDisconnect?: boolean;
    chatUserPlaceholderOnStart?: boolean;
    chatAssistantAddOnDelta?: boolean;
    chatAssistantPlaceholderOnStart?: boolean;

    tokenProvider?: TokenProvider;

    webrtc?: {
      iceServers?: RTCIceServer[];
      dataChannelLabel?: string;
      offerOptions?: RTCOfferOptions & { voiceActivityDetection?: boolean };
      configuration?: RTCConfiguration;
    };

    media?: { getUserMedia?: Constraints };
    session?: Partial<SessionConfig>;
    autoSessionUpdate?: boolean;

    // Greet настройки
    greetEnabled?: boolean;
    greetInstructions?: string;
    greetModalities?: Array<'audio' | 'text'>;

    // Hooks
    onOpen?: RealtimeClientHooks['onOpen'];
    onEvent?: RealtimeClientHooks['onEvent'];
    // onError уже в RealtimeErrorCallbacks
    onUserTranscriptionDelta?: RealtimeClientHooks['onUserTranscriptionDelta'];
    onUserTranscriptionCompleted?: RealtimeClientHooks['onUserTranscriptionCompleted'];
    onAssistantTextDelta?: RealtimeClientHooks['onAssistantTextDelta'];
    onAssistantCompleted?: RealtimeClientHooks['onAssistantCompleted'];
    onToolCall?: RealtimeClientHooks['onToolCall'];

    // Middleware
    incomingMiddleware?: IncomingMiddleware[];
    outgoingMiddleware?: OutgoingMiddleware[];

    // Policy/chat
    policyIsMeaningfulText?: (text: string) => boolean;
    chatEnabled?: boolean;
    chatIsMeaningfulText?: (text: string) => boolean;
    logger?: Logger;
    autoConnect?: boolean;
    attachChat?: boolean;

    allowConnectWithoutMic?: boolean;

    children?:
      | React.ReactNode
      | ((ctx: RealtimeContextValue) => React.ReactNode);
  };

export type CoreConfig = Omit<
  RealTimeClientProps,
  'autoConnect' | 'attachChat' | 'children'
>;

export type SessionMode = 'voice' | 'text';

export type InitializeMode = {
  type: SessionMode;
  options?: Partial<any>;
};

export type EnhancedRealTimeClientProps = RealTimeClientProps & {
  initializeMode?: InitializeMode;
  attemptsToReconnect?: number;
  onReconnectAttempt?: (attempt: number, maxAttempts: number) => void;
  onReconnectSuccess?: () => void;
  onReconnectFailed?: (error: any) => void;
};

export type RealTimeClientHandle = {
  enableMicrophone: () => Promise<void>;
  getClient: () => RealtimeClientClass | null;
  getStatus: () =>
    | 'idle'
    | 'connecting'
    | 'connected'
    | 'disconnected'
    | 'error';
  setTokenProvider: (tp: TokenProvider) => void;

  disableMicrophone: () => Promise<void>;
  connect: () => Promise<void>;
  disconnect: () => Promise<void>;

  sendRaw: (e: any) => Promise<void> | void;
  sendResponse: (opts?: any) => void;
  sendResponseStrict: (opts: {
    instructions: string;
    modalities?: Array<'audio' | 'text'>;
    conversation?: 'auto' | 'none';
  }) => void;
  updateSession: (patch: Partial<any>) => void;

  addMessage: (m: AddableMessage | AddableMessage[]) => string | string[];
  clearAdded: () => void;
  clearChatHistory: () => void;
  getNextTs: () => number;

  // Новые методы для управления режимами
  switchToTextMode: (customParams?: Partial<any>) => Promise<void>;
  switchToVoiceMode: (customParams?: Partial<any>) => Promise<void>;
  getCurrentMode: () => SessionMode;
  getModeStatus: () => 'idle' | 'connecting' | 'connected' | 'disconnected';
};
import { createContext } from 'react';
import type { RealtimeContextValue } from '@react-native-openai-realtime/types';

export const RealtimeContext = createContext<RealtimeContextValue>({
  client: null,
  status: 'idle',
  chat: [],
  connect: async () => {},
  disconnect: async () => {},
  sendResponse: () => {},
  sendResponseStrict: () => {},
  updateSession: () => {},
  clearChatHistory: () => {},
  sendRaw: () => {},
  addMessage: () => '',
  clearAdded: () => {},

  getNextTs: () => Date.now(),
});

export const RealtimeProvider = RealtimeContext.Provider;
export * from './RealtimeContext';
export * from './RealtimeContext';
policy — это глобальные правила по умолчанию для всей библиотеки. chat — это настройки только встроенного чат-хранилища/адаптера. Если заданы оба, chat имеет приоритет для чата.

Детально

policy.isMeaningfulText

Глобальный дефолтный предикат “текст осмысленный?”.
Используется библиотекой как fallback, когда для конкретного модуля (например, чата) не задан свой предикат.
Может быть полезен и вам самим в middleware/хуках (в примере мы использовали этот предикат, чтобы фильтровать «пустые» assistant-дeльты).
chat.isMeaningfulText

Предикат только для встроенного чата (ChatStore/ChatAdapter): решает, создавать ли пузырь на финализации и удалять ли пустышки.
Имеет приоритет над policy.isMeaningfulText.
Работает, только если chat.enabled !== false (встроенный чат включен).
Если chat.enabled=false, встроенного чата нет, и chat.isMeaningfulText игнорируется.
Как выбирается итоговое правило для чата
Внутри RealtimeClient при создании ChatStore:
const predicate = options.chat?.isMeaningfulText ?? options.policy?.isMeaningfulText ?? defaultPredicate;

Когда что использовать

Хочу одно правило везде и сразу: задайте только policy.isMeaningfulText. Чат возьмет его как дефолт (если вы не переопределите chat.isMeaningfulText).
Хочу отдельную логику именно для чата: задайте chat.isMeaningfulText (оно перекроет policy только для чата).
Хочу отключить встроенный чат, но продолжать фильтровать события в middleware/хуках: chat.enabled = false, используйте policy.isMeaningfulText в своих перехватчиках.
Использую ChatAdapter вручную: можете передать свой isMeaningfulText прямо в attachChatAdapter(..., { isMeaningfulText }) — он перекроет любые options.
Мини-примеры

Один предикат для всего (просто в policy)
policy: {
isMeaningfulText: t => t.replace(/[^\p{L}\p{N}]+/gu,'').trim().length >= 2,
},
// chat не задаем — чат возьмет из policy

Чату — мягче, глобально — строже
policy: {
isMeaningfulText: t => t.replace(/\s+/g,'').length >= 3, // глобально строгий
},
chat: {
enabled: true,
isMeaningfulText: t => !!t.trim(), // для чата помягче
}

Без встроенного чата, но с глобальным правилом (для своих middleware)
chat: { enabled: false },
policy: {
isMeaningfulText: t => !/^(эм+|мм+|ээ+|угу+|ага+).?$/i.test(t.trim()),
}

Если нужна наглядность

policy — это «общая политика по умолчанию».
chat — это «частные настройки именно чата» (перекрывают policy для чата).
chat.enabled включает/выключает встроенный чат целиком. Policy ничего не включает/выключает, это просто набор правил/эвристик, которые могут использовать разные части системы.

Зачем нужна isMeaningfulText и что она делает

Это предикат “текст осмысленный?”. Он решает, считать ли фрагмент текста полноценным содержимым или “пустышкой”.
Где используется:
Встроенный чат (ChatStore/ChatAdapter): при финализации сообщения, если текст неосмысленный (пусто, одни пробелы/пунктуация/междометия) — пузырь не попадает в чат или удаляется.
В middleware/хуках: можно программно “съедать” ничтожные дельты ассистента/пользователя, чтобы UI не моргал “…” и не копил мусор.
Разница между policy.isMeaningfulText и chat.isMeaningfulText

policy.isMeaningfulText — глобальный дефолт для всей библиотеки (его удобно использовать и в middleware/хуках). Если нигде не переопределено, чату достанется именно он.
chat.isMeaningfulText — настройка только встроенного чата (ChatStore/ChatAdapter). Если указали — она перекрывает policy для чата.
Итого: итоговое правило для чата выбирается так:
chat.isMeaningfulText ?? policy.isMeaningfulText ?? (t => !!t.trim())
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';

export const DEFAULTS: RealtimeClientOptionsBeforePrune = {
  // tokenProvider — обязателен, без него нельзя получить ephemeral token
  tokenProvider: async () => {
    throw new Error(
      'tokenProvider is required: provide a function returning ephemeral token'
    );
  },

  webrtc: {
    iceServers: [
      { urls: 'stun:stun.l.google.com:19302' },
      { urls: 'stun:stun1.l.google.com:19302' },
    ],
    dataChannelLabel: 'oai-events',
    offerOptions: {
      offerToReceiveAudio: true,
      voiceActivityDetection: true,
    } as any,
    configuration: { iceCandidatePoolSize: 10 },
  },

  media: {
    getUserMedia: {
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      } as any,
      video: false,
    },
  },

  session: {
    model: 'gpt-4o-realtime-preview-2024-12-17',
    voice: 'alloy',
    modalities: ['audio', 'text'],
    input_audio_transcription: { model: 'whisper-1' },
    turn_detection: {
      type: 'server_vad',
      silence_duration_ms: 700,
      prefix_padding_ms: 300,
      threshold: 0.5,
    },
  },

  autoSessionUpdate: true,

  greet: {
    enabled: true,
    response: {
      instructions: 'Привет! Я на связи и готов помочь.',
      modalities: ['audio', 'text'],
    },
  },

  hooks: {},

  middleware: {
    incoming: [],
    outgoing: [],
  },

  policy: {
    isMeaningfulText: (t: string) => !!t.trim(),
  },

  chat: {
    enabled: true,
    isMeaningfulText: (t: string) => !!t.trim(),
  },

  logger: {
    info: (...a) => console.log('[INFO]', ...a),
    debug: (...a) => console.log('[DEBUG]', ...a),
    warn: (...a) => console.log('[WARN]', ...a),
    error: (...a) => console.log('[ERROR]', ...a),
  },
};
export * from './defaultOptions';
// src/managers/MessageSender.ts
import type {
  ResponseCreateParams,
  ResponseCreateStrict,
  RealtimeClientOptionsBeforePrune,
} from '@react-native-openai-realtime/types';
import { ErrorHandler } from '@react-native-openai-realtime/handlers';
import { DataChannelManager } from '@react-native-openai-realtime/managers';

export class MessageSender {
  private dataChannelManager: DataChannelManager;
  private options: RealtimeClientOptionsBeforePrune;
  private errorHandler: ErrorHandler;

  constructor(
    dataChannelManager: DataChannelManager,
    options: RealtimeClientOptionsBeforePrune,
    errorHandler: ErrorHandler
  ) {
    this.dataChannelManager = dataChannelManager;
    this.options = options;
    this.errorHandler = errorHandler;
  }

  async sendRaw(event: any): Promise<void> {
    try {
      if (!this.dataChannelManager.isOpen()) {
        throw new Error('DataChannel is not open');
      }

      // Outgoing middleware
      if (this.options.middleware?.outgoing) {
        for (const mw of this.options.middleware.outgoing) {
          const res = await Promise.resolve(mw(event));

          if (res === 'stop') {
            return;
          }
          if (res && typeof res === 'object') {
            event = res;
          }
        }
      }

      if (!this.dataChannelManager.isOpen()) {
        throw new Error('DataChannel is not open');
      }

      this.dataChannelManager.send(event);
    } catch (e: any) {
      this.errorHandler.handle('data_channel', e, 'warning', true, { event });
    }
  }

  sendResponse(): void;
  sendResponse(params: ResponseCreateParams): void;
  sendResponse(params?: any): void {
    const response = params ?? {};
    this.sendRaw({ type: 'response.create', response });
  }

  sendResponseStrict(options: ResponseCreateStrict) {
    this.sendRaw({ type: 'response.create', response: options });
  }

  updateSession(patch: Partial<any>) {
    this.sendRaw({ type: 'session.update', session: patch });
  }

  sendToolOutput(call_id: string, output: any) {
    this.sendRaw({
      type: 'conversation.item.create',
      item: {
        type: 'function_call_output',
        call_id,
        output: JSON.stringify(output),
      },
    });
  }

  // ИСПРАВЛЕНО: Правильная отправка текстовых сообщений
  async sendTextMessage(
    text: string,
    options?: {
      responseModality?: 'text' | 'audio';
      instructions?: string;
      conversation?: 'auto' | 'none';
    }
  ): Promise<void> {
    const msg = (text ?? '').trim();
    if (!msg) return;

    // 1. Создаем conversation item с текстом пользователя
    await this.sendRaw({
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'user',
        content: [
          {
            type: 'input_text',
            text: msg,
          },
        ],
      },
    });

    // 2. Создаем response с правильными параметрами
    const modality = options?.responseModality ?? 'text';

    // ВАЖНО: используем 'auto' вместо 'none' для сохранения контекста
    const response: ResponseCreateStrict = {
      instructions:
        options?.instructions ?? 'Ответь на сообщение пользователя.',
      modalities: modality === 'text' ? ['text'] : ['audio', 'text'],
      // По умолчанию 'auto' - добавляет в историю разговора
      conversation: options?.conversation ?? 'auto',
    };

    this.sendResponseStrict(response);
  }
}
// core/managers/OpenAIApiClient.ts
import { ErrorHandler } from '@react-native-openai-realtime/handlers';

export class OpenAIApiClient {
  private errorHandler: ErrorHandler;

  constructor(errorHandler: ErrorHandler) {
    this.errorHandler = errorHandler;
  }

  async postSDP(localSdp: string, ephemeralKey: string): Promise<string> {
    try {
      const resp = await fetch('https://api.openai.com/v1/realtime/calls', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${ephemeralKey}`,
          'Content-Type': 'application/sdp',
          'OpenAI-Beta': 'realtime=v1',
        },
        body: localSdp || '',
      });

      const text = await resp.text();

      if (!resp.ok) {
        this.errorHandler.handle(
          'openai_api',
          new Error(`OpenAI ${resp.status}: ${text.slice(0, 200)}`)
        );
        throw new Error(text);
      }

      if (!text.startsWith('v=')) {
        this.errorHandler.handle(
          'openai_api',
          new Error(`Invalid SDP from OpenAI: ${text.slice(0, 100)}`)
        );
        throw new Error('Invalid SDP');
      }

      return text;
    } catch (e: any) {
      this.errorHandler.handle('openai_api', e);
      throw e;
    }
  }
}
// В файле: MediaManager.ts
// Замените класс целиком:

import { mediaDevices, MediaStream } from 'react-native-webrtc';
import type { RTCPeerConnection } from 'react-native-webrtc';
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';
import {
  ErrorHandler,
  SuccessHandler,
} from '@react-native-openai-realtime/handlers';

export class MediaManager {
  private localStream: MediaStream | null = null;
  private remoteStream: MediaStream | null = null;
  private options: RealtimeClientOptionsBeforePrune;
  private errorHandler: ErrorHandler;
  private successHandler: SuccessHandler;

  constructor(
    options: RealtimeClientOptionsBeforePrune,
    errorHandler: ErrorHandler,
    successHandler: SuccessHandler
  ) {
    this.options = options;
    this.errorHandler = errorHandler;
    this.successHandler = successHandler;
  }

  // ✅ ИСПРАВЛЕНО: Не вызываем errorHandler внутри
  async getUserMedia(): Promise<MediaStream> {
    // Сначала останавливаем старый stream если есть
    this.stopLocalStream();

    const constraints = this.options.media?.getUserMedia!;
    const stream = await mediaDevices.getUserMedia(constraints);
    this.localStream = stream;
    this.successHandler.getUserMediaSetted(stream);
    this.successHandler.localStreamSetted(stream);
    return stream;
  }

  addLocalStreamToPeerConnection(pc: RTCPeerConnection, stream: MediaStream) {
    // КРИТИЧНО: Проверяем, что PeerConnection не закрыт
    if (!pc || pc.connectionState === 'closed') {
      const error = new Error('Cannot add tracks: PeerConnection is closed');
      this.errorHandler.handle('local_stream', error, 'critical', false);
      throw error;
    }

    stream.getTracks().forEach((track) => {
      try {
        // Дополнительная проверка перед добавлением каждого трека
        if (pc.connectionState === 'closed') {
          throw new Error('PeerConnection closed during track addition');
        }

        pc.addTrack(track, stream);
        this.successHandler.localStreamAddedTrack(track);
      } catch (e: any) {
        this.errorHandler.handle('local_stream', e);
        throw e; // Пробрасываем ошибку, чтобы прервать connect()
      }
    });
  }

  setupRemoteStream(pc: RTCPeerConnection) {
    // @ts-ignore
    pc.ontrack = (event: any) => {
      try {
        if (!this.remoteStream) {
          this.remoteStream = new MediaStream();
        }
        this.remoteStream.addTrack(event.track);
        this.successHandler.remoteStreamSetted(this.remoteStream);
      } catch (e: any) {
        this.errorHandler.handle('remote_stream', e);
      }
    };
  }

  getLocalStream() {
    return this.localStream;
  }

  getRemoteStream() {
    return this.remoteStream;
  }

  stopLocalStream() {
    if (this.localStream) {
      try {
        this.localStream.getTracks().forEach((t) => {
          try {
            t.stop();
          } catch {}
        });
      } catch {}
      this.localStream = null;
    }
  }

  cleanup() {
    this.stopLocalStream();
    this.remoteStream = null;
  }
}
import {
  ErrorHandler,
  SuccessHandler,
} from '@react-native-openai-realtime/handlers';
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';
import { RTCPeerConnection } from 'react-native-webrtc';

export class PeerConnectionManager {
  private pc: RTCPeerConnection | null = null;
  private options: RealtimeClientOptionsBeforePrune;
  private errorHandler: ErrorHandler;
  private successHandler: SuccessHandler;

  constructor(
    options: RealtimeClientOptionsBeforePrune,
    errorHandler: ErrorHandler,
    successHandler: SuccessHandler
  ) {
    this.options = options;
    this.errorHandler = errorHandler;
    this.successHandler = successHandler;
  }

  create(): RTCPeerConnection {
    try {
      // ВАЖНО: Если уже есть pc - закрываем его ПЕРЕД созданием нового
      if (this.pc) {
        try {
          this.pc.close();
        } catch {}
        this.pc = null;
      }

      this.successHandler.peerConnectionCreatingStarted();

      const pc = new RTCPeerConnection(
        this.options.webrtc?.configuration ??
          ({
            iceServers: this.options.webrtc?.iceServers,
            iceCandidatePoolSize:
              this.options.webrtc?.configuration?.iceCandidatePoolSize ?? 10,
          } as RTCConfiguration)
      );

      this.pc = pc;
      this.successHandler.peerConnectionCreated(pc);

      // Setup listeners
      this.setupListeners(pc);

      return pc;
    } catch (e: any) {
      this.errorHandler.handle('peer_connection', e);
      throw e;
    }
  }

  private setupListeners(pc: RTCPeerConnection) {
    // @ts-ignore
    pc.onconnectionstatechange = () =>
      this.successHandler.rtcPeerConnectionStateChange(pc.connectionState);

    // @ts-ignore
    pc.onicecandidate = (e: any) => {
      if (!e.candidate) {
        this.successHandler.iceGatheringComplete();
      }
    };

    // @ts-ignore
    pc.oniceconnectionstatechange = () =>
      this.successHandler.iceGatheringStateChange(pc.iceConnectionState);
  }

  async createOffer() {
    if (!this.pc) {
      throw new Error('PeerConnection not created');
    }

    try {
      const offerOptions = this.options.webrtc?.offerOptions as any;
      const offer = await this.pc.createOffer(offerOptions);
      return offer;
    } catch (e: any) {
      this.errorHandler.handle('create_offer', e);
      throw e;
    }
  }

  async setLocalDescription(offer: any) {
    if (!this.pc) {
      throw new Error('PeerConnection not created');
    }

    try {
      await this.pc.setLocalDescription(offer);
    } catch (e: any) {
      this.errorHandler.handle('set_local_description', e);
      throw e;
    }
  }

  async setRemoteDescription(answer: string) {
    if (!this.pc) {
      throw new Error('PeerConnection not created');
    }

    try {
      await this.pc.setRemoteDescription({ type: 'answer', sdp: answer });
    } catch (e: any) {
      this.errorHandler.handle('set_remote_description', e);
      throw e;
    }
  }

  async waitForIceGathering(): Promise<void> {
    if (!this.pc) {
      throw new Error('PeerConnection not created');
    }

    return new Promise((resolve) => {
      try {
        if (this.pc!.iceGatheringState === 'complete') {
          this.successHandler.iceGatheringComplete();
          resolve();
          return;
        }

        const timeout = setTimeout(() => {
          this.successHandler.iceGatheringTimeout();
          // @ts-ignore
          this.pc!.onicegatheringstatechange = null;
          resolve();
        }, 2500);

        // @ts-ignore
        this.pc!.onicegatheringstatechange = () => {
          this.successHandler.iceGatheringStateChange(
            this.pc!.iceGatheringState
          );
          if (this.pc!.iceGatheringState === 'complete') {
            clearTimeout(timeout);
            // @ts-ignore
            this.pc!.onicegatheringstatechange = null;
            this.successHandler.iceGatheringComplete();
            resolve();
          }
        };
      } catch (e: any) {
        this.errorHandler.handle('ice_gathering', e, 'warning', true);
        resolve();
      }
    });
  }

  getPeerConnection() {
    return this.pc;
  }

  close() {
    if (this.pc) {
      try {
        this.pc.close();
      } catch {}
      this.pc = null;
    }
  }

  isConnected() {
    return !!this.pc && this.pc.connectionState === 'connected';
  }
}
export * from './DataChannelManager';
export * from './EventRouter';
export * from './MediaManager';
export * from './MessageSender';
export * from './OpenAIApiManager';
export * from './PeerConnectionManager';
import type { RTCPeerConnection } from 'react-native-webrtc';
import type RTCDataChannel from 'react-native-webrtc/lib/typescript/RTCDataChannel';
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';
import {
  ErrorHandler,
  SuccessHandler,
} from '@react-native-openai-realtime/handlers';

type MessageHandler = (message: any) => void | Promise<void>;

export class DataChannelManager {
  private dc: RTCDataChannel | null = null;
  private options: RealtimeClientOptionsBeforePrune;
  private errorHandler: ErrorHandler;
  private successHandler: SuccessHandler;
  private onMessage?: MessageHandler;

  constructor(
    options: RealtimeClientOptionsBeforePrune,
    errorHandler: ErrorHandler,
    successHandler: SuccessHandler
  ) {
    this.options = options;
    this.errorHandler = errorHandler;
    this.successHandler = successHandler;
  }

  create(pc: RTCPeerConnection, onMessage: MessageHandler): RTCDataChannel {
    try {
      // КРИТИЧНО: Проверяем, что PeerConnection не закрыт
      if (!pc || pc.connectionState === 'closed') {
        throw new Error('Cannot create DataChannel: PeerConnection is closed');
      }

      // Закрываем старый DataChannel если есть
      if (this.dc) {
        try {
          this.dc.close();
        } catch {}
        this.dc = null;
      }

      this.onMessage = onMessage;

      // @ts-ignore
      const dc = pc.createDataChannel(this.options.webrtc?.dataChannelLabel!, {
        ordered: true,
        maxRetransmits: 3,
      });

      this.dc = dc;
      this.setupListeners(dc);

      return dc;
    } catch (e: any) {
      this.errorHandler.handle('data_channel', e);
      throw e;
    }
  }

  private setupListeners(dc: RTCDataChannel) {
    // @ts-ignore
    dc.onopen = () => {
      try {
        this.successHandler.dataChannelOpen(dc);
        this.handleOpen();
      } catch (e: any) {
        this.errorHandler.handle('data_channel', e);
      }
    };

    // @ts-ignore
    dc.onmessage = async (message: any) => {
      try {
        const text =
          typeof message.data === 'string'
            ? message.data
            : String(message.data);
        let evt: any;

        try {
          evt = JSON.parse(text);
        } catch (err: any) {
          this.errorHandler.handle(
            'data_channel',
            err instanceof Error ? err : new Error(String(err)),
            'warning',
            true,
            {
              raw:
                typeof text === 'string' && text.length > 2000
                  ? text.slice(0, 2000) + '…'
                  : text,
              hint: 'Failed to JSON.parse DataChannel message',
            }
          );
          return;
        }

        this.successHandler.dataChannelMessage(evt);

        if (this.onMessage) {
          await this.onMessage(evt);
        }
      } catch (err: any) {
        this.errorHandler.handle('data_channel', err);
      }
    };

    // @ts-ignore
    dc.onclose = () => this.successHandler.dataChannelClose();

    // @ts-ignore
    dc.onerror = (error: any) =>
      this.errorHandler.handle('data_channel', error, 'warning', true);
  }

  private handleOpen() {
    // КРИТИЧНО: Проверяем готовность перед отправкой команд
    if (!this.dc || this.dc.readyState !== 'open') {
      this.options.logger?.warn?.(
        '[DataChannel] handleOpen called but channel not ready'
      );
      return;
    }

    // session.update (auto)
    if (this.options.autoSessionUpdate && this.options.session) {
      try {
        this.send({
          type: 'session.update',
          session: this.options.session,
        });
      } catch (e: any) {
        this.options.logger?.error?.(
          '[DataChannel] Failed to send session.update:',
          e
        );
      }
    }

    // greet
    if (this.options.greet?.enabled !== false) {
      try {
        const response = {
          instructions:
            this.options.greet?.response?.instructions ?? 'Привет! Я на связи.',
          modalities: this.options.greet?.response?.modalities ?? [
            'audio',
            'text',
          ],
        };
        this.send({ type: 'response.create', response });
      } catch (e: any) {
        this.options.logger?.error?.('[DataChannel] Failed to send greet:', e);
      }
    }

    this.options.hooks?.onOpen?.(this.dc);
  }

  send(event: any): void {
    if (!this.dc || this.dc.readyState !== 'open') {
      throw new Error('DataChannel is not open');
    }

    this.dc.send(JSON.stringify(event));
  }

  getDataChannel() {
    return this.dc;
  }

  close() {
    if (this.dc) {
      try {
        this.dc.close();
        this.successHandler.dataChannelClose();
      } catch {}
      this.dc = null;
    }
  }

  isOpen() {
    return !!this.dc && this.dc.readyState === 'open';
  }
}
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';
import { createDefaultRouter } from '@react-native-openai-realtime/helpers';
import type { RealtimeClientClass } from '@react-native-openai-realtime/components';

type Listener = (payload: any) => void;

export class EventRouter {
  private listeners = new Map<string, Set<Listener>>();
  private incomingRouter: (msg: any) => void | Promise<void>;
  private functionArgsBuffer = new Map<string, string>();
  private options: RealtimeClientOptionsBeforePrune;

  private sendRef: (e: any) => Promise<void>;
  private clientRef: any;

  constructor(
    options: RealtimeClientOptionsBeforePrune,
    sendRaw: (e: any) => Promise<void>,
    client?: any
  ) {
    this.options = options;
    this.sendRef = sendRaw;
    this.clientRef = client;

    this.incomingRouter = createDefaultRouter(
      this.emit.bind(this),
      options,
      this.functionArgsBuffer,
      sendRaw
    );
  }

  setContext(
    client: RealtimeClientClass | null,
    sendRaw: (e: any) => Promise<void>
  ) {
    this.clientRef = client;
    this.sendRef = sendRaw;
  }

  on(type: string, handler: Listener) {
    const set = this.listeners.get(type) ?? new Set<Listener>();
    set.add(handler);
    this.listeners.set(type, set);
    return () => set.delete(handler);
  }

  private emit(type: string, payload?: any) {
    const set = this.listeners.get(type);
    if (set) set.forEach((fn) => fn(payload));
  }

  async processIncomingMessage(evt: any) {
    // incoming middleware — теперь с реальным send и client
    if (this.options.middleware?.incoming) {
      for (const mw of this.options.middleware.incoming) {
        const res = await mw({
          event: evt,
          send: this.sendRef,
          client: this.clientRef,
        });
        if (res === 'stop') return;
        if (res && typeof res === 'object') {
          evt = res;
        }
      }
    }

    await this.incomingRouter(evt);
    // ВАЖНО: НЕ вызываем здесь hooks.onEvent — он уже вызывается внутри createDefaultRouter
  }

  cleanup() {
    this.listeners.clear();
    this.functionArgsBuffer.clear();
  }
}
// src/middlewares/speechActivityMiddleware.ts
import type {
  MiddlewareCtx,
  SpeechActivityState,
  Listener,
} from '@react-native-openai-realtime/types';

class SpeechActivityStore {
  private state: SpeechActivityState = {
    isUserSpeaking: false,
    isAssistantSpeaking: false,
    inputBuffered: false,
    outputBuffered: false,
    lastUserEventAt: null,
    lastAssistantEventAt: null,
  };
  private listeners = new Set<Listener>();

  get() {
    return this.state;
  }
  subscribe(fn: Listener) {
    this.listeners.add(fn);
    return () => this.listeners.delete(fn);
  }
  private emit() {
    const s = this.state;
    this.listeners.forEach((fn) => fn(s));
  }

  setUserSpeaking(v: boolean) {
    if (this.state.isUserSpeaking !== v) {
      this.state = {
        ...this.state,
        isUserSpeaking: v,
        lastUserEventAt: Date.now(),
      };
      this.emit();
    }
  }
  setAssistantSpeaking(v: boolean) {
    if (this.state.isAssistantSpeaking !== v) {
      this.state = {
        ...this.state,
        isAssistantSpeaking: v,
        lastAssistantEventAt: Date.now(),
      };
      this.emit();
    }
  }
  setInputBuffered(v: boolean) {
    if (this.state.inputBuffered !== v) {
      this.state = {
        ...this.state,
        inputBuffered: v,
        lastUserEventAt: Date.now(),
      };
      this.emit();
    }
  }
  setOutputBuffered(v: boolean) {
    if (this.state.outputBuffered !== v) {
      this.state = {
        ...this.state,
        outputBuffered: v,
        lastAssistantEventAt: Date.now(),
      };
      this.emit();
    }
  }
}

export const speechActivityStore = new SpeechActivityStore();
export type SpeechActivityStoreType = typeof speechActivityStore;

export function createSpeechActivityMiddleware(store = speechActivityStore) {
  return ({ event }: MiddlewareCtx) => {
    const t = event?.type;

    // ========== ПОЛЬЗОВАТЕЛЬ ==========
    if (t === 'input_audio_buffer.speech_started' || t === 'speech_started') {
      store.setUserSpeaking(true);
      store.setAssistantSpeaking(false);
    }
    if (t === 'input_audio_buffer.speech_stopped' || t === 'speech_stopped') {
      store.setUserSpeaking(false);
      store.setInputBuffered(false);
    }
    if (
      t === 'input_audio_buffer.committed' ||
      t === 'input_audio_buffer.commit'
    ) {
      store.setInputBuffered(true);
      store.setUserSpeaking(true);
    }
    if (
      t === 'input_audio_buffer.cleared' ||
      t === 'input_audio_buffer.clear'
    ) {
      store.setInputBuffered(false);
    }

    // ========== АССИСТЕНТ ==========

    // ✅ ИСПРАВЛЕНИЕ: Начало генерации ответа
    if (t === 'response.created') {
      // Не включаем speaking сразу - подождем пока не начнется реальное аудио
      // store.setAssistantSpeaking(true); // ❌ Убрали преждевременную активацию
    }

    // ✅ ИСПРАВЛЕНИЕ: Начало воспроизведения аудио (РЕАЛЬНОЕ начало!)
    if (t === 'output_audio_buffer.started' || t === 'response.audio.delta') {
      store.setOutputBuffered(true);
      store.setAssistantSpeaking(true);
      store.setInputBuffered(false);
      store.setUserSpeaking(false);
    }

    // ✅ ИСПРАВЛЕНИЕ: Остановка воспроизведения аудио (РЕАЛЬНЫЙ конец!)
    // Это самое важное событие - оно означает что аудио ДЕЙСТВИТЕЛЬНО закончилось
    if (t === 'output_audio_buffer.stopped') {
      store.setAssistantSpeaking(false);
      store.setOutputBuffered(false);
    }

    // Очистка буфера (отмена)
    if (t === 'output_audio_buffer.cleared') {
      store.setAssistantSpeaking(false);
      store.setOutputBuffered(false);
    }

    // ✅ ИСПРАВЛЕНИЕ: Отмена ответа
    if (t === 'response.canceled' || t === 'response.cancelled') {
      store.setAssistantSpeaking(false);
      store.setOutputBuffered(false);
    }

    // ⚠️ НЕ реагируем на response.done / response.text.done / response.audio_transcript.done
    // потому что они не означают завершение ВОСПРОИЗВЕДЕНИЯ аудио!
    // Воспроизведение продолжается пока не придет output_audio_buffer.stopped

    return;
  };
}
export * from './speechMiddleware';
// src/adapters/ChatAdapter.ts
import {
  ChatAdapterOptions,
  ChatMsg,
} from '@react-native-openai-realtime/types';
import { RealtimeClientClass } from '@react-native-openai-realtime/components';
/**
 * Без дублирования: только подписка на встроенный ChatStore клиента.
 * Если chatStore отключён — возвращаем noop и ничего не делаем.
 */
export function attachChatAdapter(
  client: RealtimeClientClass, // RealtimeClientClass
  setChat: React.Dispatch<React.SetStateAction<ChatMsg[]>>,
  _opts?: ChatAdapterOptions
) {
  try {
    const initial =
      typeof client?.getChat === 'function' ? client.getChat() : [];
    if (Array.isArray(initial)) {
      setChat(initial);
    }
  } catch {
    // no-op
  }

  if (typeof client?.onChatUpdate === 'function') {
    const unsub = client.onChatUpdate((chat: ChatMsg[]) => setChat(chat));
    return () => {
      try {
        unsub?.();
      } catch {}
    };
  }

  // Fallback: если встроенного ChatStore нет — ничего не делаем
  return () => {};
}
export * from './ChatAdapter';
export * from './ChatStore';
// src/adapters/ChatStore.ts
import type { ChatMsg } from '@react-native-openai-realtime/types';

type Side = 'user' | 'assistant';
type Listener = (chat: ChatMsg[]) => void;

type StoreRow = {
  inChat: boolean; // уже есть сообщение в чате
  hasText: boolean; // был ли хоть какой-то текст
  buffer: string; // накопитель для дельт, если мы не показываем их сразу
};

type ChatStoreOptions = {
  isMeaningfulText?: (t: string) => boolean;

  // Управление поведением добавления в чат во время транскрипции
  userAddOnDelta?: boolean; // true — добавлять юзер-сообщение при первой дельте
  userPlaceholderOnStart?: boolean; // true — создать пустое сообщение на user:item_started
  assistantAddOnDelta?: boolean; // true — добавлять ассистентское сообщение при первой дельте
  assistantPlaceholderOnStart?: boolean; // true — создать пустое на assistant:response_started
};

export class ChatStore {
  private chat: ChatMsg[] = [];
  private listeners = new Set<Listener>();

  private seqRef = 0;
  private userOrderRef = new Map<string, number>();
  private respOrderRef = new Map<string, number>();

  private userState = new Map<string, StoreRow>();
  private assistantState = new Map<string, StoreRow>();

  private isMeaningful: (text: string) => boolean;

  private cfg: Required<Omit<ChatStoreOptions, 'isMeaningfulText'>> = {
    userAddOnDelta: true,
    userPlaceholderOnStart: false,
    assistantAddOnDelta: true,
    assistantPlaceholderOnStart: false,
  };

  constructor(opts?: ChatStoreOptions) {
    this.isMeaningful = opts?.isMeaningfulText ?? ((t: string) => !!t.trim());
    if (opts) {
      this.cfg = {
        ...this.cfg,
        userAddOnDelta: opts.userAddOnDelta ?? this.cfg.userAddOnDelta,
        userPlaceholderOnStart:
          opts.userPlaceholderOnStart ?? this.cfg.userPlaceholderOnStart,
        assistantAddOnDelta:
          opts.assistantAddOnDelta ?? this.cfg.assistantAddOnDelta,
        assistantPlaceholderOnStart:
          opts.assistantPlaceholderOnStart ??
          this.cfg.assistantPlaceholderOnStart,
      };
    }
  }

  get() {
    return this.chat;
  }

  subscribe(fn: Listener) {
    this.listeners.add(fn);
    return () => this.listeners.delete(fn);
  }

  private emit() {
    const snap = this.chat.slice();
    this.listeners.forEach((fn) => fn(snap));
  }

  private orderBy(side: Side) {
    return side === 'user' ? this.userOrderRef : this.respOrderRef;
  }

  private stateBy(side: Side) {
    return side === 'user' ? this.userState : this.assistantState;
  }

  private ensureOrder(side: Side, id: string) {
    const map = this.orderBy(side);
    if (!map.has(id)) {
      map.set(id, ++this.seqRef);
    }
    return map.get(id)!;
  }

  startUser(itemId: string) {
    // Регистрируем порядок
    this.userOrderRef.set(itemId, ++this.seqRef);

    const placeholder = this.cfg.userPlaceholderOnStart;
    this.userState.set(itemId, {
      inChat: placeholder,
      hasText: false,
      buffer: '',
    });

    if (placeholder) {
      const ts0 = this.userOrderRef.get(itemId)!;
      const now = Date.now();
      const msg: ChatMsg & { time: number } = {
        id: itemId,
        itemId,
        type: 'text',
        role: 'user',
        text: '',
        ts: ts0,
        time: now,
        status: 'streaming',
      };
      this.chat = [...this.chat, msg];
      this.emit();
    }
  }

  startAssistant(responseId: string) {
    this.respOrderRef.set(responseId, ++this.seqRef);

    const placeholder = this.cfg.assistantPlaceholderOnStart;
    this.assistantState.set(responseId, {
      inChat: placeholder,
      hasText: false,
      buffer: '',
    });

    if (placeholder) {
      const ts0 = this.respOrderRef.get(responseId)!;
      const now = Date.now();
      const msg: ChatMsg & { time: number } = {
        id: responseId,
        responseId,
        role: 'assistant',
        text: '',
        ts: ts0,
        type: 'text',
        time: now,
        status: 'streaming',
      };
      this.chat = [...this.chat, msg];
      this.emit();
    }
  }

  putDelta(side: Side, id: string, delta: string) {
    if (!id || !delta) return;

    const store = this.stateBy(side);
    const st = store.get(id) || { inChat: false, hasText: false, buffer: '' };
    // Убедимся, что есть порядковый ts
    const ts0 = this.orderBy(side).get(id) ?? this.ensureOrder(side, id);

    const addOnDelta =
      side === 'user' ? this.cfg.userAddOnDelta : this.cfg.assistantAddOnDelta;

    if (!st.inChat) {
      if (addOnDelta) {
        // Добавляем новое сообщение в чат при первой дельте
        const now = Date.now();
        const msg: ChatMsg & { time: number } = {
          id,
          type: 'text',
          itemId: side === 'user' ? id : undefined,
          responseId: side === 'assistant' ? id : undefined,
          role: side,
          text: (st.buffer || '') + delta,
          ts: ts0,
          time: now,
          status: 'streaming',
        };
        this.chat = [...this.chat, msg];
        st.inChat = true;
        st.hasText = true;
        st.buffer = ''; // сбрасываем буфер, так как теперь рендерим прямо в чате
      } else {
        // Не добавляем в чат — просто накапливаем буфер
        st.buffer += delta;
        st.hasText = true;
        // emit не делаем — в чате пока ничего не меняется
      }
    } else {
      // Сообщение уже в чате (плейсхолдер или ранее добавленное по дельте) — дописываем текст
      this.chat = this.chat.map((m) => {
        const match = side === 'user' ? m.itemId === id : m.responseId === id;
        return match ? { ...m, text: (m.text || '') + delta } : m;
      });
      st.hasText = true;
    }

    store.set(id, st);
    if (st.inChat) this.emit();
  }

  finalize(
    side: Side,
    id: string,
    status: 'done' | 'canceled',
    finalText?: string
  ) {
    const store = this.stateBy(side);
    const st = store.get(id); // сохраним ссылку, чтобы вытащить buffer
    store.delete(id);

    const idx = this.chat.findIndex((m) =>
      side === 'user' ? m.itemId === id : m.responseId === id
    );

    // Текст, который будем фиксировать:
    const buffered = st?.buffer ?? '';
    const textToUse = finalText ?? buffered;

    if (idx === -1) {
      if (this.isMeaningful(textToUse)) {
        const ts0 = this.orderBy(side).get(id) ?? Date.now();
        const now = Date.now();
        const msg: ChatMsg & { time: number } = {
          id,
          type: 'text',
          itemId: side === 'user' ? id : undefined,
          responseId: side === 'user' ? undefined : id,
          role: side,
          text: textToUse,
          ts: ts0,
          time: now,
          status: 'done',
        };
        this.chat = [...this.chat, msg];
        this.emit();
      }
      return;
    }

    // Сообщение уже в ленте — обновим финальный текст/статус
    const msg = this.chat[idx];
    // Если финального текста нет, а в чате был плейсхолдер — подставим буфер
    const mergedText = finalText ?? (msg?.text || '' || buffered);

    if (!this.isMeaningful(mergedText)) {
      const copy = [...this.chat];
      copy.splice(idx, 1);
      this.chat = copy;
      this.emit();
      return;
    }

    const copy = [...this.chat];
    copy[idx] = { ...msg!, text: mergedText, status };
    this.chat = copy;
    this.emit();
  }

  destroy() {
    this.listeners.clear();
    this.chat = [];
    this.userOrderRef.clear();
    this.respOrderRef.clear();
    this.userState.clear();
    this.assistantState.clear();
    this.seqRef = 0;
  }
}
import { ChatStore } from '@react-native-openai-realtime/adapters';
import {
  ErrorHandler,
  SuccessHandler,
} from '@react-native-openai-realtime/handlers';
import { applyDefaults } from '@react-native-openai-realtime/helpers';
import {
  PeerConnectionManager,
  MediaManager,
  DataChannelManager,
  MessageSender,
  EventRouter,
  OpenAIApiClient,
} from '@react-native-openai-realtime/managers';
import type {
  RealtimeClientOptionsBeforePrune,
  ResponseCreateParams,
  ResponseCreateStrict,
  TokenProvider,
  RealtimeEventMap,
  RealtimeEventListener,
} from '@react-native-openai-realtime/types';

type ConnectionState =
  | 'idle'
  | 'connecting'
  | 'connected'
  | 'disconnected'
  | 'error';
type ConnectionListener = (state: ConnectionState) => void;

export class RealtimeClientClass {
  private options: RealtimeClientOptionsBeforePrune;

  private connectionState: ConnectionState = 'idle';
  private connectionListeners = new Set<ConnectionListener>();

  private connecting = false;
  private disconnecting = false;

  private peerConnectionManager: PeerConnectionManager;
  private mediaManager: MediaManager;
  private dataChannelManager: DataChannelManager;
  private messageSender: MessageSender;
  private eventRouter: EventRouter;
  private apiClient: OpenAIApiClient;

  private errorHandler: ErrorHandler;
  private successHandler: SuccessHandler;

  private chatStore?: ChatStore;
  private chatWired = false;

  private connectSeq = 0;

  // Добавляем отслеживание состояния DataChannel
  private dataChannelReady = false;
  private peerConnectionConnected = false;

  constructor(
    userOptions: RealtimeClientOptionsBeforePrune,
    success?: SuccessHandler,
    error?: ErrorHandler
  ) {
    this.options = applyDefaults(userOptions);

    this.errorHandler =
      error ??
      new ErrorHandler(
        (event) => {
          if (event.severity === 'critical') {
            this.setConnectionState('error');
          }
          this.options.hooks?.onError?.(event);
        },
        { error: this.options.logger?.error }
      );

    const callbacks = {
      onPeerConnectionCreatingStarted: () => {
        this.peerConnectionConnected = false;
        this.dataChannelReady = false;
        this.setConnectionState('connecting');
      },
      onRTCPeerConnectionStateChange: (
        state:
          | 'new'
          | 'connecting'
          | 'connected'
          | 'disconnected'
          | 'failed'
          | 'closed'
      ) => {
        if (state === 'connected') {
          this.peerConnectionConnected = true;
          this.updateConnectionState();
        } else if (state === 'connecting' || state === 'new') {
          this.peerConnectionConnected = false;
          this.setConnectionState('connecting');
        } else if (state === 'failed') {
          this.peerConnectionConnected = false;
          this.dataChannelReady = false;
          this.setConnectionState('error');
        } else if (state === 'disconnected' || state === 'closed') {
          this.peerConnectionConnected = false;
          this.dataChannelReady = false;
          this.setConnectionState('disconnected');
        }
      },
      onDataChannelOpen: () => {
        this.dataChannelReady = true;
        this.updateConnectionState();
      },
      onDataChannelClose: () => {
        this.dataChannelReady = false;
        this.setConnectionState('disconnected');
      },
    };

    this.successHandler =
      success ?? new SuccessHandler(callbacks as any, undefined);

    this.peerConnectionManager = new PeerConnectionManager(
      this.options,
      this.errorHandler,
      this.successHandler
    );
    this.mediaManager = new MediaManager(
      this.options,
      this.errorHandler,
      this.successHandler
    );
    this.dataChannelManager = new DataChannelManager(
      this.options,
      this.errorHandler,
      this.successHandler
    );
    this.messageSender = new MessageSender(
      this.dataChannelManager,
      this.options,
      this.errorHandler
    );
    this.eventRouter = new EventRouter(
      this.options,
      this.sendRaw.bind(this),
      this
    );
    this.apiClient = new OpenAIApiClient(this.errorHandler);

    if (this.options.chat?.enabled !== false) {
      this.chatStore = new ChatStore({
        isMeaningfulText:
          this.options.chat?.isMeaningfulText ??
          this.options.policy?.isMeaningfulText,
        userAddOnDelta: this.options.chat?.userAddOnDelta,
        userPlaceholderOnStart: this.options.chat?.userPlaceholderOnStart,
        assistantAddOnDelta: this.options.chat?.assistantAddOnDelta,
        assistantPlaceholderOnStart:
          this.options.chat?.assistantPlaceholderOnStart,
      });
      this.wireChatStore();
    }
  }

  /**
   * Обновляет состояние подключения на основе состояния PeerConnection и DataChannel
   * Соединение считается полностью установленным только когда оба готовы
   */
  private updateConnectionState() {
    // Проверяем также readyState DataChannel для надежности
    const dc = this.dataChannelManager.getDataChannel();
    const dcActuallyOpen = dc && dc.readyState === 'open';

    if (
      this.peerConnectionConnected &&
      this.dataChannelReady &&
      dcActuallyOpen
    ) {
      this.setConnectionState('connected');
      this.options.logger?.info?.(
        '[RealtimeClient] ✅ Fully connected (PC + DC ready)'
      );
    } else if (this.peerConnectionConnected || this.dataChannelReady) {
      // Хотя бы одно из соединений установлено, но не оба
      this.options.logger?.debug?.(
        `[RealtimeClient] Partial connection (PC: ${this.peerConnectionConnected}, DC: ${this.dataChannelReady}, DC state: ${dc?.readyState})`
      );
      // Остаемся в connecting, пока не будут готовы оба
      if (this.connectionState !== 'connected') {
        this.setConnectionState('connecting');
      }
    }
  }

  setTokenProvider(tp: TokenProvider) {
    if (typeof tp !== 'function')
      throw new Error('setTokenProvider: invalid tokenProvider');
    this.options.tokenProvider = tp;
  }

  private setConnectionState(state: ConnectionState) {
    if (this.connectionState !== state) {
      this.connectionState = state;
      this.options.logger?.debug?.(`[RealtimeClient] Status changed: ${state}`);
      this.connectionListeners.forEach((listener) => listener(state));
    }
  }

  public getConnectionState(): ConnectionState {
    return this.connectionState;
  }

  public getStatus() {
    return this.connectionState;
  }

  /**
   * Возвращает true только если и PeerConnection и DataChannel полностью готовы
   */
  public isFullyConnected(): boolean {
    const dc = this.dataChannelManager.getDataChannel();
    return (
      this.connectionState === 'connected' &&
      this.peerConnectionConnected &&
      this.dataChannelReady &&
      !!dc &&
      dc.readyState === 'open'
    );
  }

  public onConnectionStateChange(listener: ConnectionListener) {
    this.connectionListeners.add(listener);
    return () => this.connectionListeners.delete(listener);
  }

  private wireChatStore(force = false) {
    if (!this.chatStore) return;
    if (this.chatWired && !force) return;

    this.on('user:item_started', ({ itemId }) =>
      this.chatStore!.startUser(itemId)
    );
    this.on('assistant:response_started', ({ responseId }) =>
      this.chatStore!.startAssistant(responseId)
    );
    this.on('user:delta', ({ itemId, delta }) =>
      this.chatStore!.putDelta('user', itemId, delta)
    );
    this.on('user:completed', ({ itemId, transcript }) =>
      this.chatStore!.finalize('user', itemId, 'done', transcript)
    );
    this.on('user:failed', ({ itemId }) =>
      this.chatStore!.finalize('user', itemId, 'done')
    );
    this.on('user:truncated', ({ itemId }) =>
      this.chatStore!.finalize('user', itemId, 'done')
    );
    this.on('assistant:delta', ({ responseId, delta }) =>
      this.chatStore!.putDelta('assistant', responseId, delta)
    );
    this.on('assistant:completed', ({ responseId, status }) =>
      this.chatStore!.finalize('assistant', responseId, status)
    );

    this.chatWired = true;
  }

  on<K extends keyof RealtimeEventMap>(
    type: K,
    handler: RealtimeEventListener<K>
  ): () => void;
  on(type: string, handler: (payload: any) => void): () => void;
  on(type: string, handler: (payload: any) => void): () => void {
    return this.eventRouter.on(type, handler);
  }

  private preConnectCleanup() {
    this.peerConnectionConnected = false;
    this.dataChannelReady = false;

    try {
      this.dataChannelManager.close();
    } catch {}
    try {
      this.peerConnectionManager.close();
    } catch {}
    try {
      this.mediaManager.cleanup();
    } catch {}
  }

  private makeAbortError() {
    const err: any = new Error('connect aborted');
    err.name = 'AbortError';
    err.__ABORT__ = true;
    return err;
  }

  private assertNotAborted(mySeq: number) {
    if (mySeq !== this.connectSeq || this.disconnecting) {
      throw this.makeAbortError();
    }
  }

  async enableMicrophone() {
    try {
      const pc = this.peerConnectionManager.getPeerConnection();
      if (!pc) throw new Error('PeerConnection not created');

      const stream = await this.mediaManager.getUserMedia();
      this.mediaManager.addLocalStreamToPeerConnection(pc, stream);
      try {
        const txs = (pc as any).getTransceivers?.() || [];
        const audioTx = txs.find(
          (t: any) =>
            t?.receiver?.track?.kind === 'audio' ||
            t?.sender?.track?.kind === 'audio'
        );
        const track = stream.getAudioTracks?.()[0];
        if (audioTx?.sender && track) {
          await audioTx.sender.replaceTrack(track);
          if (typeof audioTx.setDirection === 'function') {
            audioTx.setDirection('sendrecv');
          } else {
            // @ts-ignore
            audioTx.direction = 'sendrecv';
          }
        }
      } catch {}

      const offer = await this.peerConnectionManager.createOffer();
      await this.peerConnectionManager.setLocalDescription(offer);
      await this.peerConnectionManager.waitForIceGathering();

      const ephemeralKey = await this.options.tokenProvider();
      const answer = await this.apiClient.postSDP(offer.sdp, ephemeralKey);
      await this.peerConnectionManager.setRemoteDescription(answer);

      this.successHandler.microphonePermissionGranted?.();
      this.options.logger?.info?.(
        '[RealtimeClient] 🎤 Microphone enabled & renegotiated'
      );
    } catch (e: any) {
      this.errorHandler.handle('get_user_media', e, 'critical', false);
      throw e;
    }
  }

  public async disableMicrophone() {
    try {
      const pc = this.peerConnectionManager.getPeerConnection();
      if (!pc) throw new Error('PeerConnection not created');

      const local = this.mediaManager.getLocalStream();
      if (local?.getAudioTracks) {
        local.getAudioTracks().forEach((t: any) => {
          try {
            t.stop();
          } catch {}
        });
      }

      try {
        const txs = (pc as any).getTransceivers?.() || [];
        const audioTx = txs.find(
          (t: any) =>
            t?.sender?.track?.kind === 'audio' ||
            t?.receiver?.track?.kind === 'audio'
        );
        if (audioTx?.sender) {
          await audioTx.sender.replaceTrack(null);
        }
        if (audioTx) {
          if (typeof audioTx.setDirection === 'function') {
            audioTx.setDirection('recvonly');
          } else {
            // @ts-ignore
            audioTx.direction = 'recvonly';
          }
        }
      } catch {}

      const offer = await this.peerConnectionManager.createOffer();
      await this.peerConnectionManager.setLocalDescription(offer);
      await this.peerConnectionManager.waitForIceGathering();
      const ephemeralKey = await this.options.tokenProvider();
      const answer = await this.apiClient.postSDP(offer.sdp, ephemeralKey);
      await this.peerConnectionManager.setRemoteDescription(answer);

      this.mediaManager.stopLocalStream();

      this.options.logger?.info?.(
        '[RealtimeClient] 🔇 Microphone disabled & renegotiated'
      );
    } catch (e: any) {
      this.errorHandler.handle('local_stream', e, 'warning', true);
      throw e;
    }
  }

  async connect() {
    if (this.connecting) {
      this.errorHandler.handle(
        'init_peer_connection',
        new Error('connect() called while connecting'),
        'warning',
        true
      );
      return;
    }
    this.connecting = true;
    const mySeq = ++this.connectSeq;

    try {
      this.setConnectionState('connecting');
      this.preConnectCleanup();

      if (this.chatStore && !this.chatWired) {
        this.wireChatStore(true);
      }

      this.assertNotAborted(mySeq);

      let ephemeralKey: string;
      try {
        const fn = this.options.tokenProvider;
        if (typeof fn !== 'function')
          throw new Error('tokenProvider is not set');
        ephemeralKey = await fn();
        if (!ephemeralKey) throw new Error('Empty ephemeral token');
      } catch (e: any) {
        this.setConnectionState('error');
        this.errorHandler.handle('fetch_token', e, 'critical', false);
        throw e;
      }

      this.assertNotAborted(mySeq);

      const wantsAudioModality =
        Array.isArray(this.options.session?.modalities) &&
        this.options.session!.modalities!.includes('audio');
      const wantsTurnDetection = !!this.options.session?.turn_detection;
      const mustCaptureMic = wantsAudioModality || wantsTurnDetection;

      const shouldTryMic =
        mustCaptureMic || this.options.allowConnectWithoutMic === false;

      let localStream: any = null;
      let needsRecvOnlyTransceiver = false;

      if (shouldTryMic) {
        try {
          localStream = await this.mediaManager.getUserMedia();
          this.assertNotAborted(mySeq);
          this.options.logger?.info?.(
            '[RealtimeClient] ✅ Microphone permission granted'
          );
        } catch (e: any) {
          this.options.logger?.warn?.(
            '[RealtimeClient] ⚠️ Microphone permission denied:',
            e.message || e
          );

          if (this.options.allowConnectWithoutMic === false) {
            this.errorHandler.handle('get_user_media', e, 'critical', false);
            throw e;
          }

          this.errorHandler.handle('get_user_media', e, 'warning', true, {
            reason: 'Will use recvonly transceiver as fallback',
            allowConnectWithoutMic: true,
          });

          needsRecvOnlyTransceiver = true;
        }
      } else {
        this.options.logger?.info?.(
          '[RealtimeClient] 📝 Text mode - no microphone needed'
        );
        needsRecvOnlyTransceiver = true;
      }

      this.assertNotAborted(mySeq);

      const pc = this.peerConnectionManager.create();
      this.assertNotAborted(mySeq);

      this.mediaManager.setupRemoteStream(pc);

      if (localStream) {
        this.mediaManager.addLocalStreamToPeerConnection(pc, localStream);
        this.options.logger?.info?.(
          '[RealtimeClient] 🎤 Local microphone stream added to PeerConnection'
        );
      } else if (needsRecvOnlyTransceiver) {
        try {
          // @ts-ignore
          if (typeof pc.addTransceiver === 'function') {
            // @ts-ignore
            pc.addTransceiver('audio', { direction: 'recvonly' });
            this.successHandler.iosTransceiverSetted?.();
            this.options.logger?.info?.(
              '[RealtimeClient] 🔇 Added recvonly audio transceiver (no mic)'
            );
          } else {
            this.options.logger?.warn?.(
              '[RealtimeClient] ⚠️ addTransceiver not available, continuing anyway'
            );
          }
        } catch (e2: any) {
          this.errorHandler.handle('ios_transceiver', e2, 'info', true);
          this.options.logger?.warn?.(
            '[RealtimeClient] ⚠️ Could not add transceiver:',
            e2.message || e2
          );
        }
      }

      this.assertNotAborted(mySeq);

      this.dataChannelManager.create(pc, async (evt) => {
        await this.eventRouter.processIncomingMessage(evt);
      });

      const offer = await this.peerConnectionManager.createOffer();
      await this.peerConnectionManager.setLocalDescription(offer);
      await this.peerConnectionManager.waitForIceGathering();

      this.assertNotAborted(mySeq);

      const answer = await this.apiClient.postSDP(offer.sdp, ephemeralKey);
      await this.peerConnectionManager.setRemoteDescription(answer);

      this.options.logger?.info?.(
        '[RealtimeClient] 🎉 Connection process completed'
      );
    } catch (e: any) {
      if (e?.__ABORT__ || e?.name === 'AbortError') {
        this.options.logger?.info?.('[RealtimeClient] 🛑 Connection aborted');
        if (this.getStatus() !== 'disconnected') {
          this.setConnectionState('disconnected');
        }
      } else {
        this.options.logger?.error?.(
          '[RealtimeClient] ❌ Connection failed:',
          e.message || e
        );
        if (this.getStatus() !== 'error') {
          this.setConnectionState('error');
          this.errorHandler.handle('init_peer_connection', e);
        }
        throw e;
      }
    } finally {
      this.connecting = false;
    }
  }

  async disconnect() {
    if (this.disconnecting) return;
    this.disconnecting = true;

    this.connectSeq++;

    try {
      this.successHandler.hangUpStarted();

      try {
        this.dataChannelManager.close();
      } catch {}
      try {
        this.mediaManager.cleanup();
      } catch {}
      try {
        this.peerConnectionManager.close();
      } catch {}
      try {
        this.eventRouter.cleanup();
      } catch {}

      this.peerConnectionConnected = false;
      this.dataChannelReady = false;
      this.chatWired = false;

      if (
        this.options.deleteChatHistoryOnDisconnect !== false &&
        this.chatStore
      ) {
        try {
          this.options.logger?.debug?.(
            '[RealtimeClient] Destroying chat history'
          );
          this.chatStore.destroy();
        } catch {}
      } else {
        try {
          this.options.logger?.debug?.(
            '[RealtimeClient] Preserving chat history on disconnect'
          );
        } catch {}
      }

      this.setConnectionState('disconnected');
      this.successHandler.hangUpDone();
    } catch (e: any) {
      this.errorHandler.handle('hangup', e, 'warning', true);
    } finally {
      this.disconnecting = false;
    }
  }

  async sendRaw(event: any): Promise<void> {
    return this.messageSender.sendRaw(event);
  }

  sendResponse(): void;
  sendResponse(params: ResponseCreateParams): void;
  sendResponse(params?: any): void {
    this.messageSender.sendResponse(params);
  }

  sendResponseStrict(options: ResponseCreateStrict) {
    this.messageSender.sendResponseStrict(options);
  }

  updateSession(patch: Partial<any>) {
    this.messageSender.updateSession(patch);
  }

  sendToolOutput(call_id: string, output: any) {
    this.messageSender.sendToolOutput(call_id, output);
  }

  getPeerConnection() {
    return this.peerConnectionManager.getPeerConnection();
  }

  getDataChannel() {
    return this.dataChannelManager.getDataChannel();
  }

  getLocalStream() {
    return this.mediaManager.getLocalStream();
  }

  getRemoteStream() {
    return this.mediaManager.getRemoteStream();
  }

  getChat() {
    return this.chatStore?.get() ?? [];
  }

  public clearChatHistory() {
    if (this.chatStore) {
      this.options.logger?.info?.(
        '[RealtimeClient] Manually clearing chat history'
      );
      this.chatStore.destroy();
    }
  }

  onChatUpdate(handler: (chat: any[]) => void) {
    if (!this.chatStore) {
      return () => {};
    }
    return this.chatStore.subscribe(handler);
  }

  isConnected() {
    return this.isFullyConnected();
  }
}
import {
  AddableMessage,
  ChatMsg,
  CoreConfig,
  ExtendedChatMsg,
  RealtimeClientOptionsBeforePrune,
  RealtimeContextValue,
  TokenProvider,
} from '@react-native-openai-realtime/types';
import {
  useCallback,
  useEffect,
  useMemo,
  useRef,
  useState,
  forwardRef,
  useImperativeHandle,
} from 'react';
import { AppState, AppStateStatus } from 'react-native';
import { RealtimeClientClass } from '@react-native-openai-realtime/components';
import { attachChatAdapter } from '@react-native-openai-realtime/adapters';
import { RealtimeProvider } from '@react-native-openai-realtime/context';
import type {
  EnhancedRealTimeClientProps,
  RealTimeClientHandle,
  SessionMode,
} from '@react-native-openai-realtime/types';
import { prune } from '@react-native-openai-realtime/helpers';
import {
  SuccessHandler,
  ErrorHandler,
} from '@react-native-openai-realtime/handlers';
import { useSessionOptions } from '@react-native-openai-realtime/hooks'; // ВАЖНО: см. вторую часть файла ниже
import { ErrorCallbackPayload } from '@react-native-openai-realtime/types';

const makeId = () => `${Date.now()}-${Math.random().toString(36).slice(2, 8)}`;

export const RealTimeClient = forwardRef<
  RealTimeClientHandle,
  EnhancedRealTimeClientProps
>((props, ref) => {
  const {
    tokenProvider,
    webrtc,
    media,
    chatInverted,
    session,
    autoSessionUpdate,
    greetEnabled,
    greetInstructions,
    greetModalities,
    onOpen,
    onEvent,
    onError,
    onUserTranscriptionDelta,
    onUserTranscriptionCompleted,
    onAssistantTextDelta,
    onAssistantCompleted,
    deleteChatHistoryOnDisconnect = true,
    onToolCall,
    incomingMiddleware,
    outgoingMiddleware,
    policyIsMeaningfulText,
    chatEnabled,
    chatIsMeaningfulText,
    logger,
    autoConnect = false,
    attachChat = true,
    children,
    chatUserAddOnDelta,
    chatUserPlaceholderOnStart,
    chatAssistantAddOnDelta,
    chatAssistantPlaceholderOnStart,
    allowConnectWithoutMic = true,

    // Новые пропсы
    initializeMode,
    attemptsToReconnect = 1,
    onReconnectAttempt,
    onReconnectSuccess,
    onReconnectFailed,

    // Success callbacks
    onHangUpStarted,
    onHangUpDone,
    onPeerConnectionCreatingStarted,
    onPeerConnectionCreated,
    onRTCPeerConnectionStateChange,
    onGetUserMediaSetted,
    onLocalStreamSetted,
    onLocalStreamAddedTrack,
    onLocalStreamRemovedTrack,
    onRemoteStreamSetted,
    onDataChannelOpen,
    onDataChannelMessage,
    onDataChannelClose,
    onIceGatheringComplete,
    onIceGatheringTimeout,
    onIceGatheringStateChange,
    onMicrophonePermissionGranted,
    onMicrophonePermissionDenied,
    onIOSTransceiverSetted,
    onSuccess,
  } = props;

  const clientRef = useRef<RealtimeClientClass | null>(null);
  const [hookClient, setHookClient] = useState<RealtimeClientClass | null>(
    null
  );

  const connectionUnsubRef = useRef<(() => void) | null>(null);
  const detachChatRef = useRef<null | (() => void)>(null);
  const mountedRef = useRef(false);
  const connectCalledRef = useRef(false);
  const reconnectAttemptRef = useRef(0);
  const isReconnectingRef = useRef(false);
  const sessionInitializedRef = useRef(false);
  const initInProgressRef = useRef(false);

  const [status, setStatus] = useState<
    'idle' | 'connecting' | 'connected' | 'disconnected' | 'error'
  >('idle');
  const [chat, setChat] = useState<ChatMsg[]>([]);
  const [addedMessages, setAddedMessages] = useState<ExtendedChatMsg[]>([]);

  const optionsSnapshot: CoreConfig = useMemo(() => {
    return prune({
      tokenProvider,
      webrtc,
      media,
      chatInverted,
      session,
      autoSessionUpdate,
      greet:
        greetEnabled !== undefined || greetInstructions || greetModalities
          ? {
              enabled: greetEnabled ?? true,
              response: {
                instructions: greetInstructions,
                modalities: greetModalities,
              },
            }
          : undefined,
      hooks: prune({
        onOpen,
        onEvent,
        onError: (event: ErrorCallbackPayload) => {
          // Проксируем и одновременно отмечаем критические ошибки
          if (event.severity === 'critical') {
            setStatus('error');
          }
          onError?.(event);
        },
        onUserTranscriptionDelta,
        onUserTranscriptionCompleted,
        onAssistantTextDelta,
        onAssistantCompleted,
        onToolCall,
      }) as any,
      middleware: prune({
        incoming: incomingMiddleware,
        outgoing: outgoingMiddleware,
      }) as any,
      policy: prune({ isMeaningfulText: policyIsMeaningfulText }),
      deleteChatHistoryOnDisconnect,
      chat: prune({
        enabled: chatEnabled,
        isMeaningfulText: chatIsMeaningfulText,
        userAddOnDelta: chatUserAddOnDelta,
        userPlaceholderOnStart: chatUserPlaceholderOnStart,
        assistantAddOnDelta: chatAssistantAddOnDelta,
        assistantPlaceholderOnStart: chatAssistantPlaceholderOnStart,
      }),
      logger,
      allowConnectWithoutMic,
    }) as CoreConfig;
  }, [
    tokenProvider,
    deleteChatHistoryOnDisconnect,
    webrtc,
    media,
    chatInverted,
    session,
    autoSessionUpdate,
    greetEnabled,
    greetInstructions,
    greetModalities,
    onOpen,
    onEvent,
    onError,
    onUserTranscriptionDelta,
    onUserTranscriptionCompleted,
    onAssistantTextDelta,
    onAssistantCompleted,
    onToolCall,
    incomingMiddleware,
    outgoingMiddleware,
    policyIsMeaningfulText,
    chatEnabled,
    chatIsMeaningfulText,
    logger,
    chatUserAddOnDelta,
    chatUserPlaceholderOnStart,
    chatAssistantAddOnDelta,
    chatAssistantPlaceholderOnStart,
    allowConnectWithoutMic,
  ]);

  const ensureClient = useCallback(() => {
    if (!clientRef.current) {
      const errorHandler = new ErrorHandler(
        (event) => {
          if (event.severity === 'critical') {
            setStatus('error');
          }
          onError?.(event);
        },
        { error: logger?.error }
      );

      const successHandler = new SuccessHandler(
        {
          onPeerConnectionCreatingStarted: () => {
            setStatus('connecting');
            onPeerConnectionCreatingStarted?.();
          },
          onPeerConnectionCreated: (pc) => {
            onPeerConnectionCreated?.(pc);
          },
          onRTCPeerConnectionStateChange: (state) => {
            if (state === 'connected') {
              setStatus('connected');
              reconnectAttemptRef.current = 0;
            } else if (state === 'connecting' || state === 'new') {
              setStatus('connecting');
            } else if (state === 'failed') {
              setStatus('error');
            } else if (state === 'disconnected' || state === 'closed') {
              setStatus('disconnected');
            }
            onRTCPeerConnectionStateChange?.(state);
          },
          onDataChannelOpen: (channel) => {
            setStatus('connected');
            onDataChannelOpen?.(channel);
          },
          onDataChannelClose: () => {
            setStatus('disconnected');
            onDataChannelClose?.();
          },
          onGetUserMediaSetted,
          onLocalStreamSetted,
          onLocalStreamAddedTrack,
          onLocalStreamRemovedTrack,
          onRemoteStreamSetted,
          onIceGatheringComplete,
          onIceGatheringTimeout,
          onIceGatheringStateChange,
          onMicrophonePermissionGranted,
          onMicrophonePermissionDenied,
          onIOSTransceiverSetted,
          onHangUpStarted,
          onHangUpDone,
          onDataChannelMessage,
        },
        onSuccess
      );

      clientRef.current = new RealtimeClientClass(
        optionsSnapshot as RealtimeClientOptionsBeforePrune,
        successHandler,
        errorHandler
      );

      // Подписка на изменение connection state
      setStatus(clientRef.current.getConnectionState?.() ?? 'idle');
      const unsub = clientRef.current.onConnectionStateChange?.((s) =>
        setStatus(s as any)
      );
      connectionUnsubRef.current = unsub ?? null;

      if (tokenProvider) {
        try {
          clientRef.current.setTokenProvider(tokenProvider);
        } catch {}
      }
    }

    // Важное: пробрасываем реальный клиент в хук
    setHookClient(clientRef.current!);
    return clientRef.current!;
  }, [
    optionsSnapshot,
    tokenProvider,
    onHangUpStarted,
    onHangUpDone,
    onPeerConnectionCreatingStarted,
    onPeerConnectionCreated,
    onRTCPeerConnectionStateChange,
    onGetUserMediaSetted,
    onLocalStreamSetted,
    onLocalStreamAddedTrack,
    onLocalStreamRemovedTrack,
    onRemoteStreamSetted,
    onDataChannelOpen,
    onDataChannelMessage,
    onDataChannelClose,
    onIceGatheringComplete,
    onIceGatheringTimeout,
    onIceGatheringStateChange,
    onMicrophonePermissionGranted,
    onMicrophonePermissionDenied,
    onIOSTransceiverSetted,
    onSuccess,
    onError,
    logger,
  ]);

  // Создаём клиент заранее при монтировании, чтобы хук получил валидный инстанс
  useEffect(() => {
    const c = ensureClient();
    setHookClient(c);
    return () => {
      try {
        connectionUnsubRef.current?.();
      } catch {}
    };
  }, [ensureClient]);

  // Хук — теперь получает реальный клиент или null и сам «дождётся» готовности
  const sessionOptions = useSessionOptions(hookClient);

  useEffect(() => {
    if (clientRef.current && tokenProvider) {
      try {
        clientRef.current.setTokenProvider(tokenProvider);
      } catch {}
    }
  }, [tokenProvider]);

  useEffect(() => {
    const onAppState = (state: AppStateStatus) => {
      if (state === 'background') {
        const currentStatus = clientRef.current?.getConnectionState?.();
        if (currentStatus === 'connecting') {
          setTimeout(() => {
            if (AppState.currentState === 'background') {
              clientRef.current?.disconnect().catch(() => {});
            }
          }, 1200);
        } else {
          clientRef.current?.disconnect().catch(() => {});
        }
      }
    };
    const sub = AppState.addEventListener('change', onAppState);
    return () => sub.remove();
  }, []);

  // Логика переподключения (без повторной инициализации режима — это делает эффект ниже)
  const handleReconnect = useCallback(async () => {
    if (
      isReconnectingRef.current ||
      reconnectAttemptRef.current >= attemptsToReconnect
    ) {
      return;
    }

    isReconnectingRef.current = true;
    reconnectAttemptRef.current += 1;

    onReconnectAttempt?.(reconnectAttemptRef.current, attemptsToReconnect);

    try {
      await new Promise((resolve) => setTimeout(resolve, 2000));

      if (mountedRef.current && clientRef.current) {
        await clientRef.current.disconnect();
        await new Promise((resolve) => setTimeout(resolve, 500));
        await clientRef.current.connect();

        onReconnectSuccess?.();
        reconnectAttemptRef.current = 0;
        // Важно: после reconnect не вызываем initSession здесь — пусть сделает эффект на "connected"
        sessionInitializedRef.current = false;
      }
    } catch (reconnectError) {
      if (reconnectAttemptRef.current >= attemptsToReconnect) {
        onReconnectFailed?.(reconnectError);
      } else {
        setTimeout(() => handleReconnect(), 1000);
      }
    } finally {
      isReconnectingRef.current = false;
    }
  }, [
    attemptsToReconnect,
    onReconnectAttempt,
    onReconnectSuccess,
    onReconnectFailed,
  ]);

  // Отслеживаем ошибки для переподключения
  useEffect(() => {
    if (status === 'error' && !isReconnectingRef.current) {
      handleReconnect();
    }
  }, [status, handleReconnect]);

  const connect = useCallback(async () => {
    const client = ensureClient();
    try {
      if (attachChat && !detachChatRef.current) {
        const isMeaningful =
          chatIsMeaningfulText ??
          policyIsMeaningfulText ??
          ((t: string) => !!t.trim());
        detachChatRef.current = attachChatAdapter(client, setChat, {
          isMeaningfulText: isMeaningful,
        });
      }
      await client.connect();
    } catch (e) {
      throw e;
    }
  }, [ensureClient, attachChat, chatIsMeaningfulText, policyIsMeaningfulText]);

  const disconnect = useCallback(async () => {
    const client = clientRef.current;
    if (!client) return;
    try {
      sessionInitializedRef.current = false;
      await client.disconnect();
    } finally {
      if (detachChatRef.current) {
        detachChatRef.current();
        detachChatRef.current = null;
      }
      if (deleteChatHistoryOnDisconnect) {
        setChat([]);
        setAddedMessages([]);
      }
    }
  }, [deleteChatHistoryOnDisconnect]);

  useEffect(() => {
    mountedRef.current = true;
    return () => {
      mountedRef.current = false;
      connectCalledRef.current = false;
      try {
        connectionUnsubRef.current?.();
      } catch {}
    };
  }, []);

  // ЕДИНСТВЕННОЕ место инициализации начального режима: после успешного подключения и когда хук «готов»
  useEffect(() => {
    const initializeSession = async () => {
      if (
        status !== 'connected' ||
        !initializeMode ||
        sessionInitializedRef.current ||
        !sessionOptions.clientReady
      ) {
        return;
      }

      if (initInProgressRef.current) return;
      initInProgressRef.current = true;

      try {
        await sessionOptions.initSession(
          initializeMode.type,
          initializeMode.options
        );
        sessionInitializedRef.current = true;
      } catch (e) {
        // логируем, но не паникуем
        logger?.error?.('Failed to init initial session', e);
      } finally {
        initInProgressRef.current = false;
      }
    };

    initializeSession();
  }, [
    status,
    initializeMode,
    sessionOptions,
    sessionOptions.clientReady,
    logger,
  ]);

  useEffect(() => {
    if (!autoConnect || !mountedRef.current || connectCalledRef.current) return;

    connectCalledRef.current = true;
    const t = setTimeout(() => {
      if (mountedRef.current) {
        connect().catch(() => {});
      }
    }, 50);

    return () => {
      clearTimeout(t);
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [autoConnect]);

  const getNextTs = useCallback((): number => {
    try {
      const chatMax =
        (chat?.length ? Math.max(...chat.map((m: any) => m?.ts ?? 0)) : 0) || 0;
      const addedMax =
        (addedMessages?.length
          ? Math.max(...addedMessages.map((m: any) => m?.ts ?? 0))
          : 0) || 0;
      const maxTs = Math.max(chatMax, addedMax);
      return maxTs > 0 ? maxTs + 1 : Date.now();
    } catch {
      return Date.now();
    }
  }, [chat, addedMessages]);

  const normalize = useCallback(
    (m: AddableMessage): ExtendedChatMsg => {
      const base = {
        id: (m as any).id ?? makeId(),
        role: (m as any).role ?? 'assistant',
        ts: (m as any).ts ?? getNextTs(),
        time: Date.now(),
      };

      if (
        (m as any).type === 'ui' ||
        ('kind' in (m as any) && 'payload' in (m as any))
      ) {
        return {
          ...base,
          type: 'ui',
          kind: (m as any).kind,
          payload: (m as any).payload,
        } as unknown as ExtendedChatMsg;
      }

      return {
        ...base,
        type: 'text',
        text: (m as any).text ?? '',
        status: 'done',
      } as unknown as ExtendedChatMsg;
    },
    [getNextTs]
  );

  const addMessage = useCallback(
    (m: AddableMessage | AddableMessage[]) => {
      const arr = Array.isArray(m) ? m : [m];
      const normalized = arr.map(normalize);
      setAddedMessages((prev) => [...prev, ...normalized]);
      const ids = normalized.map((x) => (x as any).id as string);
      return Array.isArray(m) ? ids : ids[0];
    },
    [normalize]
  );

  const clearAdded = useCallback(() => setAddedMessages([]), []);

  const mergedChat = useMemo<ExtendedChatMsg[]>(() => {
    const merged = [...(chat ?? []), ...addedMessages];
    return chatInverted
      ? merged.sort((a: any, b: any) => (a.ts ?? 0) - (b.ts ?? 0))
      : merged.sort((a: any, b: any) => (b.ts ?? 0) - (a.ts ?? 0));
  }, [chat, addedMessages, chatInverted]);

  const clearChatHistory = useCallback(() => {
    clientRef.current?.clearChatHistory();
    // Если нужно синхронно почистить локальный чат:
    // setChat([]);
    // setAddedMessages([]);
  }, []);

  // Методы переключения режимов через хук
  const switchToTextMode = useCallback(
    async (customParams?: Partial<any>) => {
      if (!sessionOptions.clientReady) {
        throw new Error('Client not ready');
      }
      await sessionOptions.initSession('text', customParams);
      sessionInitializedRef.current = true;
    },
    [sessionOptions]
  );

  const switchToVoiceMode = useCallback(
    async (customParams?: Partial<any>) => {
      if (!sessionOptions.clientReady) {
        throw new Error('Client not ready');
      }
      await sessionOptions.initSession('voice', customParams);
      sessionInitializedRef.current = true;
    },
    [sessionOptions]
  );

  const getCurrentMode = useCallback((): SessionMode => {
    return sessionOptions?.mode ?? 'text';
  }, [sessionOptions]);

  const getModeStatus = useCallback(() => {
    return sessionOptions?.isModeReady ?? 'idle';
  }, [sessionOptions]);

  const value: RealtimeContextValue = useMemo(
    () => ({
      client: clientRef.current,
      status,
      clearChatHistory,
      chat: mergedChat,
      connect,
      disconnect,
      sendResponse: (opts?: any) => clientRef.current?.sendResponse(opts),
      sendResponseStrict: (opts: {
        instructions: string;
        modalities?: Array<'audio' | 'text'>;
        conversation?: 'auto' | 'none';
      }) => clientRef.current?.sendResponseStrict(opts),
      updateSession: (patch: Partial<any>) =>
        clientRef.current?.updateSession(patch),
      sendRaw: (e: any) => clientRef.current?.sendRaw(e),
      addMessage,
      clearAdded,
      getNextTs,
    }),
    [
      status,
      mergedChat,
      connect,
      disconnect,
      addMessage,
      clearAdded,
      clearChatHistory,
      getNextTs,
    ]
  );

  useImperativeHandle(
    ref,
    () => ({
      getClient: () => clientRef.current,
      enableMicrophone: async () =>
        await clientRef.current?.enableMicrophone?.(),
      disableMicrophone: async () =>
        await clientRef.current?.disableMicrophone?.(),
      getStatus: () => status,
      setTokenProvider: (tp: TokenProvider) => {
        try {
          clientRef.current?.setTokenProvider(tp);
        } catch {}
      },
      connect: async () => {
        await connect();
      },
      disconnect: async () => {
        await disconnect();
      },
      sendRaw: (e: any) => clientRef.current?.sendRaw(e),
      sendResponse: (opts?: any) => clientRef.current?.sendResponse(opts),
      sendResponseStrict: (opts) => clientRef.current?.sendResponseStrict(opts),
      updateSession: (patch) => clientRef.current?.updateSession(patch),

      addMessage,
      clearAdded,
      clearChatHistory,
      getNextTs,

      switchToTextMode,
      switchToVoiceMode,
      getCurrentMode,
      getModeStatus,
    }),
    [
      status,
      connect,
      disconnect,
      addMessage,
      clearAdded,
      clearChatHistory,
      getNextTs,
      switchToTextMode,
      switchToVoiceMode,
      getCurrentMode,
      getModeStatus,
    ]
  );

  const renderedChildren =
    typeof children === 'function' ? (children as any)(value) : children;

  return (
    <RealtimeProvider value={value}>
      {renderedChildren ?? null}
    </RealtimeProvider>
  );
});
export * from './RealtimeClientClass';
export * from './RealTimeClient';
import { RealtimeContext } from '@react-native-openai-realtime/context';
import { useContext } from 'react';
import type { RealtimeContextValue } from '@react-native-openai-realtime/types';

export function useRealtime(): RealtimeContextValue {
  // ✅ Получаем контекст
  const context = useContext(RealtimeContext);

  // ✅ Проверяем, что контекст существует
  if (!context) {
    throw new Error('useRealtime must be used within a RealtimeProvider');
  }

  return context;
}
import { useEffect, useRef, useState } from 'react';
import { useRealtime } from '@react-native-openai-realtime/hooks';
import type { RealtimeClientClass } from '@react-native-openai-realtime/components';

type Mode = 'server' | 'stats' | 'auto';

export type UseMicrophoneActivityOptions = {
  client?: RealtimeClientClass | null;
  mode?: Mode;
  silenceMs?: number;
  levelThreshold?: number;
  pollInterval?: number;
};

export function useMicrophoneActivity(opts?: UseMicrophoneActivityOptions) {
  const { client: ctxClient } = useRealtime();
  const client = opts?.client ?? (ctxClient as RealtimeClientClass | null);
  const [isMicActive, setIsMicActive] = useState(false);
  const [level, setLevel] = useState(0);
  const [remoteLevel, setRemoteLevel] = useState(0); // уровень собеседника
  const [isCapturing, setIsCapturing] = useState(false);

  const lastHeardRef = useRef<number>(0);
  const silenceMs = opts?.silenceMs ?? 600;
  const threshold = opts?.levelThreshold ?? 0.02;
  const pollInterval = opts?.pollInterval ?? 250;
  const mode = opts?.mode ?? 'auto';

  useEffect(() => {
    if (!client) return;

    let unsubDelta: (() => void) | null = null;
    let unsubCompleted: (() => void) | null = null;
    let silenceTimer: any = null;

    const enableServer = mode === 'server' || mode === 'auto';
    if (enableServer) {
      unsubDelta = client.on('user:delta', () => {
        lastHeardRef.current = Date.now();
        setIsMicActive(true);
        if (silenceTimer) clearTimeout(silenceTimer);
        silenceTimer = setTimeout(() => {
          if (Date.now() - lastHeardRef.current >= silenceMs) {
            setIsMicActive(false);
          }
        }, silenceMs + 20);
      });
      unsubCompleted = client.on('user:completed', () => {
        if (silenceTimer) clearTimeout(silenceTimer);
        silenceTimer = setTimeout(() => setIsMicActive(false), silenceMs / 2);
      });
    }

    let poll: any = null;
    const enableStats = mode !== 'server';
    if (enableStats) {
      poll = setInterval(async () => {
        try {
          const pc = client.getPeerConnection?.();
          if (!pc) return;

          // Проверка захвата аудио
          const localStream = client.getLocalStream?.();
          const capturing =
            !!localStream &&
            typeof localStream.getAudioTracks === 'function' &&
            localStream
              .getAudioTracks()
              .some((t: any) => t.enabled && t.readyState === 'live');
          setIsCapturing(capturing);

          // Уровень исходящего аудио (микрофон)
          if (pc.getSenders) {
            const senders = pc.getSenders();
            const audioSender = senders.find(
              (s: any) => s.track && s.track.kind === 'audio'
            );
            if (audioSender && audioSender.getStats) {
              const stats = await audioSender.getStats();
              const lvl = extractAudioLevel(stats);
              setLevel(lvl);

              if (mode === 'stats') {
                setIsMicActive(lvl > threshold);
              }
              if (mode === 'auto') {
                if (Date.now() - lastHeardRef.current > 2 * silenceMs) {
                  setIsMicActive(lvl > threshold);
                }
              }
            }
          }

          // Уровень входящего аудио (собеседник)
          if (pc.getReceivers) {
            const receivers = pc.getReceivers();
            const audioReceiver = receivers.find(
              (r: any) => r.track && r.track.kind === 'audio'
            );
            if (audioReceiver && audioReceiver.getStats) {
              const stats = await audioReceiver.getStats();
              const lvl = extractAudioLevel(stats);
              setRemoteLevel(lvl);
            }
          }
        } catch {
          // no-op
        }
      }, pollInterval);
    } else {
      const local = client.getLocalStream?.();
      const capturing =
        !!local &&
        typeof local.getAudioTracks === 'function' &&
        local
          .getAudioTracks()
          .some((t: any) => t.enabled && t.readyState === 'live');
      setIsCapturing(capturing);
    }

    return () => {
      if (unsubDelta) unsubDelta();
      if (unsubCompleted) unsubCompleted();
      if (silenceTimer) clearTimeout(silenceTimer);
      if (poll) clearInterval(poll);
    };
  }, [client, mode, silenceMs, threshold, pollInterval]);

  return { isMicActive, level, remoteLevel, isCapturing };
}

// Вспомогательная функция для извлечения уровня из статистики
function extractAudioLevel(stats: RTCStatsReport): number {
  let lvl = 0;
  stats.forEach((r: any) => {
    // Уровень аудио в media-source или track
    if (r.type === 'media-source' && typeof r.audioLevel === 'number') {
      lvl = Math.max(lvl, r.audioLevel);
    }
    if (r.type === 'track' && typeof r.audioLevel === 'number') {
      lvl = Math.max(lvl, r.audioLevel);
    }
    // Энергия аудио
    if (
      typeof r.totalAudioEnergy === 'number' &&
      typeof r.totalSamplesDuration === 'number' &&
      r.totalSamplesDuration > 0
    ) {
      const energy = r.totalAudioEnergy / r.totalSamplesDuration;
      lvl = Math.max(lvl, Math.min(1, Math.sqrt(energy)));
    }
    // Старые реализации
    if (typeof r.audioInputLevel === 'number') {
      lvl = Math.max(lvl, Math.min(1, r.audioInputLevel / 32767));
    }
    // Для входящего аудио может быть audioOutputLevel
    if (typeof r.audioOutputLevel === 'number') {
      lvl = Math.max(lvl, Math.min(1, r.audioOutputLevel / 32767));
    }
  });
  return lvl;
}
import {
  speechActivityStore,
  SpeechActivityStoreType,
} from '@react-native-openai-realtime/middlewares';
import { useEffect, useState } from 'react';
import type { SpeechActivityState } from '@react-native-openai-realtime/types';

export function useSpeechActivity(
  store = speechActivityStore as SpeechActivityStoreType
) {
  const [state, setState] = useState<SpeechActivityState>(() => store.get());
  useEffect(() => store.subscribe(setState) as any, [store]);
  return state;
}
export * from './useRealtime';
export * from './useSpeechActivity';
export * from './useMicrophoneActivity';
export * from './useSessionOptions';
export * from './useSessionOptions';
import { useCallback, useEffect, useRef, useState } from 'react';
import InCallManager from 'react-native-incall-manager';
import { RealtimeClientClass } from '@react-native-openai-realtime/components';

const delay = (ms: number) => new Promise((r) => setTimeout(r, ms));

export const useSessionOptions = (client: RealtimeClientClass | null) => {
  const clientRef = useRef<RealtimeClientClass | null>(client);
  const lastResponseIdRef = useRef<string | null>(null);
  const initInProgressRef = useRef(false);

  const [mode, setMode] = useState<'text' | 'voice'>('text');
  const [isModeReady, setIsModeReady] = useState<
    'idle' | 'connecting' | 'connected' | 'disconnected'
  >('idle');
  const [clientReady, setClientReady] = useState<boolean>(!!client);

  useEffect(() => {
    clientRef.current = client;
    setClientReady(!!client);
  }, [client]);

  const waitUntilDataChannelOpen = useCallback(async (timeoutMs = 5000) => {
    const start = Date.now();
    while (Date.now() - start < timeoutMs) {
      const c = clientRef.current;
      if (c && typeof c.getDataChannel === 'function') {
        try {
          const dc = c.getDataChannel();
          if (dc && dc.readyState === 'open') {
            return true;
          }
        } catch {}
      }
      await delay(100);
    }
    return false;
  }, []);

  const setRemoteTracksEnabled = useCallback((enabled: boolean) => {
    try {
      const remote = clientRef.current?.getRemoteStream?.();
      if (remote && typeof remote.getAudioTracks === 'function') {
        remote.getAudioTracks().forEach((t: any) => {
          t.enabled = enabled;
        });
      }
    } catch (e) {
      throw new Error('Failed to set remote tracks enabled' + e);
    }
  }, []);

  const setMicrophoneEnabled = useCallback((enabled: boolean) => {
    try {
      const c = clientRef.current;
      if (!c || typeof c.getLocalStream !== 'function') return;

      const localStream = c.getLocalStream?.();
      if (localStream) {
        localStream.getAudioTracks().forEach((track: any) => {
          track.enabled = enabled;
        });
      }
    } catch (e) {
      // no-op
      throw new Error('Failed to set microphone enabled' + e);
    }
  }, []);

  const restartSpeakerRoute = useCallback(async () => {
    try {
      InCallManager.start({ media: 'audio', auto: false, ringback: '' });
      InCallManager.setSpeakerphoneOn(true);
      InCallManager.setForceSpeakerphoneOn(true);
    } catch (e) {
      // no-op
      throw new Error('Failed to restart speaker route' + e);
    }
  }, []);

  // Возвращаем функция подписки, без автоподписки по mount
  const subscribeToAssistantEvents = useCallback(
    (onAssistantStarted?: () => void) => {
      const c = clientRef.current;
      if (!c || typeof (c as any).on !== 'function') return () => {};

      const off1 = (c as any).on(
        'assistant:response_started',
        ({ responseId }: any) => {
          lastResponseIdRef.current = responseId;
          onAssistantStarted?.();
        }
      );

      const off2 = (c as any).on(
        'assistant:completed',
        ({ responseId }: any) => {
          if (lastResponseIdRef.current === responseId) {
            lastResponseIdRef.current = null;
          }
        }
      );

      return () => {
        try {
          off1?.();
          off2?.();
        } catch {}
      };
    },
    []
  );

  const cancelAssistantNow = useCallback(
    async (onComplete?: () => void, onFail?: (err: any) => void) => {
      try {
        const c = clientRef.current as any;
        const chan = c?.getDataChannel?.();
        if (!chan || chan.readyState !== 'open') return;

        const rid = lastResponseIdRef.current ?? undefined;

        InCallManager.stop();
        await c?.sendRaw({
          type: 'response.cancel',
          ...(rid ? { response_id: rid } : {}),
        });
        await c?.sendRaw({
          type: 'output_audio_buffer.clear',
        });

        setRemoteTracksEnabled(false);
        setMicrophoneEnabled(false);
        await delay(120);
        onComplete?.();
      } catch (e) {
        onFail?.(e);
      }
    },
    [setRemoteTracksEnabled, setMicrophoneEnabled]
  );

  const enforceTextSession = useCallback(
    async (customParams?: Partial<any>) => {
      const c = clientRef.current as any;
      if (!c) throw new Error('Client not ready');

      try {
        await cancelAssistantNow();

        setRemoteTracksEnabled(false);
        setMicrophoneEnabled(false);
        InCallManager.stop();

        const defaultTextParams = {
          modalities: ['text'],
          turn_detection: null,
          input_audio_transcription: null,
        };

        await c.sendRaw({
          type: 'session.update',
          session: {
            ...defaultTextParams,
            ...customParams,
          },
        });
      } catch (e) {
        throw new Error('Failed to enforce text session' + e);
      }
    },
    [cancelAssistantNow, setRemoteTracksEnabled, setMicrophoneEnabled]
  );

  const enforceVoiceSession = useCallback(
    async (customParams?: Partial<any>) => {
      const c = clientRef.current as any;
      if (!c) throw new Error('Client not ready');

      try {
        const defaultVoiceParams = {
          model: 'gpt-4o-realtime-preview-2024-12-17',
          voice: 'shimmer',
          modalities: ['text', 'audio'],
          turn_detection: {
            type: 'server_vad',
            threshold: 0.6,
            prefix_padding_ms: 200,
            silence_duration_ms: 1200,
          },
          input_audio_transcription: { model: 'whisper-1' },
        };

        await c.sendRaw({
          type: 'session.update',
          session: {
            ...defaultVoiceParams,
            ...customParams,
          },
        });

        await delay(300);
        await restartSpeakerRoute();

        setRemoteTracksEnabled(true);
        setMicrophoneEnabled(true);
      } catch (e) {
        throw new Error('Failed to enforce voice session' + e);
      }
    },
    [restartSpeakerRoute, setRemoteTracksEnabled, setMicrophoneEnabled]
  );

  const initSession = useCallback(
    async (newMode: 'text' | 'voice', customParams?: Partial<any>) => {
      if (!clientReady) {
        throw new Error('Client not ready');
      }
      if (initInProgressRef.current) {
        // Не запускаем повторную инициализацию в параллель
        return;
      }
      initInProgressRef.current = true;

      setIsModeReady('connecting');

      const dcOpened = await waitUntilDataChannelOpen(5000);
      if (!dcOpened) {
        setIsModeReady('disconnected');
        initInProgressRef.current = false;
        throw new Error('DataChannel not open');
      }

      try {
        if (newMode === 'text') {
          await enforceTextSession(customParams);
        } else {
          await enforceVoiceSession(customParams);
        }

        setMode(newMode);
        setIsModeReady('connected');
      } catch (e) {
        setIsModeReady('disconnected');
        throw new Error('Failed to init session' + e);
      } finally {
        initInProgressRef.current = false;
      }
    },
    [
      clientReady,
      waitUntilDataChannelOpen,
      enforceTextSession,
      enforceVoiceSession,
    ]
  );

  const handleSendMessage = useCallback(async (text: string) => {
    if (!text.trim()) {
      throw new Error('Empty message');
    }
    const c = clientRef.current as any;
    const dc = c?.getDataChannel?.();
    if (!dc || dc.readyState !== 'open') {
      throw new Error('DataChannel not open');
    }

    try {
      await c?.sendRaw({
        type: 'conversation.item.create',
        item: {
          type: 'message',
          role: 'user',
          content: [{ type: 'input_text', text }],
        },
      });

      await c?.sendRaw({
        type: 'response.create',
        response: {
          modalities: ['text'],
        },
      });
    } catch (e) {
      throw new Error('Failed to send message' + e);
    }
  }, []);

  return {
    subscribeToAssistantEvents,
    handleSendMessage,
    initSession,
    isModeReady,
    mode,
    cancelAssistantNow,
    clientReady,
  };
};
export * from './adapters';
export * from './components';
export * from './constants';
export * from './context';
export * from './handlers';
export * from './helpers';
export * from './hooks';
export * from './managers';
export * from './middlewares';
export * from './types';
import type { SuccessCallbacks } from '@react-native-openai-realtime/types';
import type { RTCPeerConnection, MediaStreamTrack } from 'react-native-webrtc';
import { MediaStream } from 'react-native-webrtc';
import type RTCDataChannel from 'react-native-webrtc/lib/typescript/RTCDataChannel';

export class SuccessHandler {
  private callbacks: SuccessCallbacks;
  private onSuccess?: (stage: string, data?: any) => void;

  constructor(
    callbacks: SuccessCallbacks = {},
    onSuccess?: (stage: string, data?: any) => void
  ) {
    this.callbacks = callbacks;
    this.onSuccess = onSuccess;
  }

  hangUpStarted() {
    this.onSuccess?.('hang_up_started');
    this.callbacks.onHangUpStarted?.();
  }
  hangUpDone() {
    this.onSuccess?.('hang_up_done');
    this.callbacks.onHangUpDone?.();
  }

  peerConnectionCreatingStarted() {
    this.onSuccess?.('peer_connection_creating_started');
    this.callbacks.onPeerConnectionCreatingStarted?.();
  }
  peerConnectionCreated(pc: RTCPeerConnection) {
    this.onSuccess?.('peer_connection_created', { pc });
    this.callbacks.onPeerConnectionCreated?.(pc as any);
  }

  rtcPeerConnectionStateChange(
    state:
      | 'new'
      | 'connecting'
      | 'connected'
      | 'disconnected'
      | 'failed'
      | 'closed'
  ) {
    this.onSuccess?.('rtc_peer_connection_state_change', { state });
    this.callbacks.onRTCPeerConnectionStateChange?.(state);
  }

  getUserMediaSetted(stream: MediaStream) {
    this.onSuccess?.('get_user_media_setted', { stream });
    this.callbacks.onGetUserMediaSetted?.(stream as any);
  }
  localStreamSetted(stream: MediaStream) {
    this.onSuccess?.('local_stream_setted', { stream });
    this.callbacks.onLocalStreamSetted?.(stream as any);
  }
  localStreamAddedTrack(track: MediaStreamTrack) {
    this.onSuccess?.('local_stream_added_track', { track });
    this.callbacks.onLocalStreamAddedTrack?.(track as any);
  }
  localStreamRemovedTrack(track: MediaStreamTrack) {
    this.onSuccess?.('local_stream_removed_track', { track });
    this.callbacks.onLocalStreamRemovedTrack?.(track as any);
  }
  remoteStreamSetted(stream: MediaStream) {
    this.onSuccess?.('remote_stream_setted', { stream });
    this.callbacks.onRemoteStreamSetted?.(stream as any);
  }

  dataChannelOpen(channel: RTCDataChannel) {
    this.onSuccess?.('data_channel_open', { channel });
    this.callbacks.onDataChannelOpen?.(channel);
  }
  dataChannelMessage(message: any) {
    this.onSuccess?.('data_channel_message', { message });
    this.callbacks.onDataChannelMessage?.(message);
  }
  dataChannelClose() {
    this.onSuccess?.('data_channel_close');
    this.callbacks.onDataChannelClose?.();
  }

  iceGatheringComplete() {
    this.onSuccess?.('ice_gathering_complete');
    this.callbacks.onIceGatheringComplete?.();
  }
  iceGatheringTimeout() {
    this.onSuccess?.('ice_gathering_timeout');
    this.callbacks.onIceGatheringTimeout?.();
  }
  iceGatheringStateChange(state: string) {
    this.onSuccess?.('ice_gathering_state_change', { state });
    this.callbacks.onIceGatheringStateChange?.(state);
  }

  microphonePermissionGranted() {
    this.onSuccess?.('microphone_permission_granted');
    this.callbacks.onMicrophonePermissionGranted?.();
  }
  microphonePermissionDenied() {
    this.onSuccess?.('microphone_permission_denied');
    this.callbacks.onMicrophonePermissionDenied?.();
  }

  iosTransceiverSetted() {
    this.onSuccess?.('ios_transceiver_setted');
    this.callbacks.onIOSTransceiverSetted?.();
  }
}
export * from './error';
export * from './success';
// Файл: handlers/error.ts
// Проверьте что у вас ИМЕННО так:

import type {
  ErrorEvent,
  ErrorStage,
  ErrorSeverity,
} from '@react-native-openai-realtime/types';

export class ErrorHandler {
  private onError?: (event: ErrorEvent) => void;
  private logger?: { error?: (...a: any[]) => void };

  constructor(
    onError?: (event: ErrorEvent) => void,
    logger?: { error?: (...a: any[]) => void }
  ) {
    this.onError = onError;
    this.logger = logger;
  }

  handle(
    stage: ErrorStage,
    error: Error | string,
    severity: ErrorSeverity = 'critical',
    recoverable: boolean = false,
    context?: Record<string, any>
  ) {
    const errorEvent: ErrorEvent = {
      stage,
      error: error instanceof Error ? error : new Error(String(error)),
      severity,
      recoverable,
      timestamp: Date.now(),
      context,
    };

    (this.logger?.error ?? console.error)(
      `[${stage}] ${severity}:`,
      errorEvent.error,
      context
    );

    // ✅ ВАЖНО: Вызываем onError только ПОСЛЕ логирования
    // onError НЕ должен менять connectionState если severity !== 'critical'
    if (this.onError) {
      this.onError(errorEvent);
    }

    return errorEvent;
  }
}
export function prune<T extends Record<string, any>>(
  obj?: T
): Partial<T> | undefined {
  if (!obj) return undefined;
  const out: any = {};
  for (const k of Object.keys(obj)) {
    const v = obj[k];
    if (v === undefined) continue;
    if (v && typeof v === 'object' && !Array.isArray(v)) {
      const child = prune(v);
      if (child && Object.keys(child).length > 0) out[k] = child;
    } else {
      out[k] = v;
    }
  }
  return Object.keys(out).length ? out : undefined;
}
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';
import { deepMerge } from '@react-native-openai-realtime/helpers';
import { DEFAULTS } from '@react-native-openai-realtime/constants';

export function applyDefaults(
  user: RealtimeClientOptionsBeforePrune
): RealtimeClientOptionsBeforePrune {
  const merged = deepMerge<RealtimeClientOptionsBeforePrune>(DEFAULTS, user);
  // Если greet.enabled true, а instructions не заданы — подставим дефолт
  if (
    merged.greet?.enabled &&
    (!merged.greet.response || !merged.greet.response.instructions)
  ) {
    merged.greet = {
      enabled: true,
      response: {
        instructions: DEFAULTS.greet!.response!.instructions!,
        modalities: DEFAULTS.greet!.response!.modalities,
      },
    };
  }
  return merged;
}
export const VOICE_IDS = [
  'alloy',
  'ash',
  'ballad',
  'coral',
  'echo',
  'sage',
  'shimmer',
  'verse',
] as const;

export type VoiceId = (typeof VOICE_IDS)[number];
// src/helpers/createDefaultRouter.ts
import type { RealtimeClientOptionsBeforePrune } from '@react-native-openai-realtime/types';

type Emitter = (type: string, payload?: any) => void;

// Достаём input_text из созданного user item (typed-ввод)
function extractInputTextFromItem(item: any): string | null {
  try {
    const content = item?.content;
    if (!Array.isArray(content)) return null;
    for (const c of content) {
      if (!c || typeof c !== 'object') continue;
      if (c.type === 'input_text' && typeof c.text === 'string') {
        return c.text;
      }
      if (c.type === 'text' && typeof c.text === 'string') {
        return c.text;
      }
    }
    return null;
  } catch {
    return null;
  }
}

export function createDefaultRouter(
  emit: Emitter,
  options: RealtimeClientOptionsBeforePrune,
  functionArgsBuffer: Map<string, string>,
  sendRaw: (e: any) => void
) {
  const responseCompletionState = new Map<
    string,
    {
      textDone: boolean;
      audioDone: boolean;
      hasAudio: boolean; // Флаг: используется ли аудио в этом ответе
    }
  >();

  const checkAndEmitCompletion = (responseId: string) => {
    const state = responseCompletionState.get(responseId);
    if (!state) return;

    // Эмитим completed только когда:
    // 1. Текст готов И (аудио готово ИЛИ аудио не используется)
    const shouldComplete =
      state.textDone && (state.audioDone || !state.hasAudio);

    if (shouldComplete) {
      responseCompletionState.delete(responseId);

      const consumed = options.hooks?.onAssistantCompleted?.({
        responseId,
        status: 'done',
      });

      if (consumed !== 'consume') {
        emit('assistant:completed', { responseId, status: 'done' });
      }
    }
  };

  return async function route(msg: any) {
    const hooks = options.hooks;
    hooks?.onEvent?.(msg);

    // Обработка создания пользовательского item
    if (msg.type === 'conversation.item.created') {
      const item = msg.item;
      const itemId = item?.id;

      if (item?.role === 'user' && itemId) {
        emit('user:item_started', { itemId });

        const typed = extractInputTextFromItem(item);
        if (typed && String(typed).trim()) {
          emit('user:completed', { itemId, transcript: String(typed) });
        }
      }

      if (item?.role === 'assistant' && itemId) {
        emit('assistant:item_started', { itemId });
      }

      return;
    }

    if (msg.type === 'response.created') {
      const responseId = msg.response?.id || msg.response_id;
      if (responseId) {
        // Проверяем modalities из сессии или ответа
        const modalities = msg.response?.modalities ||
          options.session?.modalities || ['text'];
        const hasAudio =
          Array.isArray(modalities) && modalities.includes('audio');

        responseCompletionState.set(responseId, {
          textDone: false,
          audioDone: !hasAudio, // Если аудио нет, считаем его сразу "завершенным"
          hasAudio,
        });

        emit('assistant:response_started', { responseId });
      }
      return;
    }

    if (
      msg.type === 'response.audio.delta' ||
      msg.type === 'output_audio_buffer.started'
    ) {
      const responseId = msg.response_id;
      if (responseId && responseCompletionState.has(responseId)) {
        const state = responseCompletionState.get(responseId)!;
        state.hasAudio = true;
        state.audioDone = false; // Сбрасываем, так как аудио началось
      }
      return;
    }

    // Текстовые дельты от ассистента
    if (msg.type === 'response.text.delta') {
      const responseId = msg.response_id;
      const delta = msg.delta || '';
      const consumed = hooks?.onAssistantTextDelta?.({
        responseId,
        delta,
        channel: 'output_text',
      });
      if (consumed !== 'consume') {
        emit('assistant:delta', { responseId, delta, channel: 'output_text' });
      }
      return;
    }

    if (msg.type === 'response.output_item.text.delta') {
      const responseId = msg.response_id;
      const delta = msg.delta || '';
      const consumed = hooks?.onAssistantTextDelta?.({
        responseId,
        delta,
        channel: 'output_text',
      });
      if (consumed !== 'consume') {
        emit('assistant:delta', { responseId, delta, channel: 'output_text' });
      }
      return;
    }

    if (msg.type === 'response.audio_transcript.delta') {
      const responseId = msg.response_id;
      const delta = msg.delta || '';
      const consumed = hooks?.onAssistantTextDelta?.({
        responseId,
        delta,
        channel: 'audio_transcript',
      });
      if (consumed !== 'consume') {
        emit('assistant:delta', {
          responseId,
          delta,
          channel: 'audio_transcript',
        });
      }
      return;
    }

    if (msg.type === 'response.output_text.delta') {
      const responseId = msg.response_id;
      const delta = msg.delta || '';
      const consumed = hooks?.onAssistantTextDelta?.({
        responseId,
        delta,
        channel: 'output_text',
      });
      if (consumed !== 'consume') {
        emit('assistant:delta', { responseId, delta, channel: 'output_text' });
      }
      return;
    }

    // Транскрипция пользователя
    if (msg.type === 'conversation.item.input_audio_transcription.delta') {
      const itemId = msg.item_id;
      const delta = msg.delta || '';
      const consumed = hooks?.onUserTranscriptionDelta?.({ itemId, delta });
      if (consumed !== 'consume') {
        emit('user:delta', { itemId, delta });
      }
      return;
    }

    if (msg.type === 'conversation.item.input_audio_transcription.completed') {
      const itemId = msg.item_id;
      const transcript = msg.transcript || '';
      const consumed = hooks?.onUserTranscriptionCompleted?.({
        itemId,
        transcript,
      });
      if (consumed !== 'consume') {
        emit('user:completed', { itemId, transcript });
      }
      return;
    }

    if (msg.type === 'conversation.item.input_audio_transcription.failed') {
      const itemId = msg.item_id;
      emit('user:failed', { itemId, error: msg.error });
      return;
    }

    if (msg.type === 'conversation.item.truncated') {
      const itemId = msg.item_id || msg.item?.id;
      emit('user:truncated', { itemId });
      return;
    }

    if (
      msg.type === 'response.text.done' ||
      msg.type === 'response.output_item.text.done' ||
      msg.type === 'response.audio_transcript.done' ||
      msg.type === 'response.output_text.done'
    ) {
      const responseId = msg.response_id;

      if (responseCompletionState.has(responseId)) {
        const state = responseCompletionState.get(responseId)!;
        state.textDone = true;
        checkAndEmitCompletion(responseId);
      } else {
        // Если нет в state (старый response или что-то пошло не так) - эмитим как обычно
        const consumed = hooks?.onAssistantCompleted?.({
          responseId,
          status: 'done',
        });
        if (consumed !== 'consume') {
          emit('assistant:completed', { responseId, status: 'done' });
        }
      }
      return;
    }

    if (msg.type === 'output_audio_buffer.stopped') {
      const responseId = msg.response_id;

      if (responseId && responseCompletionState.has(responseId)) {
        const state = responseCompletionState.get(responseId)!;
        state.audioDone = true;
        checkAndEmitCompletion(responseId);
      }

      // Эмитим специальное событие для middleware
      emit('output_audio_buffer.stopped', {});
      return;
    }

    if (msg.type === 'output_audio_buffer.cleared') {
      const responseId = msg.response_id;

      if (responseId && responseCompletionState.has(responseId)) {
        const state = responseCompletionState.get(responseId)!;
        state.audioDone = true;
        checkAndEmitCompletion(responseId);
      }

      emit('output_audio_buffer.cleared', {});
      return;
    }

    // Общие события завершения/отмены
    if (msg.type === 'response.done' || msg.type === 'response.completed') {
      const responseId = msg.response_id || msg.response?.id;

      if (responseCompletionState.has(responseId)) {
        // Форсируем завершение, даже если аудио не закончилось
        // (response.done означает, что сервер закончил генерацию)
        responseCompletionState.delete(responseId);
      }

      const consumed = hooks?.onAssistantCompleted?.({
        responseId,
        status: 'done',
      });
      if (consumed !== 'consume') {
        emit('assistant:completed', { responseId, status: 'done' });
      }
      return;
    }

    if (msg.type === 'response.cancelled' || msg.type === 'response.canceled') {
      const responseId = msg.response_id || msg.response?.id;
      responseCompletionState.delete(responseId); // Очищаем state

      const consumed = hooks?.onAssistantCompleted?.({
        responseId,
        status: 'canceled',
      });
      if (consumed !== 'consume') {
        emit('assistant:completed', { responseId, status: 'canceled' });
      }
      return;
    }

    // Вызовы инструментов
    if (msg.type === 'response.function_call_arguments.delta') {
      const prev = functionArgsBuffer.get(msg.call_id) || '';
      functionArgsBuffer.set(msg.call_id, prev + (msg.delta || ''));
      emit('tool:call_delta', {
        call_id: msg.call_id,
        name: msg.name,
        delta: msg.delta || '',
      });
      return;
    }

    if (msg.type === 'response.function_call_arguments.done') {
      try {
        const argsStr = functionArgsBuffer.get(msg.call_id) || '{}';
        functionArgsBuffer.delete(msg.call_id);
        const args = JSON.parse(argsStr);

        emit('tool:call_done', { call_id: msg.call_id, name: msg.name, args });

        if (options.hooks?.onToolCall) {
          const output = await options.hooks.onToolCall({
            name: msg.name,
            args,
            call_id: msg.call_id,
          });
          if (output !== undefined) {
            sendRaw({
              type: 'conversation.item.create',
              item: {
                type: 'function_call_output',
                call_id: msg.call_id,
                output: JSON.stringify(output),
              },
            });
            sendRaw({ type: 'response.create' });
          }
        }
      } catch (e) {
        emit('error', { scope: 'tool', error: e });
      }
      return;
    }

    // Серверные ошибки
    if (msg.type === 'error') {
      emit('error', { scope: 'server', error: msg.error });
      return;
    }

    // Логирование необработанных событий
    if (options.logger?.debug) {
      const knownTypes = [
        'session.created',
        'session.updated',
        'input_audio_buffer.committed',
        'input_audio_buffer.cleared',
        'input_audio_buffer.speech_started',
        'input_audio_buffer.speech_stopped',
        'output_audio_buffer.started',
        'rate_limits.updated',
        'response.audio.delta', // Добавлено
        'response.audio.done', // Добавлено
      ];
      if (!knownTypes.includes(msg.type)) {
        options.logger.debug(`[Router] Unhandled event: ${msg.type}`, msg);
      }
    }
  };
}
export function deepMerge<T>(base: Partial<T>, patch?: Partial<T>): T {
  const out: any = Array.isArray(base)
    ? [...(base as any)]
    : { ...(base as any) };
  if (!patch) {
    return out;
  }
  for (const [k, v] of Object.entries(patch as any)) {
    if (v && typeof v === 'object' && !Array.isArray(v)) {
      out[k] = deepMerge(out[k] ?? {}, v);
    } else {
      out[k] = v;
    }
  }
  return out;
}
export * from './applyDefaults';
export * from './createDefaultRouter';
export * from './deepMerge';
export * from './prune';
export * from './voice';
